{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.conv as conv\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from faker import Faker\n",
    "\n",
    "from collections import namedtuple\n",
    "import torch.nn.init as init\n",
    "from torchvision import models\n",
    "from torchvision.models.vgg import model_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RGB'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Image.new(\"L\", (32,32))\n",
    "x = x.convert(\"RGB\")\n",
    "\n",
    "x.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1739, 0.2686, 0.7423, 0.8505, 0.2512],\n",
       "          [0.3430, 0.2187, 0.1697, 0.4549, 0.7532],\n",
       "          [0.0130, 0.4571, 0.4755, 0.9247, 0.5827],\n",
       "          [0.0250, 0.5624, 0.8674, 0.9459, 0.7332],\n",
       "          [0.9416, 0.7897, 0.5296, 0.8566, 0.1756]],\n",
       "\n",
       "         [[0.3479, 0.5373, 1.4847, 1.7010, 0.5024],\n",
       "          [0.6860, 0.4374, 0.3394, 0.9098, 1.5063],\n",
       "          [0.0260, 0.9142, 0.9510, 1.8494, 1.1655],\n",
       "          [0.0501, 1.1247, 1.7348, 1.8918, 1.4664],\n",
       "          [1.8831, 1.5795, 1.0592, 1.7132, 0.3512]],\n",
       "\n",
       "         [[0.5218, 0.8059, 2.2270, 2.5514, 0.7536],\n",
       "          [1.0290, 0.6562, 0.5090, 1.3647, 2.2595],\n",
       "          [0.0389, 1.3713, 1.4265, 2.7742, 1.7482],\n",
       "          [0.0751, 1.6871, 2.6022, 2.8376, 2.1996],\n",
       "          [2.8247, 2.3692, 1.5889, 2.5699, 0.5268]]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb = torch.tensor([[1, 2, 3]]).float()\n",
    "ones = torch.ones(1, 1, 5, 5)\n",
    "bg = rgb.view(1, 3, 1, 1) * ones\n",
    "# ones * 2\n",
    "\n",
    "mask = torch.rand((1,1,5,5))\n",
    "mask = mask.expand(-1,3,-1,-1)\n",
    "\n",
    "(1-mask) * bg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10, 10])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "im = Image.new(\"L\", (10, 10))\n",
    "draw = ImageDraw.Draw(im)\n",
    "draw.rectangle((0, 0, 10, 10), fill=255)\n",
    "\n",
    "x = transform(im).unsqueeze(0)\n",
    "\n",
    "theta = torch.zeros(1, 2, 3)\n",
    "angle = np.pi/2.\n",
    "theta[:, :, :2] = torch.tensor([[np.cos(angle), -1.0*np.sin(angle)],\n",
    "                                [np.sin(angle), np.cos(angle)]])\n",
    "theta[:, :, 2] = 0.5\n",
    "\n",
    "theta = torch.tensor([\n",
    "    [2., 0., 0.],\n",
    "    [0., 2., 0.],\n",
    "])\n",
    "theta = theta.unsqueeze(0)\n",
    "\n",
    "grid = F.affine_grid(theta, x.size())\n",
    "x_trans = F.grid_sample(x, grid)\n",
    "\n",
    "print(x_trans.shape)\n",
    "\n",
    "plt.imshow(x.squeeze().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_trans.squeeze().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3741, 0.3681, 0.3492],\n",
      "          [0.3320, 0.3454, 0.3627],\n",
      "          [0.3239, 0.9607, 0.4874]],\n",
      "\n",
      "         [[0.0445, 0.6446, 0.7109],\n",
      "          [0.3040, 0.6809, 0.0729],\n",
      "          [0.7148, 0.5619, 0.9850]]],\n",
      "\n",
      "\n",
      "        [[[0.0033, 0.4571, 0.5281],\n",
      "          [0.7055, 0.4006, 0.4891],\n",
      "          [0.3139, 0.2660, 0.2203]],\n",
      "\n",
      "         [[0.0057, 0.5814, 0.9530],\n",
      "          [0.5706, 0.5379, 0.1231],\n",
      "          [0.6729, 0.0408, 0.3090]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3741, -0.3681, -0.3492],\n",
       "          [-0.3320, -0.3454, -0.3627],\n",
       "          [-0.3239, -0.9607, -0.4874]],\n",
       "\n",
       "         [[ 0.0445,  0.6446,  0.7109],\n",
       "          [ 0.3040,  0.6809,  0.0729],\n",
       "          [ 0.7148,  0.5619,  0.9850]]],\n",
       "\n",
       "\n",
       "        [[[-0.0033, -0.4571, -0.5281],\n",
       "          [-0.7055, -0.4006, -0.4891],\n",
       "          [-0.3139, -0.2660, -0.2203]],\n",
       "\n",
       "         [[ 0.0057,  0.5814,  0.9530],\n",
       "          [ 0.5706,  0.5379,  0.1231],\n",
       "          [ 0.6729,  0.0408,  0.3090]]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2,2,3,3)\n",
    "print(a)\n",
    "b = torch.tensor([-1., 1.]).view(1, 2, 1, 1)\n",
    "# torch.tensor([[[[1]], [[-1]]]]).shape\n",
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/CoordConv-pytorch/gan-textbox\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1600, grad_fn=<SqueezeBackward0>),\n",
       " tensor([[[0.0039, 0.0038, 0.0039,  ..., 0.0040, 0.0040, 0.0040],\n",
       "          [0.0039, 0.0038, 0.0038,  ..., 0.0039, 0.0040, 0.0040],\n",
       "          [0.0039, 0.0038, 0.0038,  ..., 0.0039, 0.0039, 0.0040],\n",
       "          ...,\n",
       "          [0.0040, 0.0038, 0.0039,  ..., 0.0040, 0.0040, 0.0040],\n",
       "          [0.0040, 0.0038, 0.0039,  ..., 0.0039, 0.0040, 0.0040],\n",
       "          [0.0039, 0.0038, 0.0038,  ..., 0.0039, 0.0040, 0.0040]]],\n",
       "        grad_fn=<SoftmaxBackward>),\n",
       " tensor([[[0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0156, 0.0155],\n",
       "          [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0155, 0.0156],\n",
       "          [0.0155, 0.0157, 0.0156,  ..., 0.0156, 0.0155, 0.0156],\n",
       "          ...,\n",
       "          [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0154],\n",
       "          [0.0156, 0.0157, 0.0156,  ..., 0.0155, 0.0155, 0.0155],\n",
       "          [0.0156, 0.0157, 0.0156,  ..., 0.0156, 0.0156, 0.0156]]],\n",
       "        grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /notebooks/CoordConv-pytorch/gan-textbox\n",
    "\n",
    "from models.sagan import SpectralNorm, Self_Attn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, batch_size=64, image_size=64, conv_dim=64, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.imsize = image_size\n",
    "        layer1 = []\n",
    "        layer2 = []\n",
    "        layer3 = []\n",
    "        last = []\n",
    "\n",
    "        layer1 += [\n",
    "            SpectralNorm(nn.Conv2d(in_channels, conv_dim, 4, 2, 1)),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        ]\n",
    "        \n",
    "        curr_dim = conv_dim\n",
    "\n",
    "        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer2.append(nn.LeakyReLU(0.1))\n",
    "        curr_dim = curr_dim * 2\n",
    "\n",
    "        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer3.append(nn.LeakyReLU(0.1))\n",
    "        curr_dim = curr_dim * 2\n",
    "\n",
    "#         if self.imsize == 64:\n",
    "        layer4 = []\n",
    "        layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\n",
    "        layer4.append(nn.LeakyReLU(0.1))\n",
    "        self.l4 = nn.Sequential(*layer4)\n",
    "        curr_dim = curr_dim * 2\n",
    "\n",
    "        self.l1 = nn.Sequential(*layer1)\n",
    "        self.l2 = nn.Sequential(*layer2)\n",
    "        self.l3 = nn.Sequential(*layer3)\n",
    "\n",
    "        if self.imsize == 64:\n",
    "            last += [\n",
    "                nn.Conv2d(curr_dim, 1, 4),\n",
    "            ]\n",
    "        elif self.imsize == 128:\n",
    "            last += [\n",
    "                nn.Conv2d(curr_dim, 1, 4),\n",
    "                nn.Conv2d(1, 1, 5),\n",
    "            ]\n",
    "        self.last = nn.Sequential(*last)\n",
    "\n",
    "        self.attn1 = Self_Attn(256, \"relu\")\n",
    "        self.attn2 = Self_Attn(512, \"relu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.l2(out)\n",
    "        out = self.l3(out)\n",
    "        out, p1 = self.attn1(out)\n",
    "        out = self.l4(out)\n",
    "        out, p2 = self.attn2(out)\n",
    "        out = self.last(out)\n",
    "\n",
    "        return out.squeeze(), p1, p2  \n",
    "\n",
    "im_size = 128\n",
    "m = Discriminator(batch_size=1, image_size=im_size, conv_dim=64, in_channels=1)\n",
    "x = torch.rand((1,1,im_size,im_size))\n",
    "m(x)\n",
    "# summary(m, (1, 64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             160\n",
      "         LeakyReLU-2           [-1, 16, 32, 32]               0\n",
      "         Dropout2d-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 32, 16, 16]           4,640\n",
      "         LeakyReLU-5           [-1, 32, 16, 16]               0\n",
      "         Dropout2d-6           [-1, 32, 16, 16]               0\n",
      "       BatchNorm2d-7           [-1, 32, 16, 16]              64\n",
      "            Conv2d-8             [-1, 64, 8, 8]          18,496\n",
      "         LeakyReLU-9             [-1, 64, 8, 8]               0\n",
      "        Dropout2d-10             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-11             [-1, 64, 8, 8]             128\n",
      "           Conv2d-12            [-1, 128, 4, 4]          73,856\n",
      "        LeakyReLU-13            [-1, 128, 4, 4]               0\n",
      "        Dropout2d-14            [-1, 128, 4, 4]               0\n",
      "      BatchNorm2d-15            [-1, 128, 4, 4]             256\n",
      "           Linear-16                    [-1, 1]           2,049\n",
      "          Sigmoid-17                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 99,649\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 0.81\n",
      "Params size (MB): 0.38\n",
      "Estimated Total Size (MB): 1.21\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            block = [\n",
    "                nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Dropout2d(0.25),\n",
    "            ]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(img_shape[0], 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        ds_size = img_shape[1] // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        # print(validity.shape)\n",
    "        return validity\n",
    "m = Discriminator(img_shape=(1, 64, 64))\n",
    "summary(m, (1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "\n",
    "class Paste2dMask(nn.Module):\n",
    "    def __init__(self, im_size):\n",
    "        super(Paste2dMask, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid())\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def loss(self, y, x1, x2, gt):\n",
    "        loss_restore = self.criterion(y, gt)\n",
    "        loss_coord = torch.mean(F.relu(-(x2 - x1 - 1.0)))\n",
    "        loss = loss_restore + loss_coord\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = self.im_size\n",
    "        N = x.shape[0]\n",
    "\n",
    "        x0 = x[:, 0].view(-1, 1) * l - 1.0\n",
    "        y0 = x[:, 1].view(-1, 1) * l - 1.0\n",
    "        x1 = x[:, 2].view(-1, 1) * l + 1.0\n",
    "        y1 = x[:, 3].view(-1, 1) * l + 1.0\n",
    "\n",
    "        coord = torch.arange(l).expand(N, -1).float()\n",
    "        if cuda:\n",
    "            coord = coord.cuda()\n",
    "\n",
    "        _x0 = F.relu6((coord - x0) * 6.0)\n",
    "        _x1 = F.relu6((x1 - coord) * 6.0)\n",
    "        x_mask = (_x0 * _x1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        x_mask = x_mask.view(N, 1, l)\n",
    "\n",
    "        _y0 = F.relu6((coord - y0) * 6.0)\n",
    "        _y1 = F.relu6((y1 - coord) * 6.0)\n",
    "        y_mask = (_y0 * _y1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        y_mask = y_mask.view(N, l, 1)  # align to y-axis\n",
    "\n",
    "        mask = torch.ones(N, l, l)\n",
    "        if cuda:\n",
    "            mask = mask.cuda()\n",
    "        mask = mask * x_mask * y_mask\n",
    "        return mask.view(-1, 1, l, l)\n",
    "\n",
    "\n",
    "class Paste2dMulti(nn.Module):\n",
    "    def __init__(self, im_size, num_images):\n",
    "        super(Paste2dMulti, self).__init__()\n",
    "        self.im_size = im_size\n",
    "        self.paste2dMask = Paste2dMask(im_size)\n",
    "        self.num_images = num_images\n",
    "\n",
    "    def _paste(self, coord):\n",
    "        w = torch.Tensor([[self.im_size]]).float().expand(coord.shape[0], 4)\n",
    "        coord = torch.round(w * coord)\n",
    "        ims = []\n",
    "        for i, _coord in enumerate(coord):\n",
    "            x0, y0, x1, y1 = _coord[0:4]\n",
    "            ims.append(F.pad(ims[i], (x0, y0, 3, 4), \"constant\", 0))\n",
    "        return F.pad(input, (1, 2, 3, 4), \"constant\", 0)\n",
    "\n",
    "    def _pad_image(self, coord, image):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            coord: (N, 4=(x0, y0, x1, y1))\n",
    "            image: (N, C, H, W)\n",
    "        Return:\n",
    "            padded image: (N, C, self.im_size, self.im_size)\n",
    "        \"\"\"\n",
    "        N, C, H, W = image.shape\n",
    "        y = []\n",
    "        for _coord, _im in zip(coord, image):\n",
    "            f = lambda i: int(_coord[i].item())\n",
    "            x0, y0, _, _ = f(0), f(1), f(2), f(3)  # lost gradient\n",
    "            x1, y1 = x0 + W, y0 + H\n",
    "            if any(\n",
    "                [\n",
    "                    x0 >= x1,\n",
    "                    y0 >= y1,\n",
    "                    x0 > self.im_size,\n",
    "                    x1 < 1,\n",
    "                    y0 > self.im_size,\n",
    "                    y1 < 1,\n",
    "                ]\n",
    "            ):\n",
    "                # null mask\n",
    "                y.append(torch.zeros(1, C, self.im_size, self.im_size).type(_im.type()))\n",
    "            else:\n",
    "                y.append(\n",
    "                    F.pad(\n",
    "                        _im,\n",
    "                        (x0, self.im_size - x1, y0, self.im_size - y1),\n",
    "                        \"constant\",\n",
    "                        0,\n",
    "                    ).unsqueeze(0)\n",
    "                )\n",
    "        return torch.cat(y, dim=0)\n",
    "\n",
    "    def forward(self, coords, images):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            coords: (N, num_images, 4=(x0, y0, x1, y1))\n",
    "            images: (N, num_images, C, H, W)\n",
    "        \"\"\"\n",
    "        y = []\n",
    "        for _coord, _im in zip(coords.split(1, dim=1), images.split(1, dim=1)):\n",
    "            _coord = _coord.squeeze(1)  # (N, 4)\n",
    "            _im = _im.squeeze(1)  # (N, C, H, W)\n",
    "            _im = self._pad_image(_coord, _im)  # (N, C, self.im_size, self.im_size)\n",
    "            mask = self.paste2dMask(\n",
    "                _coord / self.im_size\n",
    "            )  # (N, C, self.im_size, self.im_size)\n",
    "            _im = mask * _im\n",
    "            y.append(_im)\n",
    "        y = torch.cat(y, 1).sum(1, True)  # TODO: change to alpha blending\n",
    "        ones = torch.ones(*y.shape).cuda() if cuda else torch.ones(*y.shape)\n",
    "        y = torch.min(ones, y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class PasteLine(nn.Module):\n",
    "    def __init__(self, im_size, max_chars):\n",
    "        super(PasteLine, self).__init__()\n",
    "        self.im_size = im_size\n",
    "        self.max_chars = max_chars\n",
    "        self.paste2dMulti = Paste2dMulti(im_size, max_chars)\n",
    "\n",
    "    def forward(self, coord, chars, char_sizes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coord: (N, 2=(x0, y0)), coordinate for the line\n",
    "            chars: (N, num_chars, C, H, W)\n",
    "            char_sizes: (N, num_chars, 2=(char_w, char_h))\n",
    "        \"\"\"\n",
    "        coord = coord.unsqueeze(1)\n",
    "        coords = []  # (N, num_chars, 4=(x0, y0, x1, y1))\n",
    "        for size in char_sizes.split(1, dim=1):\n",
    "            coords.append(torch.cat((coord, coord + size), dim=2))\n",
    "            mask = torch.tensor([[[1, 0]]]).float()\n",
    "            mask = mask.cuda() if cuda else mask\n",
    "            coord = coord + size * mask  # (x0 += w, y0)\n",
    "        coords = torch.cat(coords, dim=1)\n",
    "        return self.paste2dMulti(coords, chars)\n",
    "    \n",
    "    \n",
    "def text_to_char_images(text, font, font_size=14, out_size=14):\n",
    "    font = ImageFont.truetype(font, font_size)\n",
    "    transform = transforms.ToTensor()\n",
    "    chars, sizes = [], []\n",
    "    for c in text:\n",
    "        size = font.getsize(c)\n",
    "        im = Image.new(\"L\", (out_size, out_size))\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        draw.text((0, 0), c, font=font, fill=255)\n",
    "        chars.append(transform(im).unsqueeze(0))\n",
    "        sizes.append(torch.Tensor(size).view(1, 2))\n",
    "    return torch.cat(chars), torch.cat(sizes)\n",
    "\n",
    "# max_chars = 10\n",
    "# text=['aaa', 'bbb']\n",
    "# painter = PasteLine(28, max_chars=10)\n",
    "# painter.eval()\n",
    "# for param in painter.parameters():\n",
    "#     param.requires_grad = False  # freeze weight\n",
    "\n",
    "# chars, char_sizes = [], []\n",
    "# for t in text:\n",
    "#     _chars, _char_sizes = text_to_char_images(t, \"/notebooks/CoordConv-pytorch/gan-textbox/Roboto-Regular.ttf\")\n",
    "#     padding = max_chars - len(t)\n",
    "#     if padding > 0:\n",
    "#         _chars = torch.cat(\n",
    "#             [_chars, torch.zeros((padding, 1, 14, 14))]\n",
    "#         )\n",
    "#         _char_sizes = torch.cat([_char_sizes, torch.zeros((padding, 2))])\n",
    "#     chars.append(_chars.unsqueeze(0))\n",
    "#     char_sizes.append(_char_sizes.unsqueeze(0))\n",
    "\n",
    "# chars\n",
    "# transform = transforms.ToPILImage()\n",
    "# torch.cat(chars).shape\n",
    "# x = painter(torch.tensor([[0., 0.], [0., 0.]]), torch.cat(chars, 0), torch.cat(char_sizes, 0))\n",
    "# # x.shape\n",
    "# transform(x[0])\n",
    "# transform(x[1])\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.ToPILImage()\n",
    "m = Paste2dMask(6)\n",
    "x = m(torch.tensor([[0., 0., 0.5, 0.5]]))\n",
    "# transform(x[0])\n",
    "# a = np.random.uniform(0.1, 0.8, 2)\n",
    "# b = np.random.uniform(0.1, 0.8, 2)\n",
    "a = np.array([1,2])\n",
    "b = np.array([3, 4])\n",
    "np.concatenate((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = Image.open(\"/notebooks/CoordConv-pytorch/data/fontimg/Alegreya-Regular/A.png\")\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "x = transform(im)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " 'B': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]])}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "from torchvision import datasets\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        root = \"/notebooks/CoordConv-pytorch/data/fontimg\"\n",
    "        src_dir = \"/notebooks/CoordConv-pytorch/data/fontimg/Alegreya-Regular/\"\n",
    "\n",
    "        src_im = {}\n",
    "        for f in sorted(glob.glob(src_dir + \"/*.png\")):\n",
    "            cls, _ = os.path.splitext(os.path.basename(f))\n",
    "            src_im[cls] = f\n",
    "\n",
    "        samples = []\n",
    "        for dst_folder in sorted(glob.glob(root + \"/*/\")):\n",
    "            if dst_folder != src_dir:\n",
    "                for f in sorted(glob.glob(dst_folder + \"/*.png\")):\n",
    "                    cls, _ = os.path.splitext(os.path.basename(f))\n",
    "                    if cls in src_im.keys():\n",
    "                        samples.append((src_im[cls], f))\n",
    "        self.samples = samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src, dst = self.samples[index]\n",
    "        img_A = self.transform(Image.open(src))\n",
    "        img_B = self.transform(Image.open(dst))\n",
    "\n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "d = ImageDataset(\"/notebooks/CoordConv-pytorch/data/fontimg\", \n",
    "                [\n",
    "    transforms.Resize((64, 64), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           9,472\n",
      "    InstanceNorm2d-2           [-1, 64, 64, 64]               0\n",
      "              ReLU-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4          [-1, 192, 32, 32]         110,784\n",
      "    InstanceNorm2d-5          [-1, 192, 32, 32]               0\n",
      "              ReLU-6          [-1, 192, 32, 32]               0\n",
      "            Conv2d-7          [-1, 576, 16, 16]         995,904\n",
      "    InstanceNorm2d-8          [-1, 576, 16, 16]               0\n",
      "              ReLU-9          [-1, 576, 16, 16]               0\n",
      "           Conv2d-10          [-1, 576, 16, 16]       2,986,560\n",
      "   InstanceNorm2d-11          [-1, 576, 16, 16]               0\n",
      "             ReLU-12          [-1, 576, 16, 16]               0\n",
      "          Dropout-13          [-1, 576, 16, 16]               0\n",
      "           Conv2d-14          [-1, 576, 16, 16]       2,986,560\n",
      "   InstanceNorm2d-15          [-1, 576, 16, 16]               0\n",
      "      ResnetBlock-16          [-1, 576, 16, 16]               0\n",
      "           Conv2d-17          [-1, 576, 16, 16]       2,986,560\n",
      "   InstanceNorm2d-18          [-1, 576, 16, 16]               0\n",
      "             ReLU-19          [-1, 576, 16, 16]               0\n",
      "          Dropout-20          [-1, 576, 16, 16]               0\n",
      "           Conv2d-21          [-1, 576, 16, 16]       2,986,560\n",
      "   InstanceNorm2d-22          [-1, 576, 16, 16]               0\n",
      "      ResnetBlock-23          [-1, 576, 16, 16]               0\n",
      "           Conv2d-24          [-1, 576, 16, 16]       2,986,560\n",
      "   InstanceNorm2d-25          [-1, 576, 16, 16]               0\n",
      "             ReLU-26          [-1, 576, 16, 16]               0\n",
      "          Dropout-27          [-1, 576, 16, 16]               0\n",
      "           Conv2d-28          [-1, 576, 16, 16]       2,986,560\n",
      "   InstanceNorm2d-29          [-1, 576, 16, 16]               0\n",
      "      ResnetBlock-30          [-1, 576, 16, 16]               0\n",
      "  ConvTranspose2d-31          [-1, 192, 32, 32]         995,520\n",
      "   InstanceNorm2d-32          [-1, 192, 32, 32]               0\n",
      "             ReLU-33          [-1, 192, 32, 32]               0\n",
      "  ConvTranspose2d-34           [-1, 64, 64, 64]         110,656\n",
      "   InstanceNorm2d-35           [-1, 64, 64, 64]               0\n",
      "             ReLU-36           [-1, 64, 64, 64]               0\n",
      "           Conv2d-37            [-1, 3, 64, 64]           9,411\n",
      "             Tanh-38            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 20,151,107\n",
      "Trainable params: 20,151,107\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 48.19\n",
      "Params size (MB): 76.87\n",
      "Estimated Total Size (MB): 125.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def conv_norm_relu_module(norm_type, norm_layer, input_nc, ngf, kernel_size, padding, stride=1, relu='relu'):\n",
    "\n",
    "    model = [nn.Conv2d(input_nc, ngf, kernel_size=kernel_size, padding=padding,stride=stride)]\n",
    "    if norm_layer:\n",
    "        model += [norm_layer(ngf)]\n",
    "\n",
    "    if relu=='relu':\n",
    "        model += [nn.ReLU(True)]\n",
    "    elif relu=='Lrelu':\n",
    "        model += [nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, norm_type='batch'):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, norm_type)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, norm_type):\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        # TODO: support padding types\n",
    "        assert(padding_type == 'zero')\n",
    "        p = 1\n",
    "\n",
    "        # TODO: InstanceNorm\n",
    "\n",
    "        conv_block += conv_norm_relu_module(norm_type, norm_layer, dim,dim, 3, p)\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "        else:\n",
    "            conv_block += [nn.Dropout(0.0)]\n",
    "        \n",
    "\n",
    "        if norm_type=='batch' or norm_type=='instance':\n",
    "            conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n",
    "                        norm_layer(dim)]\n",
    "        else:\n",
    "            assert(\"norm not defined\")\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResnetGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.InstanceNorm2d, use_dropout=False, n_blocks=6, norm_type='batch', gpu_ids=[]):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "        self.gpu_ids = gpu_ids\n",
    "\n",
    "\n",
    "        model = conv_norm_relu_module(norm_type, norm_layer, input_nc, ngf, 7, 3)\n",
    "                 \n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            factor_ch = 3 #2**i : 3**i is a more complicated filter\n",
    "            mult = factor_ch**i \n",
    "            model += conv_norm_relu_module(norm_type,norm_layer, ngf * mult, ngf * mult * factor_ch, 3,1, stride=2)\n",
    "\n",
    "        mult = factor_ch**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(ngf * mult, 'zero', norm_layer=norm_layer, use_dropout=use_dropout, norm_type=norm_type)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = factor_ch**(n_downsampling - i)\n",
    "\n",
    "            model += convTranspose_norm_relu_module(norm_type,norm_layer, ngf * mult, int(ngf * mult / factor_ch), 3, 1,\n",
    "                                        stride=2, output_padding=1)\n",
    "\n",
    "        if norm_type=='batch' or norm_type=='instance':\n",
    "            model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=3)]\n",
    "        else:\n",
    "            assert('norm not defined')\n",
    "\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input, encoder=False):  \n",
    "    \n",
    "        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n",
    "            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n",
    "        else:\n",
    "            return self.model(input)\n",
    "\n",
    "        \n",
    "m = ResnetGenerator(input_nc=3, output_nc=3, n_blocks=3)\n",
    "summary(m, (3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ReflectionPad2d-1          [-1, 256, 66, 66]               0\n",
      "            Conv2d-2          [-1, 256, 64, 64]         590,080\n",
      "    InstanceNorm2d-3          [-1, 256, 64, 64]               0\n",
      "              ReLU-4          [-1, 256, 64, 64]               0\n",
      "   ReflectionPad2d-5          [-1, 256, 66, 66]               0\n",
      "            Conv2d-6          [-1, 256, 64, 64]         590,080\n",
      "    InstanceNorm2d-7          [-1, 256, 64, 64]               0\n",
      "     ResidualBlock-8          [-1, 256, 64, 64]               0\n",
      "   ReflectionPad2d-9          [-1, 256, 66, 66]               0\n",
      "           Conv2d-10          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-11          [-1, 256, 64, 64]               0\n",
      "             ReLU-12          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-13          [-1, 256, 66, 66]               0\n",
      "           Conv2d-14          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-15          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-16          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-17          [-1, 256, 66, 66]               0\n",
      "           Conv2d-18          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-19          [-1, 256, 64, 64]               0\n",
      "             ReLU-20          [-1, 256, 64, 64]               0\n",
      "  ReflectionPad2d-21          [-1, 256, 66, 66]               0\n",
      "           Conv2d-22          [-1, 256, 64, 64]         590,080\n",
      "   InstanceNorm2d-23          [-1, 256, 64, 64]               0\n",
      "    ResidualBlock-24          [-1, 256, 64, 64]               0\n",
      "         Upsample-25        [-1, 256, 128, 128]               0\n",
      "           Conv2d-26        [-1, 128, 128, 128]         819,328\n",
      "        LayerNorm-27        [-1, 128, 128, 128]               0\n",
      "             ReLU-28        [-1, 128, 128, 128]               0\n",
      "         Upsample-29        [-1, 128, 256, 256]               0\n",
      "           Conv2d-30         [-1, 64, 256, 256]         204,864\n",
      "        LayerNorm-31         [-1, 64, 256, 256]               0\n",
      "             ReLU-32         [-1, 64, 256, 256]               0\n",
      "  ReflectionPad2d-33         [-1, 64, 262, 262]               0\n",
      "           Conv2d-34          [-1, 3, 256, 256]           9,411\n",
      "             Tanh-35          [-1, 3, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 4,574,083\n",
      "Trainable params: 4,574,083\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.00\n",
      "Forward/backward pass size (MB): 471.56\n",
      "Params size (MB): 17.45\n",
      "Estimated Total Size (MB): 493.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "        std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(features, features, 3),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(features, features, 3),\n",
    "            nn.InstanceNorm2d(features),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels=3, dim=64, n_residual=3, n_upsample=2, style_dim=8):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        dim = dim * 2 ** n_upsample\n",
    "        for _ in range(n_residual):\n",
    "            layers += [ResidualBlock(dim)]\n",
    "\n",
    "        for _ in range(n_upsample):\n",
    "            layers += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(dim, dim // 2, 5, stride=1, padding=2),\n",
    "                LayerNorm(dim // 2),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            dim = dim // 2\n",
    "\n",
    "        # Output layer\n",
    "        layers += [nn.ReflectionPad2d(3), nn.Conv2d(dim, out_channels, 7), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, content_code):\n",
    "        # Update AdaIN parameters by MLP prediction based off style code\n",
    "#         self.assign_adain_params(self.mlp(style_code))\n",
    "        img = self.model(content_code)\n",
    "        return img\n",
    "\n",
    "m = Decoder(dim=64)\n",
    "# m(torch.rand((1, 3)))\n",
    "summary(m, (256, 64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = Faker()\n",
    "tk = fake.word()\n",
    "font = ImageFont.truetype(\n",
    "    \"/notebooks/post-generator/asset/fonts_en/Roboto/Roboto-Regular.ttf\", 14\n",
    ")\n",
    "samples = []\n",
    "size = font.getsize(tk)\n",
    "# im = Image.new(\"L\", (opt.img_size, opt.img_size))\n",
    "# draw = ImageDraw.Draw(im)\n",
    "# x0, y0 = (opt.img_size - size[0]) / 2, (opt.img_size - size[1]) / 2\n",
    "# draw.text((x0, y0), tk, font=font, fill=255)\n",
    "# # im.save(\"%s/%d.png\" % (self.root, i), \"PNG\")\n",
    "# samples.append((im, np.array([])))\n",
    "# size\n",
    "\n",
    "x = F.pad( torch.rand((1, 1, 5, 5)), (-5, 5, 0, 0),\"constant\",0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_to_char_images(\n",
    "    text,\n",
    "    font,\n",
    "    font_size=14,\n",
    "    out_size=14,\n",
    "):\n",
    "    font = ImageFont.truetype(font, font_size)\n",
    "    transform = transforms.ToTensor()\n",
    "    chars, sizes = [], []\n",
    "    for c in text:\n",
    "        size = font.getsize(c)\n",
    "        im = Image.new(\"L\", (out_size, out_size))\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        draw.text((0, 0), c, font=font, fill=255)\n",
    "        chars.append(transform(im).unsqueeze(0))\n",
    "        sizes.append(torch.Tensor(size).view(1, 2))\n",
    "    return torch.cat(chars), torch.cat(sizes)\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, img_size=64, num_samples=100):\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                # transforms.Resize(opt.img_size),\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "        self.font = \"/notebooks/post-generator/asset/fonts_en/Roboto/Roboto-Regular.ttf\"\n",
    "        self.font_size = 14\n",
    "        self.out_size = 14\n",
    "        self.max_chars = 10\n",
    "        self.img_size = img_size\n",
    "        self.samples = self._sample(num_samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        im, text_status, text = self.samples[index]\n",
    "        chars, sizes = text_to_char_images(\n",
    "            text, self.font, self.font_size, self.out_size\n",
    "        )\n",
    "        padding = self.max_chars - len(text)\n",
    "        if padding > 0:\n",
    "            chars = torch.cat(\n",
    "                [chars, torch.zeros((padding, 1, self.out_size, self.out_size))]\n",
    "            )\n",
    "            sizes = torch.cat([sizes, torch.zeros((padding, 2))])\n",
    "        return self.transform(im), torch.tensor(text_status).float(), chars, sizes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _sample(self, num_samples):\n",
    "        fake = Faker()\n",
    "        tk = fake.word()\n",
    "        font = ImageFont.truetype(self.font, self.font_size)\n",
    "        samples = []\n",
    "        for i in range(num_samples):\n",
    "            tk_w, tk_h = font.getsize(tk)\n",
    "            im = Image.new(\"L\", (self.img_size, self.img_size))\n",
    "            draw = ImageDraw.Draw(im)\n",
    "            x0, y0 = (self.img_size - tk_w) / 2, (self.img_size - tk_h) / 2\n",
    "            draw.text((x0, y0), tk, font=font, fill=255 )\n",
    "            # im.save(\"%s/%d.png\" % (self.root, i), \"PNG\")\n",
    "            samples.append((im, np.array([tk_w, tk_h]), tk))\n",
    "        return samples\n",
    "\n",
    "transform = transforms.ToPILImage()\n",
    "data = MyDataset(num_samples=3)\n",
    "\n",
    "data[0][3].shape\n",
    "# transform(data[0][2][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[51.2000, 51.2000, 59.2000, 64.2000]])\n",
      "tensor([[59.2000, 51.2000, 66.2000, 64.2000]])\n",
      "tensor([[66.2000, 51.2000, 69.2000, 64.2000]])\n",
      "tensor([[69.2000, 51.2000, 73.2000, 67.2000]])\n",
      "tensor([[73.2000, 51.2000, 78.2000, 64.2000]])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAAn0lEQVR4nO3QMQrCQBBA0U8iEtJpEbRISglBUJDcR/Ag3sMuegNr0fSpLARLxW3sVEIQIa5FbjAWWsxrZpv5DAtKKaWUUkoppf7P1DTT+Tb0m0C0q4oJ4C9ul5YkkLmpvwSy9rgr2Q/tEGaGfh2A5ILodYAaBs5VFrBv2zyegRV94tmLwYWTF5elYB82eZIeDaz3o54oEG6rYm6gs7o/PpMfJdEo7VLQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x7FA43A7589E8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "class Paste2dMask(nn.Module):\n",
    "    def __init__(self, im_size):\n",
    "        super(Paste2dMask, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid())\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def loss(self, y, x1, x2, gt):\n",
    "        loss_restore = self.criterion(y, gt)\n",
    "        loss_coord = torch.mean(F.relu(-(x2 - x1 - 1.0)))\n",
    "        loss = loss_restore + loss_coord\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = self.im_size\n",
    "        N = x.shape[0]\n",
    "\n",
    "        x0 = x[:, 0].view(-1, 1) * l - 1.0\n",
    "        y0 = x[:, 1].view(-1, 1) * l - 1.0\n",
    "        x1 = x[:, 2].view(-1, 1) * l + 1.0\n",
    "        y1 = x[:, 3].view(-1, 1) * l + 1.0\n",
    "\n",
    "        coord = torch.arange(l).expand(N, -1).float()\n",
    "        if cuda:\n",
    "            coord = coord.cuda()\n",
    "\n",
    "        _x0 = F.relu6((coord - x0) * 6.0)\n",
    "        _x1 = F.relu6((x1 - coord) * 6.0)\n",
    "        x_mask = (_x0 * _x1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        x_mask = x_mask.view(N, 1, l)\n",
    "\n",
    "        _y0 = F.relu6((coord - y0) * 6.0)\n",
    "        _y1 = F.relu6((y1 - coord) * 6.0)\n",
    "        y_mask = (_y0 * _y1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        y_mask = y_mask.view(N, l, 1)  # align to y-axis\n",
    "\n",
    "        mask = torch.ones(N, l, l)\n",
    "        if cuda:\n",
    "            mask = mask.cuda()\n",
    "        mask = mask * x_mask * y_mask\n",
    "        return mask.view(-1, 1, l, l)\n",
    "    \n",
    "class Paste2dMulti(nn.Module):\n",
    "    def __init__(self, im_size, num_images):\n",
    "        super(Paste2dMulti, self).__init__()\n",
    "        self.im_size = im_size\n",
    "        self.paste2dMask = Paste2dMask(im_size)\n",
    "        self.num_images = num_images\n",
    "    \n",
    "    def _paste(self, coord):\n",
    "        w = torch.Tensor([[self.im_size]]).float().expand(coord.shape[0],4)\n",
    "        coord = torch.round(w * coord)\n",
    "        ims = []\n",
    "        for i, _coord in enumerate(coord):\n",
    "            x0, y0, x1, y1 = _coord[0:4]\n",
    "            ims.append(F.pad(image[i], (x0,y0,3,4), \"constant\", 0))\n",
    "        return F.pad(input, (1,2,3,4), \"constant\", 0)\n",
    "\n",
    "    def _pad_image(self, coord, image):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            coord: (N, 4=(x0, y0, x1, y1))\n",
    "            image: (N, C, H, W)\n",
    "        Return:\n",
    "            Tensor (N, C, self.im_size, self.im_size), padded image\n",
    "        \"\"\"\n",
    "        print(coord)\n",
    "        N, C, H, W = image.shape\n",
    "        y = []\n",
    "        for _coord, _im in zip(coord, image):\n",
    "            f = lambda i: int(_coord[i].item())\n",
    "            x0, y0, _, _  = f(0), f(1), f(2), f(3)  # lost gradient\n",
    "            x1, y1 = x0+W, y0+H\n",
    "            if any([x0 >= x1, y0 >= y1, x0 > self.im_size, x1 < 1, y0 > self.im_size, y1 < 1]):\n",
    "                # null mask\n",
    "                y.append(torch.zeros(_im.shape[0], self.im_size, self.im_size).type(_im.type()))\n",
    "            else:\n",
    "                y.append(\n",
    "                    F.pad(\n",
    "                        _im,\n",
    "                        (x0, self.im_size - x1, y0, self.im_size - y1),\n",
    "                        \"constant\",\n",
    "                        0,\n",
    "                    )\n",
    "                )\n",
    "        return torch.cat(y, dim=0)\n",
    "    \n",
    "    def forward(self, coords, images):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            coords: (N, num_images, 4=(x0, y0, x1, y1))\n",
    "            images: (N, num_images, C, H, W)\n",
    "        \"\"\"\n",
    "        y = []\n",
    "        for _coord, _im in zip(coords.split(1, dim=1), images.split(1, dim=1)):\n",
    "            _coord = _coord.squeeze(1)  # (N, 4)\n",
    "            _im = _im.squeeze(1)  # (N, C, H, W)\n",
    "            _im = self._pad_image(_coord, _im)  # (N, C, self.im_size, self.im_size)\n",
    "            mask = self.paste2dMask(_coord / self.im_size)  # (N, C, self.im_size, self.im_size)\n",
    "            _im = mask * _im\n",
    "            y.append(_im)\n",
    "        y = torch.cat(y, 1).sum(1, True)  # TODO: change to alpha blending\n",
    "        y = torch.min(torch.ones(*y.shape), y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class PasteLine(nn.Module):\n",
    "    def __init__(self, im_size, max_chars):\n",
    "        super(PasteLine, self).__init__()\n",
    "        self.im_size = im_size\n",
    "        self.max_chars = max_chars\n",
    "        self.paste2dMulti = Paste2dMulti(im_size, max_chars)\n",
    "\n",
    "    def forward(self, coord, chars, char_sizes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coord: (N, 2=(x0, y0)), coordinate for the line\n",
    "            chars: (N, num_chars, C, H, W)\n",
    "            char_sizes: (N, num_chars, 2=(char_w, char_h))\n",
    "        \"\"\"\n",
    "        coord = coord * self.im_size # relative coords [0..1] to absoluate coord\n",
    "        coord = coord.unsqueeze(1)\n",
    "        coords = []  # (N, num_chars, 4=(x0, y0, x1, y1))\n",
    "        for size in char_sizes.split(1, dim=1):\n",
    "            coords.append(torch.cat((coord, coord + size), dim=2))\n",
    "            coord = coord + size * torch.tensor([[[1, 0]]]).float()  # (x0 += w, y0)\n",
    "        coords = torch.cat(coords, dim=1)\n",
    "        return self.paste2dMulti(coords, chars)\n",
    "    \n",
    "def to_char_images(text, font='/notebooks/post-generator/asset/fonts_en/Roboto/Roboto-Regular.ttf', font_size=14, out_size=14):\n",
    "    font = ImageFont.truetype(font, font_size)\n",
    "    transform = transforms.ToTensor()\n",
    "    chars, sizes = [], []\n",
    "    for c in text:\n",
    "        size = font.getsize(c)\n",
    "        im = Image.new(\"L\", (out_size, out_size))\n",
    "        draw = ImageDraw.Draw(im)\n",
    "        draw.text((0, 0), c, font=font, fill=255)\n",
    "        chars.append(transform(im).unsqueeze(0))\n",
    "        sizes.append(torch.Tensor(size).view(1, 2))\n",
    "    return torch.cat(chars), torch.cat(sizes)\n",
    "\n",
    "\n",
    "chars, sizes = to_char_images('deijf')\n",
    "chars = chars.unsqueeze(0)\n",
    "sizes = sizes.unsqueeze(0)\n",
    "\n",
    "# print(ims.shape, sizes.shape)\n",
    "m = PasteLine(64, 4)\n",
    "y = m(torch.Tensor([[0.8, 0.8]]), chars, sizes)\n",
    "\n",
    "transform = transforms.ToPILImage()\n",
    "transform(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-to-line-image through neural network and keeps gradient\n",
    "# 1. text to char images\n",
    "# 2. paste char images as a single line\n",
    "\n",
    "class Paste2d(nn.Module):\n",
    "    def __init__(self, im_size):\n",
    "        super(Paste2d, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid())\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def loss(self, y, x1, x2, gt):\n",
    "        loss_restore = self.criterion(y, gt)\n",
    "        loss_coord = torch.mean(F.relu(-(x2 - x1 - 1.0)))\n",
    "        loss = loss_restore + loss_coord\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = self.im_size\n",
    "        N = x.shape[0]\n",
    "\n",
    "        x0 = x[:, 0].view(-1, 1) * l - 1.0\n",
    "        y0 = x[:, 1].view(-1, 1) * l - 1.0\n",
    "        x1 = x[:, 2].view(-1, 1) * l + 1.0\n",
    "        y1 = x[:, 3].view(-1, 1) * l + 1.0\n",
    "\n",
    "        coord = torch.arange(l).expand(N, -1).float()\n",
    "        if cuda:\n",
    "            coord = coord.cuda()\n",
    "\n",
    "        _x0 = F.relu6((coord - x0) * 6.0)\n",
    "        _x1 = F.relu6((x1 - coord) * 6.0)\n",
    "        x_mask = (_x0 * _x1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        x_mask = x_mask.view(N, 1, l)\n",
    "\n",
    "        _y0 = F.relu6((coord - y0) * 6.0)\n",
    "        _y1 = F.relu6((y1 - coord) * 6.0)\n",
    "        y_mask = (_y0 * _y1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        y_mask = y_mask.view(N, l, 1)  # align to y-axis\n",
    "\n",
    "        mask = torch.ones(N, l, l)\n",
    "        if cuda:\n",
    "            mask = mask.cuda()\n",
    "        mask = mask * x_mask * y_mask\n",
    "        return mask.view(-1, 1, l, l)\n",
    "\n",
    "\n",
    "class PasteLine(nn.Module):\n",
    "    def __init__(self, im_size):\n",
    "        super(PasteLine, self).__init__()\n",
    "        self.n_chars = 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        for _x0 in x:\n",
    "            \n",
    "        return mask.view(-1, 1, l, l)\n",
    "\n",
    "    \n",
    "text = 'this is a test'\n",
    "char_ims = [char_im(c) for c in text]\n",
    "\n",
    "m = PasteLine()\n",
    "y = m(char_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paste2d(nn.Module):\n",
    "    def __init__(self, im_size):\n",
    "        super(Paste2d, self).__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid())\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def loss(self, y, x1, x2, gt):\n",
    "        loss_restore = self.criterion(y, gt)\n",
    "        loss_coord = torch.mean(F.relu(-(x2 - x1 - 1.0)))\n",
    "        loss = loss_restore + loss_coord\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        l = self.im_size\n",
    "        N = x.shape[0]\n",
    "\n",
    "        x0 = x[:, 0].view(-1, 1) * l - 1.0\n",
    "        y0 = x[:, 1].view(-1, 1) * l - 1.0\n",
    "        x1 = x[:, 2].view(-1, 1) * l + 1.0\n",
    "        y1 = x[:, 3].view(-1, 1) * l + 1.0\n",
    "        \n",
    "        coord = torch.arange(l).expand(N, -1).float()\n",
    "        if cuda:\n",
    "          coord = coord.cuda()\n",
    "          \n",
    "        _x0 = F.relu6((coord - x0) * 6.0)\n",
    "        _x1 = F.relu6((x1 - coord) * 6.0)\n",
    "        x_mask = (_x0 * _x1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        x_mask = x_mask.view(N, 1, l)\n",
    "        \n",
    "        _y0 = F.relu6((coord - y0) * 6.0)\n",
    "        _y1 = F.relu6((y1 - coord) * 6.0)\n",
    "        y_mask = (_y0 * _y1) / 36  # normalize again after relu6 (multiply by 6.)\n",
    "        y_mask = y_mask.view(N, l, 1)  # align to y-axis\n",
    "        \n",
    "        mask = torch.ones(N,l,l)\n",
    "        if cuda:\n",
    "          mask = mask.cuda()\n",
    "        mask = mask * x_mask * y_mask\n",
    "        return mask.view(-1, 1, l, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAAQUlEQVR4nO3MMQoAIAwEwVP8d+LLtVBEsIlgJTtNtggnAQCAp5KNWyXbo4YH2rxS2yNFB3L08eOB4iv9CAAAcK8DxMQHj79aXzgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x7F475D40A7F0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "img_size = 64\n",
    "painter = Paste2d(img_size)\n",
    "\n",
    "def sample_text_layout(n_samples=100):\n",
    "    w, h = img_size, img_size\n",
    "    fake = Faker()\n",
    "\n",
    "    def _sample_one(i):\n",
    "        # sample text seed\n",
    "        # textbox = (0, 0, fake.pyint(min_value=w*2/3, max_value=w, step=1))  # (x, y, w, h)\n",
    "        token_h = 5\n",
    "        textbox_wh = (w * 2 / 3, token_h)\n",
    "        #     textbox_xy = (w - textbox_wh[0] / 2, h - textbox_wh[1] / 2)\n",
    "        textbox_xy = (0, (h - textbox_wh[1]) / 2)\n",
    "        n_tokens = fake.pyint(1, 3, 1)\n",
    "        token_gap = fake.pyint(3, 5, 1)\n",
    "        tokens_width = (fake.pyint(3, 6, 1) for _ in range(n_tokens))\n",
    "\n",
    "        _x, _y = textbox_xy\n",
    "        tokens_xywh = []\n",
    "        for _w in tokens_width:\n",
    "            tokens_xywh.append((_x, _y, _w, token_h))\n",
    "            _x += _w + token_gap\n",
    "\n",
    "        # render token boxes\n",
    "        transform = transforms.ToPILImage()\n",
    "        y = torch.zeros(1, 1, w, h)\n",
    "        for _, (x0, y0, _w, _h) in enumerate(tokens_xywh):\n",
    "            x1, y1 = x0 + _w, y0 + _h\n",
    "            x = torch.tensor([[x0 / w, y0 / h, x1 / w, y1 / h]]).float()\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "            _y = painter(x)\n",
    "            y += _y.cpu()\n",
    "\n",
    "        im = transform(y[0].cpu())\n",
    "        return im\n",
    "#         im.save(\"%s/%d.png\" % (opt.data_path, i), \"PNG\")\n",
    "\n",
    "    return _sample_one(0)\n",
    "        \n",
    "sample_text_layout(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAAqUlEQVR4nO3asQ3CMBBAURu5YxQaPEOUOagRSzAKe7hkACpvkSFSIBYAHV8n/d+k8iVP11mp9+1cIjsuz9D5h9Dpf0gAXXpAez8e10vQC25Bcz+l34AAOgF0AugE0AmgE0AngE4AXXpAoz+gvE7r12f7HOk3IIBOAJ0AOgF0AugE0AmgE0AngE4AnQC69AD+YqvP8cvx9BsQQCeATgCdALrqr8dwAujSA3a7NgxzbkbZywAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F39506F9EB8>"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "# Brush = Enum('Brush', 'RECT PHOTO TEXT SPRITE')\n",
    "Brush = Enum('Brush', 'RECT')\n",
    "\n",
    "def paint(x, draw):\n",
    "    x, y, w, h, r, g, b, brush = x\n",
    "    x0, y0, x1, y1 = x, y, x+w, y+h\n",
    "\n",
    "    if Brush(brush) == Brush.RECT:\n",
    "        draw.rectangle((x0, y0, x1, y1), fill=(r, g, b))\n",
    "#     elif brush == Brush.PHOTO:\n",
    "#         pass\n",
    "#     elif brush == Brush.TEXT:\n",
    "#         pass\n",
    "#     elif brush == Brush.SPRITE:\n",
    "#         pass\n",
    "\n",
    "def _sample_x(frame_xywh, xywh=None, brush=None, rgb=None,min_wh=(5, 5)):\n",
    "    if xywh is None:\n",
    "        w = fake.pyint(min_wh[0], frame_xywh[2])\n",
    "        h = fake.pyint(min_wh[1], frame_xywh[3])\n",
    "        x = fake.pyint(0, frame_xywh[2] - w) + frame_xywh[0]\n",
    "        y = fake.pyint(0, frame_xywh[3] - h)+ frame_xywh[1]\n",
    "        xywh = x, y, w, h\n",
    "    if brush is None:\n",
    "        brush = random.choice(list(Brush))\n",
    "    if rgb is None:\n",
    "        rgb = [int(x) for x in fake.rgb_color().split(\",\")]\n",
    "    return (*xywh, *rgb, brush.value)\n",
    "    \n",
    "def cut(xywh, ratio=random.choice([1/6, 1/5, 1/4, 1/3, 1/2])):\n",
    "    x, y, w, h = xywh\n",
    "    r0 = random.choice([ratio, 1-ratio])\n",
    "    w0 = int(w * ratio)\n",
    "    w1 = w - w0\n",
    "    xywh0 = (x, y, w0, h)\n",
    "    xywh1 = (x+w0, y, w1, h)\n",
    "    return xywh0, xywh1\n",
    "\n",
    "# type(Brush.RECT.value)\n",
    "# random.choice(list(Brush))\n",
    "\n",
    "xs = []\n",
    "for f in cut((0, 0, 64, 64)):\n",
    "    xs += [_sample_x(f, xywh=f, brush=Brush.RECT), _sample_x(f)]  # bg, item\n",
    "    \n",
    "im = Image.new(\"RGB\", (img_shape[1], img_shape[2]))\n",
    "draw = ImageDraw.Draw(im)\n",
    "for x in xs:\n",
    "    paint(x, draw)\n",
    "    \n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/CoordConv-pytorch/experiments/gan/munit\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAAcUlEQVR4nO2WSwrAIAxEM73/nacbi/aTMmA3jfNWivpIAglG5PDl7J/Uy6gB4c5j8sfDbTYCCyxYTJA2nSpIx5EmQN71moB5DmIExMkw7CQBIoIcFOw1kQQkGK2QuGQzPRONMRUo++8zxqwDb4sPXDPskHoTJm6lG5wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=64x64 at 0x7F395389F1D0>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /notebooks/CoordConv-pytorch/experiments/gan/munit\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # *block(in_dim, 128, normalize=False),\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Linear(128, 128),\n",
    "            *block(128, 128, normalize=False),\n",
    "            *block(128, 256, normalize=False),\n",
    "            *block(256, 512, normalize=False),\n",
    "            *block(512, 1024, normalize=False),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            # nn.Tanh(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "painter = Generator(in_dim=4)\n",
    "painter.load_state_dict(torch.load('saved_models/95000.pt', map_location=\"cpu\"))\n",
    "\n",
    "transform = transforms.ToPILImage()\n",
    "y = painter(torch.tensor([[0, 0, 0, 0]]).float())\n",
    "transform(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2728, 0.6679],\n",
      "        [0.2252, 0.6757],\n",
      "        [0.1806, 0.6840]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.0000, 0.0000, 3.8174, 6.0000, 6.0000],\n",
      "        [0.0000, 0.0000, 5.2428, 6.0000, 6.0000],\n",
      "        [0.0000, 0.5822, 6.0000, 6.0000, 6.0000]], grad_fn=<HardtanhBackward0>)\n",
      "tensor([[6.0000, 6.0000, 6.0000, 2.0382, 0.0000],\n",
      "        [6.0000, 6.0000, 6.0000, 2.2699, 0.0000],\n",
      "        [6.0000, 6.0000, 6.0000, 2.5186, 0.0000]], grad_fn=<HardtanhBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.6362, 0.3397, 0.0000],\n",
       "        [0.0000, 0.0000, 0.8738, 0.3783, 0.0000],\n",
       "        [0.0000, 0.0970, 1.0000, 0.4198, 0.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid())\n",
    "x = m(torch.rand(3, 1))\n",
    "print(x)\n",
    "\n",
    "x1 = x[:, 0].view(-1, 1)*5\n",
    "x2 = x[:, 1].view(-1, 1)*5\n",
    "\n",
    "_x1 = F.relu6((torch.arange(5).expand(3,-1).float()-x1) * 6)\n",
    "_x2 = F.relu6((x2-torch.arange(5).float()) * 6)\n",
    "print(_x1)\n",
    "print(_x2)\n",
    "y = _x1 * _x2\n",
    "y = y / 36.\n",
    "y\n",
    "# y = y / 36.\n",
    "# print(_x1)\n",
    "# _x2 = F.relu6((x2-torch.arange(5).float()) * 6)\n",
    "# print(_x2)\n",
    "# y = _x1 * _x2\n",
    "# y = y / 36.\n",
    "\n",
    "# torch.arange(5).expand(3,-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 5.4000, 6.0000, 6.0000, 6.0000], grad_fn=<HardtanhBackward0>)\n",
      "tensor([6.0000, 6.0000, 6.0000, 0.6000, 0.0000], grad_fn=<HardtanhBackward0>)\n",
      "tensor([0.0000, 0.9000, 1.0000, 0.1000, 0.0000], grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 0.9000, 1.0000, 0.1000, 0.0000], grad_fn=<DivBackward0>)\n",
      "tensor(-99.6000, grad_fn=<MeanBackward0>)\n",
      "tensor([-0.2000])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor([2.1, 2.1, 2.1, 2.1, 2.1], requires_grad=True)\n",
    "x1 = torch.tensor([0.1], requires_grad=True)\n",
    "x2 = torch.tensor([3.1], requires_grad=True)\n",
    "gt = torch.tensor([100.])\n",
    "\n",
    "_x1 = F.relu6((torch.arange(5).float()-x1) * 6)\n",
    "print(_x1)\n",
    "_x2 = F.relu6((x2-torch.arange(5).float()) * 6)\n",
    "print(_x2)\n",
    "y = _x1 * _x2\n",
    "y = y / 36.\n",
    "# y = y - torch.tensor([1., 1., 2., 0, 0])\n",
    "# y = y - torch.arange(5).float()\n",
    "print(y)\n",
    "# y = x * 100\n",
    "# y = y * 100\n",
    "# y = torch.arange(10).float() - y\n",
    "# y = y.div(y + 1e-6)\n",
    "# y = y / (y + .0001)\n",
    "# y = y * y * y\n",
    "\n",
    "loss = (y - gt).mean()\n",
    "loss.backward()\n",
    "print(y)\n",
    "print(loss)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3., 4.]), tensor([[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = Faker()\n",
    "\n",
    "img_size = 5\n",
    "index = fake.pyint(min=0, max=img_size-1)\n",
    "F.one_hot(torch.tensor(index), img_size)\n",
    "torch.arange(0,img_size)\n",
    "\n",
    "def sampler():\n",
    "    x, y = [], []\n",
    "    for i in range(img_size):\n",
    "        x.append(torch.tensor(i))\n",
    "        y.append(F.one_hot(torch.tensor(i),img_size))\n",
    "    x = torch.stack(x).float()\n",
    "    y = torch.stack(y).float()\n",
    "    return x, y\n",
    "\n",
    "sampler()\n",
    "\n",
    "\n",
    "\n",
    "# a = torch.tensor([3.],  requires_grad=True)\n",
    "# y = a.expand(int(a[0]))\n",
    "# gt = torch.tensor([1., 1., 1.])\n",
    "\n",
    "# b = torch.tensor([2., 2.],  requires_grad=True)\n",
    "# (x > 1) & (x > 2)\n",
    "# a = x.add(1e-8)\n",
    "# a = x/x\n",
    "# y = x2.sum()\n",
    "# y.backward()\n",
    "# print(x.grad)\n",
    "# x\n",
    "\n",
    "# c = torch.cat((a,b), 0)\n",
    "# y = c.mean()\n",
    "# loss = (gt - y).mean()\n",
    "# loss.backward()\n",
    "\n",
    "# print(y)\n",
    "# print(loss)\n",
    "# print(a.grad)\n",
    "\n",
    "# y = a * 10\n",
    "# y.backward()\n",
    "# print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,088\n",
      "         LeakyReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 16, 16]         131,200\n",
      "    InstanceNorm2d-4          [-1, 128, 16, 16]               0\n",
      "         LeakyReLU-5          [-1, 128, 16, 16]               0\n",
      "            Conv2d-6            [-1, 256, 8, 8]         524,544\n",
      "    InstanceNorm2d-7            [-1, 256, 8, 8]               0\n",
      "         LeakyReLU-8            [-1, 256, 8, 8]               0\n",
      "            Conv2d-9            [-1, 512, 4, 4]       2,097,664\n",
      "   InstanceNorm2d-10            [-1, 512, 4, 4]               0\n",
      "        LeakyReLU-11            [-1, 512, 4, 4]               0\n",
      "        ZeroPad2d-12            [-1, 512, 5, 5]               0\n",
      "           Conv2d-13              [-1, 1, 4, 4]           8,192\n",
      "================================================================\n",
      "Total params: 2,762,688\n",
      "Trainable params: 2,762,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.41\n",
      "Params size (MB): 10.54\n",
      "Estimated Total Size (MB): 12.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "#         img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img)\n",
    "\n",
    "\n",
    "d = Discriminator()\n",
    "\n",
    "summary(d, (1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_real_samples(n_sample=100, save_path=\"data/layout/\"):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    width, height = 28, 28\n",
    "    fake = Faker()\n",
    "\n",
    "    def _sample(save_path):\n",
    "        n_circles = fake.pyint(min=1, max=3)\n",
    "        radius = fake.pyint(min=2, max=5)\n",
    "        space = fake.pyint(min=2, max=4)\n",
    "        x0 = fake.pyint(min=0, max=width - 1 - (n_circles * (radius + space)))\n",
    "        y0 = fake.pyint(min=0, max=height - 1 - radius)\n",
    "\n",
    "        im = Image.new(\"L\", (width, height))\n",
    "        draw = ImageDraw.Draw(im)\n",
    "\n",
    "        for _ in range(n_circles):\n",
    "            draw.rectangle((x0, y0, x0 + radius, y0 + radius), fill=255)\n",
    "            x0 += radius + space\n",
    "#         plt.imshow(im)\n",
    "#         im.save(save_path, \"PNG\")\n",
    "\n",
    "    for i in range(n_sample):\n",
    "        _sample(os.path.join(save_path, \"%d.png\" % (i)))\n",
    "\n",
    "generate_real_samples(n_sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f191f640208>"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = Image.open(\"data/0.png\")\n",
    "plt.imshow(np.array(im))\n",
    "# im.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAABACAIAAABdtOgoAAAFSElEQVR4nO2aSyi0XxjAz9xeGWM+FAvJ/ZaaiCJFLk0uiSIsbJCGDaWIkJRy2UluZWNDyCUbLIRxt3DZWLiUGLcwjeuMGYzzLU4d099n5vX/PnNenN9m3vd5n6d+c2bOO+c57wBAoVAoFAqFQqFQKBSOsbq6CiGEEO7v7/N4PNI6P4zAwEBoQkxMDGmj7wbf/OW8vDwzp5TPhc/nHx8fm86Au7s7Ozs70l4/hqSkJDTuLy8vBoMBHefm5pL2+jEMDAygQZ+fnx8aGkLHs7OzpL1+Br9+/Xp4eECDrlAo0tLS8Gzw9PQkbccWb2/v+vr65eXltrY20i4fpKioCI24Xq93cHAQiUSXl5coUldXR9qOLYODg3gNTdrlg6ysrCD1oaEhFGlra/taDYGjo6Ner8crCD8/P9JGrAkICMDeaWlpKBgeHv61GoLi4mLTJVxJSQlpI9Y0NTUhabVaLRKJcHx7exvFe3p6COqxZGNjA0I4OTmJnMfHx0kbsYPP5x8dHSHp9vZ200s1NTVfpSEIDg6GEG5uboaEhCBnrVZrY2ND2osFiYmJeNpGRESYXvLw8Hh5efkSDUFrayuEsLy8HABweHiInBMSEkh7saC/vx/p7u7uvr2qVCq53xAwDKNWq41Go5ubGwCgo6MDObe2tpJWs4Tp8r+2tvZtQkFBAfcbgszMTAjhzMwMOsUt/d7eHlkxyxQWFuLx9fLyepsglUp1Oh3HG4KJiQkIYUFBATq1sbG5u7v7GovR5eVlJLqwsPBeDt6i4GZD4Orq+vz8jPpHHBwdHUXOpaWlxMws4u/vj39+CwsL30tLSUnhckNQVVUFIRwZGTEN5ufnI+GpqSlSYpZpbGyEH4SDDcHOzg6EMCMjwzTo7OxsNBohhAaDwd7enpSbOUyX/+zhWkMQHR0NIby+vn675Md316ysLCJuf+T1iZhcLkeLtg8hkUgyMzP/qdJfkZ+fDwAYHh42GAymcbFYvLi4iI7x5goXeP0J7evry8nJAQBcXV3JZDKj0Wi+cnFx0cfHBwCgVCrj4uI+1ZIlEonk7OxMIpFotVq9Xo+CQqHQ1taWYRicptFoXFxcLL5B6yBEL1KpND09HR2PjY2dnJxYrBwcHKyurgYAxMTEeHp6HhwcfJokW7KzsyUSCQCAx+NpNBqNRqPT6R4fH3k8Ho/HE4lEoaGhUqnUyckpKipqbm6OtK8JCoUC39YTExPZlMhkMlzCkYZgYWEBQtjc3PxeAt5Rb2lpsaaYZZaWlpCZWq0WCoUsq7a2tlAVFxoCPz8/JBMUFPReTkJCAsrhwnx9BatDCLu7u9kX1tbW4kLiDQHaQt/c3DSTwzDM7e0tEg4LC7OamwUaGhrwOMrlcvaFvr6+HGkIBALByckJhLCsrMx85vDwMBJubGy0jpsF+Hy+SqVCThcXFwKB4EPl6+vrXGgI0tPTkYa7u7v5TLyZuLe3R/y2CQAAcrkcf4u7uro+Wl5eXo7LST0hEAgEa2trEMLj42OLyehBDSI1NdUKet8chmE6OzvRgOp0OmdnZ/P5GRkZ+AO4uLiIj4+3jud7cGAO/h03NzdSqRSfnp+fK5XK6+trAMD09DT+S0dWVlZkZKRMJouNjf3PMu/09LS3t7eystKK1t8I+D4VFRU4Df/J/o/8j3vvv+LLzwBHR0eDwfD8/Pz09MQwjK2trVgstrOzc3JyUqlUZ2dnKC05OVmv15+fn9/f32u1Wq1W+/j4yDAMwzBisfjh4eHm5obsG6FQKBQKhUKhUCgUCoVCoVAoFAqFQqFQKBQKhUL5VvwGA2JFEU0GJQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x64 at 0x7F1928631DD8>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = \"A\"\n",
    "width = 64\n",
    "fonts = ['/notebooks/post-generator/asset/fonts_en/Roboto/Roboto-Regular.ttf', \n",
    "            '/notebooks/post-generator/asset/fonts_en/Alegreya/Alegreya-Regular.ttf']\n",
    "\n",
    "im = Image.new(\"RGB\", (width * 2, width))\n",
    "\n",
    "for i, fnt in enumerate(fonts):\n",
    "    _im = Image.new(\"RGB\", (width, width))\n",
    "    _draw = ImageDraw.Draw(_im)\n",
    "    _font = ImageFont.truetype(fnt, 40)\n",
    "    _w, _h = _draw.textsize(char, _font)  # size of token\n",
    "#     _draw.text(((width - _w) / 2, (width - _h) / 2), char, font=_font, fill=(255, 255, 255))\n",
    "    _draw.text((0, 0), char, font=_font, fill=(255, 255, 255))\n",
    "    im.paste(_im, (i * width, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f191f6c20b8>"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "width, height = 64, 64\n",
    "im = Image.new(\"L\", (width, width))\n",
    "\n",
    "draw = ImageDraw.Draw(im)\n",
    "n_circles = 3\n",
    "radius, space = 10, 3\n",
    "x0, y0 = 10, 10\n",
    "for i in range(n_circles):\n",
    "#     draw.ellipse((x0, y0, x0+radius, y0+radius), fill=1)\n",
    "    draw.rectangle((x0, y0, x0+radius, y0+radius), fill=1)\n",
    "    x0 += radius + space\n",
    "plt.imshow(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AddCoords-1           [-1, 22, 16, 16]               0\n",
      "            Conv2d-2          [-1, 256, 16, 16]           5,888\n",
      "       BatchNorm2d-3          [-1, 256, 16, 16]             512\n",
      "              ReLU-4          [-1, 256, 16, 16]               0\n",
      "            Conv2d-5          [-1, 256, 16, 16]          65,792\n",
      "       BatchNorm2d-6          [-1, 256, 16, 16]             512\n",
      "              ReLU-7          [-1, 256, 16, 16]               0\n",
      "            Linear-8                    [-1, 3]         196,611\n",
      "            Conv2d-9          [-1, 256, 16, 16]           5,888\n",
      "      BatchNorm2d-10          [-1, 256, 16, 16]             512\n",
      "             ReLU-11          [-1, 256, 16, 16]               0\n",
      "           Conv2d-12          [-1, 256, 16, 16]          65,792\n",
      "      BatchNorm2d-13          [-1, 256, 16, 16]             512\n",
      "             ReLU-14          [-1, 256, 16, 16]               0\n",
      "           Linear-15                    [-1, 4]         262,148\n",
      "================================================================\n",
      "Total params: 604,167\n",
      "Trainable params: 604,167\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 6.04\n",
      "Params size (MB): 2.30\n",
      "Estimated Total Size (MB): 8.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel, width, latent_size=256, n_class=3, bbox_size=4):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv_class = nn.Sequential(\n",
    "            nn.Conv2d(in_channel + 2, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_size, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(latent_size, n_class, 1, stride=1, padding=0),\n",
    "            # nn.Linear(latent_size, n_class, 1, stride=1, padding=0),\n",
    "        )\n",
    "        self.conv_bbox = nn.Sequential(\n",
    "            nn.Conv2d(in_channel + 2, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_size, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(latent_size, bbox_dim, 1, stride=1, padding=0),\n",
    "        )\n",
    "        self.fc_class = nn.Linear((width ** 2) * latent_size, n_class)\n",
    "        self.fc_bbox = nn.Linear((width ** 2) * latent_size, bbox_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        x = self.add_coords(x)\n",
    "        x_class = self.conv_class(x)\n",
    "        x_class = x_class.view(N, -1)\n",
    "        x_class = self.fc_class(x_class)\n",
    "        x_bbox = self.conv_bbox(x)\n",
    "        x_bbox = x_bbox.view(N, -1)\n",
    "        x_bbox = self.fc_bbox(x_bbox)\n",
    "        return x_class, x_bbox\n",
    "    \n",
    "m = Regressor(in_channel=20, width=16)\n",
    "summary(m, (20, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HRNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(HRNet, self).__init__()\n",
    "        cfg = get_cfg_defaults()\n",
    "        cfg.merge_from_file(\"./exp.yaml\")\n",
    "        self.hr = HighResolutionNet(cfg)\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv5 = nn.Conv2d(7, 7, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(7, 7, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(7, 4, 1)\n",
    "        self.pool = nn.MaxPool2d(width, stride=width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.hr(x)\n",
    "        x1 = F.interpolate(x1, scale_factor=4)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "m = HRNet(width=64)\n",
    "# summary(m, (3, 64, 64))\n",
    "x = m(torch.rand(1,1,64,64))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 200, 200]          18,816\n",
      "       BatchNorm2d-2        [-1, 128, 200, 200]             256\n",
      "              ReLU-3        [-1, 128, 200, 200]               0\n",
      "       convolution-4        [-1, 128, 200, 200]               0\n",
      "            Conv2d-5        [-1, 256, 100, 100]         294,912\n",
      "       BatchNorm2d-6        [-1, 256, 100, 100]             512\n",
      "              ReLU-7        [-1, 256, 100, 100]               0\n",
      "            Conv2d-8        [-1, 256, 100, 100]         589,824\n",
      "       BatchNorm2d-9        [-1, 256, 100, 100]             512\n",
      "           Conv2d-10        [-1, 256, 100, 100]          32,768\n",
      "      BatchNorm2d-11        [-1, 256, 100, 100]             512\n",
      "             ReLU-12        [-1, 256, 100, 100]               0\n",
      "         residual-13        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 938,112\n",
      "Trainable params: 938,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 332.03\n",
      "Params size (MB): 3.58\n",
      "Estimated Total Size (MB): 337.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from models.CornerNet import model\n",
    "\n",
    "# m = model()\n",
    "\n",
    "class convolution(nn.Module):\n",
    "    def __init__(self, k, inp_dim, out_dim, stride=1, with_bn=True):\n",
    "        super(convolution, self).__init__()\n",
    "\n",
    "        pad = (k - 1) // 2\n",
    "        self.conv = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(pad, pad), stride=(stride, stride), bias=not with_bn)\n",
    "        self.bn   = nn.BatchNorm2d(out_dim) if with_bn else nn.Sequential()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        bn   = self.bn(conv)\n",
    "        relu = self.relu(bn)\n",
    "        return relu\n",
    "\n",
    "class residual(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, k=3, stride=1):\n",
    "        super(residual, self).__init__()\n",
    "        p = (k - 1) // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(p, p), stride=(stride, stride), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, (k, k), padding=(p, p), bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_dim)\n",
    "        \n",
    "        self.skip  = nn.Sequential(\n",
    "            nn.Conv2d(inp_dim, out_dim, (1, 1), stride=(stride, stride), bias=False),\n",
    "            nn.BatchNorm2d(out_dim)\n",
    "        ) if stride != 1 or inp_dim != out_dim else nn.Sequential()\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "\n",
    "        skip  = self.skip(x)\n",
    "        return self.relu(bn2 + skip)\n",
    "\n",
    "\n",
    "pre = nn.Sequential(\n",
    "            convolution(7, 3, 128, stride=2), residual(128, 256, stride=2)\n",
    "        )\n",
    "# pre(torch.from_numpy(np.random.rand(1, 3, 128, 128)).double())\n",
    "# conv = convolution(7, 3, 128, stride=2)\n",
    "x = torch.rand(1, 3, 512, 512)\n",
    "x = pre(x)\n",
    "# x.shape\n",
    "\n",
    "summary(pre, (3, 400, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-2        [-1, 128, 256, 256]             256\n",
      "            Conv2d-3        [-1, 128, 256, 256]          16,384\n",
      "            Conv2d-4        [-1, 128, 256, 256]           1,152\n",
      "       BatchNorm2d-5        [-1, 256, 256, 256]             512\n",
      "              ReLU-6        [-1, 256, 256, 256]               0\n",
      "       fire_module-7        [-1, 256, 256, 256]               0\n",
      "            Conv2d-8        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-9        [-1, 128, 256, 256]             256\n",
      "           Conv2d-10        [-1, 128, 256, 256]          16,384\n",
      "           Conv2d-11        [-1, 128, 256, 256]           1,152\n",
      "      BatchNorm2d-12        [-1, 256, 256, 256]             512\n",
      "             ReLU-13        [-1, 256, 256, 256]               0\n",
      "      fire_module-14        [-1, 256, 256, 256]               0\n",
      "           Conv2d-15        [-1, 128, 256, 256]          32,768\n",
      "      BatchNorm2d-16        [-1, 128, 256, 256]             256\n",
      "           Conv2d-17        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-18        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-19        [-1, 256, 128, 128]             512\n",
      "             ReLU-20        [-1, 256, 128, 128]               0\n",
      "      fire_module-21        [-1, 256, 128, 128]               0\n",
      "           Conv2d-22        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-23        [-1, 128, 128, 128]             256\n",
      "           Conv2d-24        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-25        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-26        [-1, 256, 128, 128]             512\n",
      "             ReLU-27        [-1, 256, 128, 128]               0\n",
      "      fire_module-28        [-1, 256, 128, 128]               0\n",
      "           Conv2d-29        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-30        [-1, 128, 128, 128]             256\n",
      "           Conv2d-31        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-32        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-33        [-1, 256, 128, 128]             512\n",
      "             ReLU-34        [-1, 256, 128, 128]               0\n",
      "      fire_module-35        [-1, 256, 128, 128]               0\n",
      "           Conv2d-36        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-37        [-1, 128, 128, 128]             256\n",
      "           Conv2d-38        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-39        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-40        [-1, 256, 128, 128]             512\n",
      "             ReLU-41        [-1, 256, 128, 128]               0\n",
      "      fire_module-42        [-1, 256, 128, 128]               0\n",
      "           Conv2d-43        [-1, 192, 128, 128]          49,152\n",
      "      BatchNorm2d-44        [-1, 192, 128, 128]             384\n",
      "           Conv2d-45          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-46          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-47          [-1, 384, 64, 64]             768\n",
      "             ReLU-48          [-1, 384, 64, 64]               0\n",
      "      fire_module-49          [-1, 384, 64, 64]               0\n",
      "           Conv2d-50          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-51          [-1, 192, 64, 64]             384\n",
      "           Conv2d-52          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-53          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-54          [-1, 384, 64, 64]             768\n",
      "             ReLU-55          [-1, 384, 64, 64]               0\n",
      "      fire_module-56          [-1, 384, 64, 64]               0\n",
      "           Conv2d-57          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-58          [-1, 192, 64, 64]             384\n",
      "           Conv2d-59          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-60          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-61          [-1, 384, 64, 64]             768\n",
      "             ReLU-62          [-1, 384, 64, 64]               0\n",
      "      fire_module-63          [-1, 384, 64, 64]               0\n",
      "           Conv2d-64          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-65          [-1, 192, 64, 64]             384\n",
      "           Conv2d-66          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-67          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-68          [-1, 384, 64, 64]             768\n",
      "             ReLU-69          [-1, 384, 64, 64]               0\n",
      "      fire_module-70          [-1, 384, 64, 64]               0\n",
      "           Conv2d-71          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-72          [-1, 192, 64, 64]             384\n",
      "           Conv2d-73          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-74          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-75          [-1, 384, 32, 32]             768\n",
      "             ReLU-76          [-1, 384, 32, 32]               0\n",
      "      fire_module-77          [-1, 384, 32, 32]               0\n",
      "           Conv2d-78          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-79          [-1, 192, 32, 32]             384\n",
      "           Conv2d-80          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-81          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-82          [-1, 384, 32, 32]             768\n",
      "             ReLU-83          [-1, 384, 32, 32]               0\n",
      "      fire_module-84          [-1, 384, 32, 32]               0\n",
      "           Conv2d-85          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-86          [-1, 192, 32, 32]             384\n",
      "           Conv2d-87          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-88          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-89          [-1, 384, 32, 32]             768\n",
      "             ReLU-90          [-1, 384, 32, 32]               0\n",
      "      fire_module-91          [-1, 384, 32, 32]               0\n",
      "           Conv2d-92          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-93          [-1, 192, 32, 32]             384\n",
      "           Conv2d-94          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-95          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-96          [-1, 384, 32, 32]             768\n",
      "             ReLU-97          [-1, 384, 32, 32]               0\n",
      "      fire_module-98          [-1, 384, 32, 32]               0\n",
      "           Conv2d-99          [-1, 256, 32, 32]          98,304\n",
      "     BatchNorm2d-100          [-1, 256, 32, 32]             512\n",
      "          Conv2d-101          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-102          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-103          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-104          [-1, 512, 16, 16]               0\n",
      "     fire_module-105          [-1, 512, 16, 16]               0\n",
      "          Conv2d-106          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-107          [-1, 256, 16, 16]             512\n",
      "          Conv2d-108          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-109          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-110          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-111          [-1, 512, 16, 16]               0\n",
      "     fire_module-112          [-1, 512, 16, 16]               0\n",
      "          Conv2d-113          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-114          [-1, 256, 16, 16]             512\n",
      "          Conv2d-115          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-116          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-117          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-118          [-1, 512, 16, 16]               0\n",
      "     fire_module-119          [-1, 512, 16, 16]               0\n",
      "          Conv2d-120          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-121          [-1, 256, 16, 16]             512\n",
      "          Conv2d-122          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-123          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-124          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-125          [-1, 512, 16, 16]               0\n",
      "     fire_module-126          [-1, 512, 16, 16]               0\n",
      "          Conv2d-127          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-128          [-1, 256, 16, 16]             512\n",
      "          Conv2d-129          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-130          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-131          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-132          [-1, 512, 16, 16]               0\n",
      "     fire_module-133          [-1, 512, 16, 16]               0\n",
      "          Conv2d-134          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-135          [-1, 256, 16, 16]             512\n",
      "          Conv2d-136          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-137          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-139          [-1, 512, 16, 16]               0\n",
      "     fire_module-140          [-1, 512, 16, 16]               0\n",
      "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
      "          Conv2d-143          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-144          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-145          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-146          [-1, 512, 16, 16]               0\n",
      "     fire_module-147          [-1, 512, 16, 16]               0\n",
      "          Conv2d-148          [-1, 192, 16, 16]          98,304\n",
      "     BatchNorm2d-149          [-1, 192, 16, 16]             384\n",
      "          Conv2d-150          [-1, 192, 16, 16]          36,864\n",
      "          Conv2d-151          [-1, 192, 16, 16]           1,728\n",
      "     BatchNorm2d-152          [-1, 384, 16, 16]             768\n",
      "            ReLU-153          [-1, 384, 16, 16]               0\n",
      "     fire_module-154          [-1, 384, 16, 16]               0\n",
      " ConvTranspose2d-155          [-1, 384, 32, 32]       2,359,680\n",
      "           merge-156          [-1, 384, 32, 32]               0\n",
      "       hg_module-157          [-1, 384, 32, 32]               0\n",
      "          Conv2d-158          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-159          [-1, 192, 32, 32]             384\n",
      "          Conv2d-160          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-161          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-162          [-1, 384, 32, 32]             768\n",
      "            ReLU-163          [-1, 384, 32, 32]               0\n",
      "     fire_module-164          [-1, 384, 32, 32]               0\n",
      "          Conv2d-165          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-166          [-1, 192, 32, 32]             384\n",
      "          Conv2d-167          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-168          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-169          [-1, 384, 32, 32]             768\n",
      "            ReLU-170          [-1, 384, 32, 32]               0\n",
      "     fire_module-171          [-1, 384, 32, 32]               0\n",
      " ConvTranspose2d-172          [-1, 384, 64, 64]       2,359,680\n",
      "           merge-173          [-1, 384, 64, 64]               0\n",
      "       hg_module-174          [-1, 384, 64, 64]               0\n",
      "          Conv2d-175          [-1, 192, 64, 64]          73,728\n",
      "     BatchNorm2d-176          [-1, 192, 64, 64]             384\n",
      "          Conv2d-177          [-1, 192, 64, 64]          36,864\n",
      "          Conv2d-178          [-1, 192, 64, 64]           1,728\n",
      "     BatchNorm2d-179          [-1, 384, 64, 64]             768\n",
      "            ReLU-180          [-1, 384, 64, 64]               0\n",
      "     fire_module-181          [-1, 384, 64, 64]               0\n",
      "          Conv2d-182          [-1, 128, 64, 64]          49,152\n",
      "     BatchNorm2d-183          [-1, 128, 64, 64]             256\n",
      "          Conv2d-184          [-1, 128, 64, 64]          16,384\n",
      "          Conv2d-185          [-1, 128, 64, 64]           1,152\n",
      "     BatchNorm2d-186          [-1, 256, 64, 64]             512\n",
      "            ReLU-187          [-1, 256, 64, 64]               0\n",
      "     fire_module-188          [-1, 256, 64, 64]               0\n",
      " ConvTranspose2d-189        [-1, 256, 128, 128]       1,048,832\n",
      "           merge-190        [-1, 256, 128, 128]               0\n",
      "       hg_module-191        [-1, 256, 128, 128]               0\n",
      "          Conv2d-192        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-193        [-1, 128, 128, 128]             256\n",
      "          Conv2d-194        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-195        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-196        [-1, 256, 128, 128]             512\n",
      "            ReLU-197        [-1, 256, 128, 128]               0\n",
      "     fire_module-198        [-1, 256, 128, 128]               0\n",
      "          Conv2d-199        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-200        [-1, 128, 128, 128]             256\n",
      "          Conv2d-201        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-202        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-203        [-1, 256, 128, 128]             512\n",
      "            ReLU-204        [-1, 256, 128, 128]               0\n",
      "     fire_module-205        [-1, 256, 128, 128]               0\n",
      " ConvTranspose2d-206        [-1, 256, 256, 256]       1,048,832\n",
      "           merge-207        [-1, 256, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 10,025,088\n",
      "Trainable params: 10,025,088\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 64.00\n",
      "Forward/backward pass size (MB): 3249.75\n",
      "Params size (MB): 38.24\n",
      "Estimated Total Size (MB): 3351.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class upsample(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.interpolate(x, scale_factor=self.scale_factor)\n",
    "\n",
    "class merge(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "def _make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, out_dim)]\n",
    "    layers += [residual(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [residual(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_pool_layer(dim):\n",
    "    return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "def _make_unpool_layer(dim):\n",
    "    return upsample(scale_factor=2)\n",
    "\n",
    "def _make_merge_layer(dim):\n",
    "    return merge()\n",
    "\n",
    "class fire_module(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, sr=2, stride=1):\n",
    "        super(fire_module, self).__init__()\n",
    "        self.conv1    = nn.Conv2d(inp_dim, out_dim // sr, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1      = nn.BatchNorm2d(out_dim // sr)\n",
    "        self.conv_1x1 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=1, stride=stride, bias=False)\n",
    "        self.conv_3x3 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=3, padding=1, \n",
    "                                  stride=stride, groups=out_dim // sr, bias=False)\n",
    "        self.bn2      = nn.BatchNorm2d(out_dim)\n",
    "        self.skip     = (stride == 1 and inp_dim == out_dim)\n",
    "        self.relu     = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        conv2 = torch.cat((self.conv_1x1(bn1), self.conv_3x3(bn1)), 1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "        if self.skip:\n",
    "            return self.relu(bn2 + x)\n",
    "        else:\n",
    "            return self.relu(bn2)\n",
    "\n",
    "class hg_module(nn.Module):\n",
    "    def __init__(\n",
    "        self, n, dims, modules, make_up_layer=_make_layer,\n",
    "        make_pool_layer=_make_pool_layer, make_hg_layer=_make_layer,\n",
    "        make_low_layer=_make_layer, make_hg_layer_revr=_make_layer_revr,\n",
    "        make_unpool_layer=_make_unpool_layer, make_merge_layer=_make_merge_layer\n",
    "    ):\n",
    "        super(hg_module, self).__init__()\n",
    "\n",
    "        curr_mod = modules[0]\n",
    "        next_mod = modules[1]\n",
    "\n",
    "        curr_dim = dims[0]\n",
    "        next_dim = dims[1]\n",
    "\n",
    "        self.n    = n\n",
    "        self.up1  = make_up_layer(curr_dim, curr_dim, curr_mod)\n",
    "        self.max1 = make_pool_layer(curr_dim)\n",
    "        self.low1 = make_hg_layer(curr_dim, next_dim, curr_mod)\n",
    "        self.low2 = hg_module(\n",
    "            n - 1, dims[1:], modules[1:],\n",
    "            make_up_layer=make_up_layer,\n",
    "            make_pool_layer=make_pool_layer,\n",
    "            make_hg_layer=make_hg_layer,\n",
    "            make_low_layer=make_low_layer,\n",
    "            make_hg_layer_revr=make_hg_layer_revr,\n",
    "            make_unpool_layer=make_unpool_layer,\n",
    "            make_merge_layer=make_merge_layer\n",
    "        ) if n > 1 else make_low_layer(next_dim, next_dim, next_mod)\n",
    "        self.low3 = make_hg_layer_revr(next_dim, curr_dim, curr_mod)\n",
    "        self.up2  = make_unpool_layer(curr_dim)\n",
    "        self.merg = make_merge_layer(curr_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        up1  = self.up1(x)\n",
    "        max1 = self.max1(x)\n",
    "        low1 = self.low1(max1)\n",
    "        low2 = self.low2(low1)\n",
    "        low3 = self.low3(low2)\n",
    "        up2  = self.up2(low3)\n",
    "        print(up1.shape)\n",
    "        print(up2.shape)\n",
    "        merg = self.merg(up1, up2)\n",
    "        return merg\n",
    "#         return up1\n",
    "\n",
    "    \n",
    "def make_pool_layer(dim):\n",
    "    return nn.Sequential()\n",
    "\n",
    "def make_unpool_layer(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "def make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [fire_module(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_hg_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim, stride=2)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "m = hg_module(\n",
    "                4, [256, 256, 384, 384, 512], [2, 2, 2, 2, 4],\n",
    "                make_pool_layer=make_pool_layer,\n",
    "                make_unpool_layer=make_unpool_layer,\n",
    "                make_up_layer=make_layer,\n",
    "                make_low_layer=make_layer,\n",
    "                make_hg_layer_revr=make_layer_revr,\n",
    "                make_hg_layer=make_hg_layer\n",
    "            )\n",
    "\n",
    "summary(m, (256, 256, 256))\n",
    "# m(torch.rand(1, 256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 256, 200, 200]          65,792\n",
      "              ReLU-2        [-1, 256, 200, 200]               0\n",
      "       convolution-3        [-1, 256, 200, 200]               0\n",
      "            Conv2d-4         [-1, 80, 200, 200]          20,560\n",
      "================================================================\n",
      "Total params: 86,352\n",
      "Trainable params: 86,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.06\n",
      "Forward/backward pass size (MB): 258.79\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 298.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def _pred_mod(dim):\n",
    "        return nn.Sequential(\n",
    "            convolution(1, 256, 256, with_bn=False),\n",
    "            nn.Conv2d(256, dim, (1, 1))\n",
    "        )\n",
    "m = _pred_mod(80)\n",
    "summary(m, (256, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-2        [-1, 128, 100, 100]             256\n",
      "              ReLU-3        [-1, 128, 100, 100]               0\n",
      "       convolution-4        [-1, 128, 100, 100]               0\n",
      "           TopPool-5        [-1, 128, 100, 100]               0\n",
      "            Conv2d-6        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-7        [-1, 128, 100, 100]             256\n",
      "              ReLU-8        [-1, 128, 100, 100]               0\n",
      "       convolution-9        [-1, 128, 100, 100]               0\n",
      "         LeftPool-10        [-1, 128, 100, 100]               0\n",
      "           Conv2d-11        [-1, 256, 100, 100]         294,912\n",
      "      BatchNorm2d-12        [-1, 256, 100, 100]             512\n",
      "           Conv2d-13        [-1, 256, 100, 100]          65,536\n",
      "      BatchNorm2d-14        [-1, 256, 100, 100]             512\n",
      "             ReLU-15        [-1, 256, 100, 100]               0\n",
      "           Conv2d-16        [-1, 256, 100, 100]         589,824\n",
      "      BatchNorm2d-17        [-1, 256, 100, 100]             512\n",
      "             ReLU-18        [-1, 256, 100, 100]               0\n",
      "      convolution-19        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 1,542,144\n",
      "Trainable params: 1,542,144\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9.77\n",
      "Forward/backward pass size (MB): 273.44\n",
      "Params size (MB): 5.88\n",
      "Estimated Total Size (MB): 289.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models.py_utils import TopPool, LeftPool\n",
    "\n",
    "class corner_pool(nn.Module):\n",
    "    def __init__(self, dim, pool1, pool2):\n",
    "        super(corner_pool, self).__init__()\n",
    "        self._init_layers(dim, pool1, pool2)\n",
    "\n",
    "    def _init_layers(self, dim, pool1, pool2):\n",
    "        self.p1_conv1 = convolution(3, dim, 128)\n",
    "        self.p2_conv1 = convolution(3, dim, 128)\n",
    "\n",
    "        self.p_conv1 = nn.Conv2d(128, dim, (3, 3), padding=(1, 1), bias=False)\n",
    "        self.p_bn1   = nn.BatchNorm2d(dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (1, 1), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = convolution(3, dim, dim)\n",
    "\n",
    "        self.pool1 = pool1()\n",
    "        self.pool2 = pool2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pool 1\n",
    "        p1_conv1 = self.p1_conv1(x)\n",
    "        pool1    = self.pool1(p1_conv1)\n",
    "\n",
    "        # pool 2\n",
    "        p2_conv1 = self.p2_conv1(x)\n",
    "        pool2    = self.pool2(p2_conv1)\n",
    "\n",
    "        # pool 1 + pool 2\n",
    "        p_conv1 = self.p_conv1(pool1 + pool2)\n",
    "        p_bn1   = self.p_bn1(p_conv1)\n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(p_bn1 + bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        return conv2\n",
    "\n",
    "m = corner_pool(256, TopPool, LeftPool)\n",
    "summary(m, (256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AddCoords-1              [-1, 3, 3, 3]               0\n",
      "            Conv2d-2              [-1, 8, 3, 3]             224\n",
      "            Conv2d-3             [-1, 16, 3, 3]           1,168\n",
      "            Conv2d-4             [-1, 32, 3, 3]           4,640\n",
      "            Conv2d-5            [-1, 128, 3, 3]          36,864\n",
      "       BatchNorm2d-6            [-1, 128, 3, 3]             256\n",
      "              ReLU-7            [-1, 128, 3, 3]               0\n",
      "       convolution-8            [-1, 128, 3, 3]               0\n",
      "           TopPool-9            [-1, 128, 3, 3]               0\n",
      "           Conv2d-10            [-1, 128, 3, 3]          36,864\n",
      "      BatchNorm2d-11            [-1, 128, 3, 3]             256\n",
      "             ReLU-12            [-1, 128, 3, 3]               0\n",
      "      convolution-13            [-1, 128, 3, 3]               0\n",
      "         LeftPool-14            [-1, 128, 3, 3]               0\n",
      "           Conv2d-15             [-1, 32, 3, 3]          36,864\n",
      "      BatchNorm2d-16             [-1, 32, 3, 3]              64\n",
      "           Conv2d-17             [-1, 32, 3, 3]           1,024\n",
      "      BatchNorm2d-18             [-1, 32, 3, 3]              64\n",
      "             ReLU-19             [-1, 32, 3, 3]               0\n",
      "           Conv2d-20             [-1, 32, 3, 3]           9,216\n",
      "      BatchNorm2d-21             [-1, 32, 3, 3]              64\n",
      "             ReLU-22             [-1, 32, 3, 3]               0\n",
      "      convolution-23             [-1, 32, 3, 3]               0\n",
      "      corner_pool-24             [-1, 32, 3, 3]               0\n",
      "           Conv2d-25              [-1, 1, 3, 3]              33\n",
      "           Conv2d-26              [-1, 1, 3, 3]               2\n",
      "================================================================\n",
      "Total params: 127,603\n",
      "Trainable params: 127,603\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.49\n",
      "Estimated Total Size (MB): 0.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.width = width\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = corner_pool(32, TopPool, LeftPool)\n",
    "        self.conv4 = nn.Conv2d(32, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(1, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C_in, H, W)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "m = SimpleNet(width=3)\n",
    "summary(m, (1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74535599, -0.47140452, -0.33333333],\n",
       "       [-0.66666667, -0.33333333, -0.        ],\n",
       "       [-0.74535599, -0.47140452, -0.33333333]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# def norm(x, width):\n",
    "#     return x * width\n",
    "\n",
    "def norm(x, width):\n",
    "    return (int)(x * (width - 1) + 0.5)\n",
    "\n",
    "def _draw_rect(points, width=64):\n",
    "    x0, y0, x1, y1 = points\n",
    "    print(x0, y0, x1, y1)\n",
    "    x0 = norm(x0, width)\n",
    "    y0 = norm(y0, width)\n",
    "    x1 = norm(x1, width)\n",
    "    y1 = norm(y1, width)\n",
    "#     if (x1 == 1):\n",
    "#         x1 -= 0.1\n",
    "#     if (y1 == 1):\n",
    "#         y1 -= 0.1\n",
    "    print(x0, y0, x1, y1)\n",
    "    im = Image.new(\"F\", (width, width))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.rectangle([x0, y0, x1, y1], fill=1, outline=None, width=0)\n",
    "    im = np.array(im)  # (H, W)\n",
    "    print(im)\n",
    "    im = np.expand_dims(im, axis=-1)  # (H, W, 1)\n",
    "    return im\n",
    "\n",
    "def draw_rect(xy, width=3):\n",
    "    x0, y0, x1, y1 = xy\n",
    "    rect = np.zeros()\n",
    "    \n",
    "\n",
    "width = 3\n",
    "xy = []\n",
    "for x0, y0 in itertools.product(range(width), range(width)):\n",
    "    for _w, _h in itertools.product(range(1, width - x0 + 1), range(1, width-y0+1)):\n",
    "        x1 = x0 + _w\n",
    "        y1 = y0 + _h\n",
    "        xy.append([x0, y0, x1, y1])\n",
    "#         x = np.array([x0, y0, x1, y1], dtype=float)\n",
    "#         x /= width\n",
    "#         draw_rect(x, width)\n",
    "        \n",
    "for (x0, y0, x1, y1) in xy:\n",
    "    rect = np.zeros((width, width))\n",
    "    for i, j in itertools.product(range(x0, x1), range(y0, y1)):\n",
    "        rect[i][j] = 1.\n",
    "#     print(rect)\n",
    "\n",
    "# x = np.array([1,2], dtype=int)\n",
    "# x = x.astype(float) / 2\n",
    "x = np.stack([np.array([1,2]), np.array([3,4]), np.array([3,4])])\n",
    "len(x)\n",
    "\n",
    "def draw_l2_distance(x, y, width=64):\n",
    "    im = np.zeros((width, width), dtype=float)\n",
    "    for (i, j), _ in np.ndenumerate(im):\n",
    "        im[i][j] = -np.linalg.norm(np.array([x, y]) - np.array([i, j])) / width\n",
    "#     im = im.transpose(1, 0).reshape(width * width)  # (W, H) -> (H, W) -> (H*W)\n",
    "    return im\n",
    "\n",
    "draw_l2_distance(1, 2, width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 100, 100])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.rand(1,3,100, 100)\n",
    "m = nn.MaxPool2d(2)\n",
    "x1 = m(x0)\n",
    "x1 = F.interpolate(x1, scale_factor=2)\n",
    "# x = torch.cat((x0, x1), dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets...\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_rect_pil(xy, width=64):\n",
    "    x0, y0, x1, y1 = xy\n",
    "    x1 -= 0.5\n",
    "    y1 -= 0.5\n",
    "    im = Image.new(\"F\", (width, width))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.rectangle([x0, y0, x1, y1], fill=1)\n",
    "    im = np.array(im)  # (H, W)\n",
    "    return im\n",
    "\n",
    "\n",
    "def rand_draw(draw_fn=draw_rect_pil, n_strokes=2, width=64):\n",
    "    canvas = np.zeros((width, width, 3), dtype=np.int8)\n",
    "    im = [canvas.copy()]\n",
    "    x = []\n",
    "    for _ in range(n_strokes):\n",
    "        x0, y0 = np.random.randint(width, size=2)\n",
    "        x1 = x0 + np.random.randint(1, width - x0 + 1)\n",
    "        y1 = y0 + np.random.randint(1, width - y0 + 1)\n",
    "        _x = np.array((x0, y0, x1, y1))\n",
    "        #         _x = np.random.rand(action_dim)\n",
    "        color = np.random.randint(255, size=(3))  # (3)\n",
    "        #         x.append(np.concatenate((_x, color / 255.0)))\n",
    "        stroke = draw_fn(_x, width)  # (w, w)\n",
    "        stroke = np.expand_dims(stroke, axis=2)  # (w, w, 1)\n",
    "        canvas = canvas * (1 - stroke) + stroke * color  # (w, h, 3)\n",
    "        x.append(_x)\n",
    "        im.append(canvas.copy())\n",
    "    x = np.stack(x) / width  # (n_strokes, action_dim+3)\n",
    "    im = np.stack(im)\n",
    "    return x, im\n",
    "\n",
    "def generate_data(width=128, n_sample=1000, n_strokes=2):\n",
    "    print(\"Generating datasets...\")\n",
    "    if not os.path.exists(\"data-rect/\"):\n",
    "        os.makedirs(\"data-rect/\")\n",
    "\n",
    "    x, im = [], []\n",
    "    for _ in range(n_sample):\n",
    "        _x, _im = rand_draw(n_strokes=n_strokes, width=width)\n",
    "        _im_inter = _im[:-1]\n",
    "        _im_target = _im[-1]\n",
    "#         print(_im_inter.shape)\n",
    "        for i in range(n_strokes):\n",
    "            im.append(np.concatenate((_im_inter[i], _im_target), axis=2))\n",
    "            x.append(_x[i])\n",
    "    print(len(x))\n",
    "    print(len(im))\n",
    "#     print(im[0][:,:,-3:])\n",
    "#     plt.imshow(Image.fromarray(im[1][:,:,:-3].astype('uint8')))\n",
    "    plt.imshow(Image.fromarray(im[1][:,:,-3:].astype('uint8')))\n",
    "\n",
    "generate_data(width=16, n_sample=1, n_strokes=2)\n",
    "# a = np.array([1,2,3])\n",
    "# a = np.random.rand(5,3)\n",
    "# b = np.array([3,4,5])\n",
    "# b = np.random.rand(5,3)\n",
    "# np.concatenate([a,b],axis=1).shape\n",
    "# x[:-1]\n",
    "\n",
    "# x, im = rand_draw()\n",
    "# fig, ax = plt.subplots(1,3)\n",
    "# ax[0].imshow(Image.fromarray(im[0].astype(np.uint8)))\n",
    "# ax[1].imshow(Image.fromarray(im[1].astype(np.uint8)))\n",
    "# ax[2].imshow(Image.fromarray(im[2].astype(np.uint8)))\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel, width, out_size=4, latent_size=128):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "#         self.conv0 = nn.Conv2d(in_channel + 2, latent_size, 3, padding=1)\n",
    "#         self.bn0 = BatchNorm2d(latent_size)\n",
    "#         self.conv1 = nn.Conv2d(latent_size, latent_size, 3, padding=1)\n",
    "#         self.bn1 = BatchNorm2d(latent_size)\n",
    "# self.conv2 = nn.Conv2d(latent_size, out_size, 1)\n",
    "        self.conv0 = nn.Conv2d(in_channel + 2, latent_size, 1, stride=1, padding=0)\n",
    "        self.bn0 = nn.BatchNorm2d(latent_size)\n",
    "        self.conv2 = nn.Conv2d(latent_size, out_size, 1, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(width, stride=width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_coords(x)\n",
    "        x = self.conv0(x)\n",
    "        x = F.relu(self.bn0(x))\n",
    "#         x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "    \n",
    "class HRNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(HRNet, self).__init__()\n",
    "        cfg = get_cfg_defaults()\n",
    "        cfg.merge_from_file(\"./hrnet.yaml\")\n",
    "        self.hr0 = HighResolutionNet(cfg)\n",
    "        self.hr1 = HighResolutionNet(cfg)\n",
    "        self.rg = Regressor(in_channel=540, width=int(width/4))\n",
    "        \n",
    "    def forward(self, im_current, im_target):\n",
    "        x0 = self.hr0(im_current)\n",
    "        x1 = self.hr1(im_target)\n",
    "        x = torch.cat((x0, x1), dim=1)\n",
    "        x = self.rg(x)\n",
    "        return x\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, in_dim=128, out_dim=4, hidden_dim=128):\n",
    "        super(Painter, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden_dim)\n",
    "        self.fc0 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "m = HRNet(64)\n",
    "x0 = torch.rand(1,3,64,64)\n",
    "x1 = torch.rand(1,3,64,64)\n",
    "m(x0, x1).shape\n",
    "\n",
    "# summary(m, (1, 128))\n",
    "\n",
    "# rnn = nn.GRU(10, 20)\n",
    "# input = torch.randn(5, 3, 10)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# # output, hn = rnn(input, h0)\n",
    "# output, hn = rnn(input)\n",
    "# print(output.shape)\n",
    "# print(hn.shape)\n",
    "\n",
    "\n",
    "# summary(rnn, (3, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
