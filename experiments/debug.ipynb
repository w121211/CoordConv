{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.conv as conv\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from coordconv import AddCoords\n",
    "from hrnet import HighResolutionNet\n",
    "from config import get_cfg_defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:2528: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HRNet, self).__init__()\n",
    "        cfg = get_cfg_defaults()\n",
    "        cfg.merge_from_file(\"./exp.yaml\")\n",
    "        self.hr = HighResolutionNet(cfg)\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv5 = nn.Conv2d(6, 6, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(6, 6, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(6, 4, 1)\n",
    "        self.pool = nn.MaxPool2d(16, stride=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hr(x)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "m = HRNet()\n",
    "# summary(m, (3, 64, 64))\n",
    "x = m(torch.rand(1,1,64,64))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 200, 200]          18,816\n",
      "       BatchNorm2d-2        [-1, 128, 200, 200]             256\n",
      "              ReLU-3        [-1, 128, 200, 200]               0\n",
      "       convolution-4        [-1, 128, 200, 200]               0\n",
      "            Conv2d-5        [-1, 256, 100, 100]         294,912\n",
      "       BatchNorm2d-6        [-1, 256, 100, 100]             512\n",
      "              ReLU-7        [-1, 256, 100, 100]               0\n",
      "            Conv2d-8        [-1, 256, 100, 100]         589,824\n",
      "       BatchNorm2d-9        [-1, 256, 100, 100]             512\n",
      "           Conv2d-10        [-1, 256, 100, 100]          32,768\n",
      "      BatchNorm2d-11        [-1, 256, 100, 100]             512\n",
      "             ReLU-12        [-1, 256, 100, 100]               0\n",
      "         residual-13        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 938,112\n",
      "Trainable params: 938,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 332.03\n",
      "Params size (MB): 3.58\n",
      "Estimated Total Size (MB): 337.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from models.CornerNet import model\n",
    "\n",
    "# m = model()\n",
    "\n",
    "class convolution(nn.Module):\n",
    "    def __init__(self, k, inp_dim, out_dim, stride=1, with_bn=True):\n",
    "        super(convolution, self).__init__()\n",
    "\n",
    "        pad = (k - 1) // 2\n",
    "        self.conv = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(pad, pad), stride=(stride, stride), bias=not with_bn)\n",
    "        self.bn   = nn.BatchNorm2d(out_dim) if with_bn else nn.Sequential()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        bn   = self.bn(conv)\n",
    "        relu = self.relu(bn)\n",
    "        return relu\n",
    "\n",
    "class residual(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, k=3, stride=1):\n",
    "        super(residual, self).__init__()\n",
    "        p = (k - 1) // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(p, p), stride=(stride, stride), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, (k, k), padding=(p, p), bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_dim)\n",
    "        \n",
    "        self.skip  = nn.Sequential(\n",
    "            nn.Conv2d(inp_dim, out_dim, (1, 1), stride=(stride, stride), bias=False),\n",
    "            nn.BatchNorm2d(out_dim)\n",
    "        ) if stride != 1 or inp_dim != out_dim else nn.Sequential()\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "\n",
    "        skip  = self.skip(x)\n",
    "        return self.relu(bn2 + skip)\n",
    "\n",
    "\n",
    "pre = nn.Sequential(\n",
    "            convolution(7, 3, 128, stride=2), residual(128, 256, stride=2)\n",
    "        )\n",
    "# pre(torch.from_numpy(np.random.rand(1, 3, 128, 128)).double())\n",
    "# conv = convolution(7, 3, 128, stride=2)\n",
    "x = torch.rand(1, 3, 512, 512)\n",
    "x = pre(x)\n",
    "# x.shape\n",
    "\n",
    "summary(pre, (3, 400, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-2        [-1, 128, 256, 256]             256\n",
      "            Conv2d-3        [-1, 128, 256, 256]          16,384\n",
      "            Conv2d-4        [-1, 128, 256, 256]           1,152\n",
      "       BatchNorm2d-5        [-1, 256, 256, 256]             512\n",
      "              ReLU-6        [-1, 256, 256, 256]               0\n",
      "       fire_module-7        [-1, 256, 256, 256]               0\n",
      "            Conv2d-8        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-9        [-1, 128, 256, 256]             256\n",
      "           Conv2d-10        [-1, 128, 256, 256]          16,384\n",
      "           Conv2d-11        [-1, 128, 256, 256]           1,152\n",
      "      BatchNorm2d-12        [-1, 256, 256, 256]             512\n",
      "             ReLU-13        [-1, 256, 256, 256]               0\n",
      "      fire_module-14        [-1, 256, 256, 256]               0\n",
      "           Conv2d-15        [-1, 128, 256, 256]          32,768\n",
      "      BatchNorm2d-16        [-1, 128, 256, 256]             256\n",
      "           Conv2d-17        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-18        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-19        [-1, 256, 128, 128]             512\n",
      "             ReLU-20        [-1, 256, 128, 128]               0\n",
      "      fire_module-21        [-1, 256, 128, 128]               0\n",
      "           Conv2d-22        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-23        [-1, 128, 128, 128]             256\n",
      "           Conv2d-24        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-25        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-26        [-1, 256, 128, 128]             512\n",
      "             ReLU-27        [-1, 256, 128, 128]               0\n",
      "      fire_module-28        [-1, 256, 128, 128]               0\n",
      "           Conv2d-29        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-30        [-1, 128, 128, 128]             256\n",
      "           Conv2d-31        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-32        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-33        [-1, 256, 128, 128]             512\n",
      "             ReLU-34        [-1, 256, 128, 128]               0\n",
      "      fire_module-35        [-1, 256, 128, 128]               0\n",
      "           Conv2d-36        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-37        [-1, 128, 128, 128]             256\n",
      "           Conv2d-38        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-39        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-40        [-1, 256, 128, 128]             512\n",
      "             ReLU-41        [-1, 256, 128, 128]               0\n",
      "      fire_module-42        [-1, 256, 128, 128]               0\n",
      "           Conv2d-43        [-1, 192, 128, 128]          49,152\n",
      "      BatchNorm2d-44        [-1, 192, 128, 128]             384\n",
      "           Conv2d-45          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-46          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-47          [-1, 384, 64, 64]             768\n",
      "             ReLU-48          [-1, 384, 64, 64]               0\n",
      "      fire_module-49          [-1, 384, 64, 64]               0\n",
      "           Conv2d-50          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-51          [-1, 192, 64, 64]             384\n",
      "           Conv2d-52          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-53          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-54          [-1, 384, 64, 64]             768\n",
      "             ReLU-55          [-1, 384, 64, 64]               0\n",
      "      fire_module-56          [-1, 384, 64, 64]               0\n",
      "           Conv2d-57          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-58          [-1, 192, 64, 64]             384\n",
      "           Conv2d-59          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-60          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-61          [-1, 384, 64, 64]             768\n",
      "             ReLU-62          [-1, 384, 64, 64]               0\n",
      "      fire_module-63          [-1, 384, 64, 64]               0\n",
      "           Conv2d-64          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-65          [-1, 192, 64, 64]             384\n",
      "           Conv2d-66          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-67          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-68          [-1, 384, 64, 64]             768\n",
      "             ReLU-69          [-1, 384, 64, 64]               0\n",
      "      fire_module-70          [-1, 384, 64, 64]               0\n",
      "           Conv2d-71          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-72          [-1, 192, 64, 64]             384\n",
      "           Conv2d-73          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-74          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-75          [-1, 384, 32, 32]             768\n",
      "             ReLU-76          [-1, 384, 32, 32]               0\n",
      "      fire_module-77          [-1, 384, 32, 32]               0\n",
      "           Conv2d-78          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-79          [-1, 192, 32, 32]             384\n",
      "           Conv2d-80          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-81          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-82          [-1, 384, 32, 32]             768\n",
      "             ReLU-83          [-1, 384, 32, 32]               0\n",
      "      fire_module-84          [-1, 384, 32, 32]               0\n",
      "           Conv2d-85          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-86          [-1, 192, 32, 32]             384\n",
      "           Conv2d-87          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-88          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-89          [-1, 384, 32, 32]             768\n",
      "             ReLU-90          [-1, 384, 32, 32]               0\n",
      "      fire_module-91          [-1, 384, 32, 32]               0\n",
      "           Conv2d-92          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-93          [-1, 192, 32, 32]             384\n",
      "           Conv2d-94          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-95          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-96          [-1, 384, 32, 32]             768\n",
      "             ReLU-97          [-1, 384, 32, 32]               0\n",
      "      fire_module-98          [-1, 384, 32, 32]               0\n",
      "           Conv2d-99          [-1, 256, 32, 32]          98,304\n",
      "     BatchNorm2d-100          [-1, 256, 32, 32]             512\n",
      "          Conv2d-101          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-102          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-103          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-104          [-1, 512, 16, 16]               0\n",
      "     fire_module-105          [-1, 512, 16, 16]               0\n",
      "          Conv2d-106          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-107          [-1, 256, 16, 16]             512\n",
      "          Conv2d-108          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-109          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-110          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-111          [-1, 512, 16, 16]               0\n",
      "     fire_module-112          [-1, 512, 16, 16]               0\n",
      "          Conv2d-113          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-114          [-1, 256, 16, 16]             512\n",
      "          Conv2d-115          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-116          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-117          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-118          [-1, 512, 16, 16]               0\n",
      "     fire_module-119          [-1, 512, 16, 16]               0\n",
      "          Conv2d-120          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-121          [-1, 256, 16, 16]             512\n",
      "          Conv2d-122          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-123          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-124          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-125          [-1, 512, 16, 16]               0\n",
      "     fire_module-126          [-1, 512, 16, 16]               0\n",
      "          Conv2d-127          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-128          [-1, 256, 16, 16]             512\n",
      "          Conv2d-129          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-130          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-131          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-132          [-1, 512, 16, 16]               0\n",
      "     fire_module-133          [-1, 512, 16, 16]               0\n",
      "          Conv2d-134          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-135          [-1, 256, 16, 16]             512\n",
      "          Conv2d-136          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-137          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-139          [-1, 512, 16, 16]               0\n",
      "     fire_module-140          [-1, 512, 16, 16]               0\n",
      "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
      "          Conv2d-143          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-144          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-145          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-146          [-1, 512, 16, 16]               0\n",
      "     fire_module-147          [-1, 512, 16, 16]               0\n",
      "          Conv2d-148          [-1, 192, 16, 16]          98,304\n",
      "     BatchNorm2d-149          [-1, 192, 16, 16]             384\n",
      "          Conv2d-150          [-1, 192, 16, 16]          36,864\n",
      "          Conv2d-151          [-1, 192, 16, 16]           1,728\n",
      "     BatchNorm2d-152          [-1, 384, 16, 16]             768\n",
      "            ReLU-153          [-1, 384, 16, 16]               0\n",
      "     fire_module-154          [-1, 384, 16, 16]               0\n",
      " ConvTranspose2d-155          [-1, 384, 32, 32]       2,359,680\n",
      "           merge-156          [-1, 384, 32, 32]               0\n",
      "       hg_module-157          [-1, 384, 32, 32]               0\n",
      "          Conv2d-158          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-159          [-1, 192, 32, 32]             384\n",
      "          Conv2d-160          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-161          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-162          [-1, 384, 32, 32]             768\n",
      "            ReLU-163          [-1, 384, 32, 32]               0\n",
      "     fire_module-164          [-1, 384, 32, 32]               0\n",
      "          Conv2d-165          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-166          [-1, 192, 32, 32]             384\n",
      "          Conv2d-167          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-168          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-169          [-1, 384, 32, 32]             768\n",
      "            ReLU-170          [-1, 384, 32, 32]               0\n",
      "     fire_module-171          [-1, 384, 32, 32]               0\n",
      " ConvTranspose2d-172          [-1, 384, 64, 64]       2,359,680\n",
      "           merge-173          [-1, 384, 64, 64]               0\n",
      "       hg_module-174          [-1, 384, 64, 64]               0\n",
      "          Conv2d-175          [-1, 192, 64, 64]          73,728\n",
      "     BatchNorm2d-176          [-1, 192, 64, 64]             384\n",
      "          Conv2d-177          [-1, 192, 64, 64]          36,864\n",
      "          Conv2d-178          [-1, 192, 64, 64]           1,728\n",
      "     BatchNorm2d-179          [-1, 384, 64, 64]             768\n",
      "            ReLU-180          [-1, 384, 64, 64]               0\n",
      "     fire_module-181          [-1, 384, 64, 64]               0\n",
      "          Conv2d-182          [-1, 128, 64, 64]          49,152\n",
      "     BatchNorm2d-183          [-1, 128, 64, 64]             256\n",
      "          Conv2d-184          [-1, 128, 64, 64]          16,384\n",
      "          Conv2d-185          [-1, 128, 64, 64]           1,152\n",
      "     BatchNorm2d-186          [-1, 256, 64, 64]             512\n",
      "            ReLU-187          [-1, 256, 64, 64]               0\n",
      "     fire_module-188          [-1, 256, 64, 64]               0\n",
      " ConvTranspose2d-189        [-1, 256, 128, 128]       1,048,832\n",
      "           merge-190        [-1, 256, 128, 128]               0\n",
      "       hg_module-191        [-1, 256, 128, 128]               0\n",
      "          Conv2d-192        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-193        [-1, 128, 128, 128]             256\n",
      "          Conv2d-194        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-195        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-196        [-1, 256, 128, 128]             512\n",
      "            ReLU-197        [-1, 256, 128, 128]               0\n",
      "     fire_module-198        [-1, 256, 128, 128]               0\n",
      "          Conv2d-199        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-200        [-1, 128, 128, 128]             256\n",
      "          Conv2d-201        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-202        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-203        [-1, 256, 128, 128]             512\n",
      "            ReLU-204        [-1, 256, 128, 128]               0\n",
      "     fire_module-205        [-1, 256, 128, 128]               0\n",
      " ConvTranspose2d-206        [-1, 256, 256, 256]       1,048,832\n",
      "           merge-207        [-1, 256, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 10,025,088\n",
      "Trainable params: 10,025,088\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 64.00\n",
      "Forward/backward pass size (MB): 3249.75\n",
      "Params size (MB): 38.24\n",
      "Estimated Total Size (MB): 3351.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class upsample(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.interpolate(x, scale_factor=self.scale_factor)\n",
    "\n",
    "class merge(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "def _make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, out_dim)]\n",
    "    layers += [residual(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [residual(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_pool_layer(dim):\n",
    "    return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "def _make_unpool_layer(dim):\n",
    "    return upsample(scale_factor=2)\n",
    "\n",
    "def _make_merge_layer(dim):\n",
    "    return merge()\n",
    "\n",
    "class fire_module(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, sr=2, stride=1):\n",
    "        super(fire_module, self).__init__()\n",
    "        self.conv1    = nn.Conv2d(inp_dim, out_dim // sr, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1      = nn.BatchNorm2d(out_dim // sr)\n",
    "        self.conv_1x1 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=1, stride=stride, bias=False)\n",
    "        self.conv_3x3 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=3, padding=1, \n",
    "                                  stride=stride, groups=out_dim // sr, bias=False)\n",
    "        self.bn2      = nn.BatchNorm2d(out_dim)\n",
    "        self.skip     = (stride == 1 and inp_dim == out_dim)\n",
    "        self.relu     = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        conv2 = torch.cat((self.conv_1x1(bn1), self.conv_3x3(bn1)), 1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "        if self.skip:\n",
    "            return self.relu(bn2 + x)\n",
    "        else:\n",
    "            return self.relu(bn2)\n",
    "\n",
    "class hg_module(nn.Module):\n",
    "    def __init__(\n",
    "        self, n, dims, modules, make_up_layer=_make_layer,\n",
    "        make_pool_layer=_make_pool_layer, make_hg_layer=_make_layer,\n",
    "        make_low_layer=_make_layer, make_hg_layer_revr=_make_layer_revr,\n",
    "        make_unpool_layer=_make_unpool_layer, make_merge_layer=_make_merge_layer\n",
    "    ):\n",
    "        super(hg_module, self).__init__()\n",
    "\n",
    "        curr_mod = modules[0]\n",
    "        next_mod = modules[1]\n",
    "\n",
    "        curr_dim = dims[0]\n",
    "        next_dim = dims[1]\n",
    "\n",
    "        self.n    = n\n",
    "        self.up1  = make_up_layer(curr_dim, curr_dim, curr_mod)\n",
    "        self.max1 = make_pool_layer(curr_dim)\n",
    "        self.low1 = make_hg_layer(curr_dim, next_dim, curr_mod)\n",
    "        self.low2 = hg_module(\n",
    "            n - 1, dims[1:], modules[1:],\n",
    "            make_up_layer=make_up_layer,\n",
    "            make_pool_layer=make_pool_layer,\n",
    "            make_hg_layer=make_hg_layer,\n",
    "            make_low_layer=make_low_layer,\n",
    "            make_hg_layer_revr=make_hg_layer_revr,\n",
    "            make_unpool_layer=make_unpool_layer,\n",
    "            make_merge_layer=make_merge_layer\n",
    "        ) if n > 1 else make_low_layer(next_dim, next_dim, next_mod)\n",
    "        self.low3 = make_hg_layer_revr(next_dim, curr_dim, curr_mod)\n",
    "        self.up2  = make_unpool_layer(curr_dim)\n",
    "        self.merg = make_merge_layer(curr_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        up1  = self.up1(x)\n",
    "        max1 = self.max1(x)\n",
    "        low1 = self.low1(max1)\n",
    "        low2 = self.low2(low1)\n",
    "        low3 = self.low3(low2)\n",
    "        up2  = self.up2(low3)\n",
    "        print(up1.shape)\n",
    "        print(up2.shape)\n",
    "        merg = self.merg(up1, up2)\n",
    "        return merg\n",
    "#         return up1\n",
    "\n",
    "    \n",
    "def make_pool_layer(dim):\n",
    "    return nn.Sequential()\n",
    "\n",
    "def make_unpool_layer(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "def make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [fire_module(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_hg_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim, stride=2)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "m = hg_module(\n",
    "                4, [256, 256, 384, 384, 512], [2, 2, 2, 2, 4],\n",
    "                make_pool_layer=make_pool_layer,\n",
    "                make_unpool_layer=make_unpool_layer,\n",
    "                make_up_layer=make_layer,\n",
    "                make_low_layer=make_layer,\n",
    "                make_hg_layer_revr=make_layer_revr,\n",
    "                make_hg_layer=make_hg_layer\n",
    "            )\n",
    "\n",
    "summary(m, (256, 256, 256))\n",
    "# m(torch.rand(1, 256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 256, 200, 200]          65,792\n",
      "              ReLU-2        [-1, 256, 200, 200]               0\n",
      "       convolution-3        [-1, 256, 200, 200]               0\n",
      "            Conv2d-4         [-1, 80, 200, 200]          20,560\n",
      "================================================================\n",
      "Total params: 86,352\n",
      "Trainable params: 86,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.06\n",
      "Forward/backward pass size (MB): 258.79\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 298.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def _pred_mod(dim):\n",
    "        return nn.Sequential(\n",
    "            convolution(1, 256, 256, with_bn=False),\n",
    "            nn.Conv2d(256, dim, (1, 1))\n",
    "        )\n",
    "m = _pred_mod(80)\n",
    "summary(m, (256, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-2        [-1, 128, 100, 100]             256\n",
      "              ReLU-3        [-1, 128, 100, 100]               0\n",
      "       convolution-4        [-1, 128, 100, 100]               0\n",
      "           TopPool-5        [-1, 128, 100, 100]               0\n",
      "            Conv2d-6        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-7        [-1, 128, 100, 100]             256\n",
      "              ReLU-8        [-1, 128, 100, 100]               0\n",
      "       convolution-9        [-1, 128, 100, 100]               0\n",
      "         LeftPool-10        [-1, 128, 100, 100]               0\n",
      "           Conv2d-11        [-1, 256, 100, 100]         294,912\n",
      "      BatchNorm2d-12        [-1, 256, 100, 100]             512\n",
      "           Conv2d-13        [-1, 256, 100, 100]          65,536\n",
      "      BatchNorm2d-14        [-1, 256, 100, 100]             512\n",
      "             ReLU-15        [-1, 256, 100, 100]               0\n",
      "           Conv2d-16        [-1, 256, 100, 100]         589,824\n",
      "      BatchNorm2d-17        [-1, 256, 100, 100]             512\n",
      "             ReLU-18        [-1, 256, 100, 100]               0\n",
      "      convolution-19        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 1,542,144\n",
      "Trainable params: 1,542,144\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9.77\n",
      "Forward/backward pass size (MB): 273.44\n",
      "Params size (MB): 5.88\n",
      "Estimated Total Size (MB): 289.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models.py_utils import TopPool, LeftPool\n",
    "\n",
    "class corner_pool(nn.Module):\n",
    "    def __init__(self, dim, pool1, pool2):\n",
    "        super(corner_pool, self).__init__()\n",
    "        self._init_layers(dim, pool1, pool2)\n",
    "\n",
    "    def _init_layers(self, dim, pool1, pool2):\n",
    "        self.p1_conv1 = convolution(3, dim, 128)\n",
    "        self.p2_conv1 = convolution(3, dim, 128)\n",
    "\n",
    "        self.p_conv1 = nn.Conv2d(128, dim, (3, 3), padding=(1, 1), bias=False)\n",
    "        self.p_bn1   = nn.BatchNorm2d(dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (1, 1), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = convolution(3, dim, dim)\n",
    "\n",
    "        self.pool1 = pool1()\n",
    "        self.pool2 = pool2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pool 1\n",
    "        p1_conv1 = self.p1_conv1(x)\n",
    "        pool1    = self.pool1(p1_conv1)\n",
    "\n",
    "        # pool 2\n",
    "        p2_conv1 = self.p2_conv1(x)\n",
    "        pool2    = self.pool2(p2_conv1)\n",
    "\n",
    "        # pool 1 + pool 2\n",
    "        p_conv1 = self.p_conv1(pool1 + pool2)\n",
    "        p_bn1   = self.p_bn1(p_conv1)\n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(p_bn1 + bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        return conv2\n",
    "\n",
    "m = corner_pool(256, TopPool, LeftPool)\n",
    "summary(m, (256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AddCoords-1              [-1, 3, 3, 3]               0\n",
      "            Conv2d-2              [-1, 8, 3, 3]             224\n",
      "            Conv2d-3             [-1, 16, 3, 3]           1,168\n",
      "            Conv2d-4             [-1, 32, 3, 3]           4,640\n",
      "            Conv2d-5            [-1, 128, 3, 3]          36,864\n",
      "       BatchNorm2d-6            [-1, 128, 3, 3]             256\n",
      "              ReLU-7            [-1, 128, 3, 3]               0\n",
      "       convolution-8            [-1, 128, 3, 3]               0\n",
      "           TopPool-9            [-1, 128, 3, 3]               0\n",
      "           Conv2d-10            [-1, 128, 3, 3]          36,864\n",
      "      BatchNorm2d-11            [-1, 128, 3, 3]             256\n",
      "             ReLU-12            [-1, 128, 3, 3]               0\n",
      "      convolution-13            [-1, 128, 3, 3]               0\n",
      "         LeftPool-14            [-1, 128, 3, 3]               0\n",
      "           Conv2d-15             [-1, 32, 3, 3]          36,864\n",
      "      BatchNorm2d-16             [-1, 32, 3, 3]              64\n",
      "           Conv2d-17             [-1, 32, 3, 3]           1,024\n",
      "      BatchNorm2d-18             [-1, 32, 3, 3]              64\n",
      "             ReLU-19             [-1, 32, 3, 3]               0\n",
      "           Conv2d-20             [-1, 32, 3, 3]           9,216\n",
      "      BatchNorm2d-21             [-1, 32, 3, 3]              64\n",
      "             ReLU-22             [-1, 32, 3, 3]               0\n",
      "      convolution-23             [-1, 32, 3, 3]               0\n",
      "      corner_pool-24             [-1, 32, 3, 3]               0\n",
      "           Conv2d-25              [-1, 1, 3, 3]              33\n",
      "           Conv2d-26              [-1, 1, 3, 3]               2\n",
      "================================================================\n",
      "Total params: 127,603\n",
      "Trainable params: 127,603\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.49\n",
      "Estimated Total Size (MB): 0.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.width = width\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = corner_pool(32, TopPool, LeftPool)\n",
    "        self.conv4 = nn.Conv2d(32, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(1, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C_in, H, W)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "m = SimpleNet(width=3)\n",
    "summary(m, (1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74535599, -0.47140452, -0.33333333],\n",
       "       [-0.66666667, -0.33333333, -0.        ],\n",
       "       [-0.74535599, -0.47140452, -0.33333333]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# def norm(x, width):\n",
    "#     return x * width\n",
    "\n",
    "def norm(x, width):\n",
    "    return (int)(x * (width - 1) + 0.5)\n",
    "\n",
    "def _draw_rect(points, width=64):\n",
    "    x0, y0, x1, y1 = points\n",
    "    print(x0, y0, x1, y1)\n",
    "    x0 = norm(x0, width)\n",
    "    y0 = norm(y0, width)\n",
    "    x1 = norm(x1, width)\n",
    "    y1 = norm(y1, width)\n",
    "#     if (x1 == 1):\n",
    "#         x1 -= 0.1\n",
    "#     if (y1 == 1):\n",
    "#         y1 -= 0.1\n",
    "    print(x0, y0, x1, y1)\n",
    "    im = Image.new(\"F\", (width, width))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.rectangle([x0, y0, x1, y1], fill=1, outline=None, width=0)\n",
    "    im = np.array(im)  # (H, W)\n",
    "    print(im)\n",
    "    im = np.expand_dims(im, axis=-1)  # (H, W, 1)\n",
    "    return im\n",
    "\n",
    "def draw_rect(xy, width=3):\n",
    "    x0, y0, x1, y1 = xy\n",
    "    rect = np.zeros()\n",
    "    \n",
    "\n",
    "width = 3\n",
    "xy = []\n",
    "for x0, y0 in itertools.product(range(width), range(width)):\n",
    "    for _w, _h in itertools.product(range(1, width - x0 + 1), range(1, width-y0+1)):\n",
    "        x1 = x0 + _w\n",
    "        y1 = y0 + _h\n",
    "        xy.append([x0, y0, x1, y1])\n",
    "#         x = np.array([x0, y0, x1, y1], dtype=float)\n",
    "#         x /= width\n",
    "#         draw_rect(x, width)\n",
    "        \n",
    "for (x0, y0, x1, y1) in xy:\n",
    "    rect = np.zeros((width, width))\n",
    "    for i, j in itertools.product(range(x0, x1), range(y0, y1)):\n",
    "        rect[i][j] = 1.\n",
    "#     print(rect)\n",
    "\n",
    "# x = np.array([1,2], dtype=int)\n",
    "# x = x.astype(float) / 2\n",
    "x = np.stack([np.array([1,2]), np.array([3,4]), np.array([3,4])])\n",
    "len(x)\n",
    "\n",
    "def draw_l2_distance(x, y, width=64):\n",
    "    im = np.zeros((width, width), dtype=float)\n",
    "    for (i, j), _ in np.ndenumerate(im):\n",
    "        im[i][j] = -np.linalg.norm(np.array([x, y]) - np.array([i, j])) / width\n",
    "#     im = im.transpose(1, 0).reshape(width * width)  # (W, H) -> (H, W) -> (H*W)\n",
    "    return im\n",
    "\n",
    "draw_l2_distance(1, 2, width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 100, 100])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.rand(1,3,100, 100)\n",
    "m = nn.MaxPool2d(2)\n",
    "x1 = m(x0)\n",
    "x1 = F.interpolate(x1, scale_factor=2)\n",
    "# x = torch.cat((x0, x1), dim=1)\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
