{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.conv as conv\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from coordconv import AddCoords\n",
    "from hrnet import HighResolutionNet\n",
    "from config import get_cfg_defaults\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HRNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(HRNet, self).__init__()\n",
    "        cfg = get_cfg_defaults()\n",
    "        cfg.merge_from_file(\"./exp.yaml\")\n",
    "        self.hr = HighResolutionNet(cfg)\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv5 = nn.Conv2d(7, 7, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(7, 7, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(7, 4, 1)\n",
    "        self.pool = nn.MaxPool2d(width, stride=width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.hr(x)\n",
    "        x1 = F.interpolate(x1, scale_factor=4)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "m = HRNet(width=64)\n",
    "# summary(m, (3, 64, 64))\n",
    "x = m(torch.rand(1,1,64,64))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 200, 200]          18,816\n",
      "       BatchNorm2d-2        [-1, 128, 200, 200]             256\n",
      "              ReLU-3        [-1, 128, 200, 200]               0\n",
      "       convolution-4        [-1, 128, 200, 200]               0\n",
      "            Conv2d-5        [-1, 256, 100, 100]         294,912\n",
      "       BatchNorm2d-6        [-1, 256, 100, 100]             512\n",
      "              ReLU-7        [-1, 256, 100, 100]               0\n",
      "            Conv2d-8        [-1, 256, 100, 100]         589,824\n",
      "       BatchNorm2d-9        [-1, 256, 100, 100]             512\n",
      "           Conv2d-10        [-1, 256, 100, 100]          32,768\n",
      "      BatchNorm2d-11        [-1, 256, 100, 100]             512\n",
      "             ReLU-12        [-1, 256, 100, 100]               0\n",
      "         residual-13        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 938,112\n",
      "Trainable params: 938,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 332.03\n",
      "Params size (MB): 3.58\n",
      "Estimated Total Size (MB): 337.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from models.CornerNet import model\n",
    "\n",
    "# m = model()\n",
    "\n",
    "class convolution(nn.Module):\n",
    "    def __init__(self, k, inp_dim, out_dim, stride=1, with_bn=True):\n",
    "        super(convolution, self).__init__()\n",
    "\n",
    "        pad = (k - 1) // 2\n",
    "        self.conv = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(pad, pad), stride=(stride, stride), bias=not with_bn)\n",
    "        self.bn   = nn.BatchNorm2d(out_dim) if with_bn else nn.Sequential()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        bn   = self.bn(conv)\n",
    "        relu = self.relu(bn)\n",
    "        return relu\n",
    "\n",
    "class residual(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, k=3, stride=1):\n",
    "        super(residual, self).__init__()\n",
    "        p = (k - 1) // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(p, p), stride=(stride, stride), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, (k, k), padding=(p, p), bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_dim)\n",
    "        \n",
    "        self.skip  = nn.Sequential(\n",
    "            nn.Conv2d(inp_dim, out_dim, (1, 1), stride=(stride, stride), bias=False),\n",
    "            nn.BatchNorm2d(out_dim)\n",
    "        ) if stride != 1 or inp_dim != out_dim else nn.Sequential()\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "\n",
    "        skip  = self.skip(x)\n",
    "        return self.relu(bn2 + skip)\n",
    "\n",
    "\n",
    "pre = nn.Sequential(\n",
    "            convolution(7, 3, 128, stride=2), residual(128, 256, stride=2)\n",
    "        )\n",
    "# pre(torch.from_numpy(np.random.rand(1, 3, 128, 128)).double())\n",
    "# conv = convolution(7, 3, 128, stride=2)\n",
    "x = torch.rand(1, 3, 512, 512)\n",
    "x = pre(x)\n",
    "# x.shape\n",
    "\n",
    "summary(pre, (3, 400, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-2        [-1, 128, 256, 256]             256\n",
      "            Conv2d-3        [-1, 128, 256, 256]          16,384\n",
      "            Conv2d-4        [-1, 128, 256, 256]           1,152\n",
      "       BatchNorm2d-5        [-1, 256, 256, 256]             512\n",
      "              ReLU-6        [-1, 256, 256, 256]               0\n",
      "       fire_module-7        [-1, 256, 256, 256]               0\n",
      "            Conv2d-8        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-9        [-1, 128, 256, 256]             256\n",
      "           Conv2d-10        [-1, 128, 256, 256]          16,384\n",
      "           Conv2d-11        [-1, 128, 256, 256]           1,152\n",
      "      BatchNorm2d-12        [-1, 256, 256, 256]             512\n",
      "             ReLU-13        [-1, 256, 256, 256]               0\n",
      "      fire_module-14        [-1, 256, 256, 256]               0\n",
      "           Conv2d-15        [-1, 128, 256, 256]          32,768\n",
      "      BatchNorm2d-16        [-1, 128, 256, 256]             256\n",
      "           Conv2d-17        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-18        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-19        [-1, 256, 128, 128]             512\n",
      "             ReLU-20        [-1, 256, 128, 128]               0\n",
      "      fire_module-21        [-1, 256, 128, 128]               0\n",
      "           Conv2d-22        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-23        [-1, 128, 128, 128]             256\n",
      "           Conv2d-24        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-25        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-26        [-1, 256, 128, 128]             512\n",
      "             ReLU-27        [-1, 256, 128, 128]               0\n",
      "      fire_module-28        [-1, 256, 128, 128]               0\n",
      "           Conv2d-29        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-30        [-1, 128, 128, 128]             256\n",
      "           Conv2d-31        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-32        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-33        [-1, 256, 128, 128]             512\n",
      "             ReLU-34        [-1, 256, 128, 128]               0\n",
      "      fire_module-35        [-1, 256, 128, 128]               0\n",
      "           Conv2d-36        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-37        [-1, 128, 128, 128]             256\n",
      "           Conv2d-38        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-39        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-40        [-1, 256, 128, 128]             512\n",
      "             ReLU-41        [-1, 256, 128, 128]               0\n",
      "      fire_module-42        [-1, 256, 128, 128]               0\n",
      "           Conv2d-43        [-1, 192, 128, 128]          49,152\n",
      "      BatchNorm2d-44        [-1, 192, 128, 128]             384\n",
      "           Conv2d-45          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-46          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-47          [-1, 384, 64, 64]             768\n",
      "             ReLU-48          [-1, 384, 64, 64]               0\n",
      "      fire_module-49          [-1, 384, 64, 64]               0\n",
      "           Conv2d-50          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-51          [-1, 192, 64, 64]             384\n",
      "           Conv2d-52          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-53          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-54          [-1, 384, 64, 64]             768\n",
      "             ReLU-55          [-1, 384, 64, 64]               0\n",
      "      fire_module-56          [-1, 384, 64, 64]               0\n",
      "           Conv2d-57          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-58          [-1, 192, 64, 64]             384\n",
      "           Conv2d-59          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-60          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-61          [-1, 384, 64, 64]             768\n",
      "             ReLU-62          [-1, 384, 64, 64]               0\n",
      "      fire_module-63          [-1, 384, 64, 64]               0\n",
      "           Conv2d-64          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-65          [-1, 192, 64, 64]             384\n",
      "           Conv2d-66          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-67          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-68          [-1, 384, 64, 64]             768\n",
      "             ReLU-69          [-1, 384, 64, 64]               0\n",
      "      fire_module-70          [-1, 384, 64, 64]               0\n",
      "           Conv2d-71          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-72          [-1, 192, 64, 64]             384\n",
      "           Conv2d-73          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-74          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-75          [-1, 384, 32, 32]             768\n",
      "             ReLU-76          [-1, 384, 32, 32]               0\n",
      "      fire_module-77          [-1, 384, 32, 32]               0\n",
      "           Conv2d-78          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-79          [-1, 192, 32, 32]             384\n",
      "           Conv2d-80          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-81          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-82          [-1, 384, 32, 32]             768\n",
      "             ReLU-83          [-1, 384, 32, 32]               0\n",
      "      fire_module-84          [-1, 384, 32, 32]               0\n",
      "           Conv2d-85          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-86          [-1, 192, 32, 32]             384\n",
      "           Conv2d-87          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-88          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-89          [-1, 384, 32, 32]             768\n",
      "             ReLU-90          [-1, 384, 32, 32]               0\n",
      "      fire_module-91          [-1, 384, 32, 32]               0\n",
      "           Conv2d-92          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-93          [-1, 192, 32, 32]             384\n",
      "           Conv2d-94          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-95          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-96          [-1, 384, 32, 32]             768\n",
      "             ReLU-97          [-1, 384, 32, 32]               0\n",
      "      fire_module-98          [-1, 384, 32, 32]               0\n",
      "           Conv2d-99          [-1, 256, 32, 32]          98,304\n",
      "     BatchNorm2d-100          [-1, 256, 32, 32]             512\n",
      "          Conv2d-101          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-102          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-103          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-104          [-1, 512, 16, 16]               0\n",
      "     fire_module-105          [-1, 512, 16, 16]               0\n",
      "          Conv2d-106          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-107          [-1, 256, 16, 16]             512\n",
      "          Conv2d-108          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-109          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-110          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-111          [-1, 512, 16, 16]               0\n",
      "     fire_module-112          [-1, 512, 16, 16]               0\n",
      "          Conv2d-113          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-114          [-1, 256, 16, 16]             512\n",
      "          Conv2d-115          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-116          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-117          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-118          [-1, 512, 16, 16]               0\n",
      "     fire_module-119          [-1, 512, 16, 16]               0\n",
      "          Conv2d-120          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-121          [-1, 256, 16, 16]             512\n",
      "          Conv2d-122          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-123          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-124          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-125          [-1, 512, 16, 16]               0\n",
      "     fire_module-126          [-1, 512, 16, 16]               0\n",
      "          Conv2d-127          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-128          [-1, 256, 16, 16]             512\n",
      "          Conv2d-129          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-130          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-131          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-132          [-1, 512, 16, 16]               0\n",
      "     fire_module-133          [-1, 512, 16, 16]               0\n",
      "          Conv2d-134          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-135          [-1, 256, 16, 16]             512\n",
      "          Conv2d-136          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-137          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-139          [-1, 512, 16, 16]               0\n",
      "     fire_module-140          [-1, 512, 16, 16]               0\n",
      "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
      "          Conv2d-143          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-144          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-145          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-146          [-1, 512, 16, 16]               0\n",
      "     fire_module-147          [-1, 512, 16, 16]               0\n",
      "          Conv2d-148          [-1, 192, 16, 16]          98,304\n",
      "     BatchNorm2d-149          [-1, 192, 16, 16]             384\n",
      "          Conv2d-150          [-1, 192, 16, 16]          36,864\n",
      "          Conv2d-151          [-1, 192, 16, 16]           1,728\n",
      "     BatchNorm2d-152          [-1, 384, 16, 16]             768\n",
      "            ReLU-153          [-1, 384, 16, 16]               0\n",
      "     fire_module-154          [-1, 384, 16, 16]               0\n",
      " ConvTranspose2d-155          [-1, 384, 32, 32]       2,359,680\n",
      "           merge-156          [-1, 384, 32, 32]               0\n",
      "       hg_module-157          [-1, 384, 32, 32]               0\n",
      "          Conv2d-158          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-159          [-1, 192, 32, 32]             384\n",
      "          Conv2d-160          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-161          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-162          [-1, 384, 32, 32]             768\n",
      "            ReLU-163          [-1, 384, 32, 32]               0\n",
      "     fire_module-164          [-1, 384, 32, 32]               0\n",
      "          Conv2d-165          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-166          [-1, 192, 32, 32]             384\n",
      "          Conv2d-167          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-168          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-169          [-1, 384, 32, 32]             768\n",
      "            ReLU-170          [-1, 384, 32, 32]               0\n",
      "     fire_module-171          [-1, 384, 32, 32]               0\n",
      " ConvTranspose2d-172          [-1, 384, 64, 64]       2,359,680\n",
      "           merge-173          [-1, 384, 64, 64]               0\n",
      "       hg_module-174          [-1, 384, 64, 64]               0\n",
      "          Conv2d-175          [-1, 192, 64, 64]          73,728\n",
      "     BatchNorm2d-176          [-1, 192, 64, 64]             384\n",
      "          Conv2d-177          [-1, 192, 64, 64]          36,864\n",
      "          Conv2d-178          [-1, 192, 64, 64]           1,728\n",
      "     BatchNorm2d-179          [-1, 384, 64, 64]             768\n",
      "            ReLU-180          [-1, 384, 64, 64]               0\n",
      "     fire_module-181          [-1, 384, 64, 64]               0\n",
      "          Conv2d-182          [-1, 128, 64, 64]          49,152\n",
      "     BatchNorm2d-183          [-1, 128, 64, 64]             256\n",
      "          Conv2d-184          [-1, 128, 64, 64]          16,384\n",
      "          Conv2d-185          [-1, 128, 64, 64]           1,152\n",
      "     BatchNorm2d-186          [-1, 256, 64, 64]             512\n",
      "            ReLU-187          [-1, 256, 64, 64]               0\n",
      "     fire_module-188          [-1, 256, 64, 64]               0\n",
      " ConvTranspose2d-189        [-1, 256, 128, 128]       1,048,832\n",
      "           merge-190        [-1, 256, 128, 128]               0\n",
      "       hg_module-191        [-1, 256, 128, 128]               0\n",
      "          Conv2d-192        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-193        [-1, 128, 128, 128]             256\n",
      "          Conv2d-194        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-195        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-196        [-1, 256, 128, 128]             512\n",
      "            ReLU-197        [-1, 256, 128, 128]               0\n",
      "     fire_module-198        [-1, 256, 128, 128]               0\n",
      "          Conv2d-199        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-200        [-1, 128, 128, 128]             256\n",
      "          Conv2d-201        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-202        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-203        [-1, 256, 128, 128]             512\n",
      "            ReLU-204        [-1, 256, 128, 128]               0\n",
      "     fire_module-205        [-1, 256, 128, 128]               0\n",
      " ConvTranspose2d-206        [-1, 256, 256, 256]       1,048,832\n",
      "           merge-207        [-1, 256, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 10,025,088\n",
      "Trainable params: 10,025,088\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 64.00\n",
      "Forward/backward pass size (MB): 3249.75\n",
      "Params size (MB): 38.24\n",
      "Estimated Total Size (MB): 3351.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class upsample(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.interpolate(x, scale_factor=self.scale_factor)\n",
    "\n",
    "class merge(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "def _make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, out_dim)]\n",
    "    layers += [residual(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [residual(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_pool_layer(dim):\n",
    "    return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "def _make_unpool_layer(dim):\n",
    "    return upsample(scale_factor=2)\n",
    "\n",
    "def _make_merge_layer(dim):\n",
    "    return merge()\n",
    "\n",
    "class fire_module(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, sr=2, stride=1):\n",
    "        super(fire_module, self).__init__()\n",
    "        self.conv1    = nn.Conv2d(inp_dim, out_dim // sr, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1      = nn.BatchNorm2d(out_dim // sr)\n",
    "        self.conv_1x1 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=1, stride=stride, bias=False)\n",
    "        self.conv_3x3 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=3, padding=1, \n",
    "                                  stride=stride, groups=out_dim // sr, bias=False)\n",
    "        self.bn2      = nn.BatchNorm2d(out_dim)\n",
    "        self.skip     = (stride == 1 and inp_dim == out_dim)\n",
    "        self.relu     = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        conv2 = torch.cat((self.conv_1x1(bn1), self.conv_3x3(bn1)), 1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "        if self.skip:\n",
    "            return self.relu(bn2 + x)\n",
    "        else:\n",
    "            return self.relu(bn2)\n",
    "\n",
    "class hg_module(nn.Module):\n",
    "    def __init__(\n",
    "        self, n, dims, modules, make_up_layer=_make_layer,\n",
    "        make_pool_layer=_make_pool_layer, make_hg_layer=_make_layer,\n",
    "        make_low_layer=_make_layer, make_hg_layer_revr=_make_layer_revr,\n",
    "        make_unpool_layer=_make_unpool_layer, make_merge_layer=_make_merge_layer\n",
    "    ):\n",
    "        super(hg_module, self).__init__()\n",
    "\n",
    "        curr_mod = modules[0]\n",
    "        next_mod = modules[1]\n",
    "\n",
    "        curr_dim = dims[0]\n",
    "        next_dim = dims[1]\n",
    "\n",
    "        self.n    = n\n",
    "        self.up1  = make_up_layer(curr_dim, curr_dim, curr_mod)\n",
    "        self.max1 = make_pool_layer(curr_dim)\n",
    "        self.low1 = make_hg_layer(curr_dim, next_dim, curr_mod)\n",
    "        self.low2 = hg_module(\n",
    "            n - 1, dims[1:], modules[1:],\n",
    "            make_up_layer=make_up_layer,\n",
    "            make_pool_layer=make_pool_layer,\n",
    "            make_hg_layer=make_hg_layer,\n",
    "            make_low_layer=make_low_layer,\n",
    "            make_hg_layer_revr=make_hg_layer_revr,\n",
    "            make_unpool_layer=make_unpool_layer,\n",
    "            make_merge_layer=make_merge_layer\n",
    "        ) if n > 1 else make_low_layer(next_dim, next_dim, next_mod)\n",
    "        self.low3 = make_hg_layer_revr(next_dim, curr_dim, curr_mod)\n",
    "        self.up2  = make_unpool_layer(curr_dim)\n",
    "        self.merg = make_merge_layer(curr_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        up1  = self.up1(x)\n",
    "        max1 = self.max1(x)\n",
    "        low1 = self.low1(max1)\n",
    "        low2 = self.low2(low1)\n",
    "        low3 = self.low3(low2)\n",
    "        up2  = self.up2(low3)\n",
    "        print(up1.shape)\n",
    "        print(up2.shape)\n",
    "        merg = self.merg(up1, up2)\n",
    "        return merg\n",
    "#         return up1\n",
    "\n",
    "    \n",
    "def make_pool_layer(dim):\n",
    "    return nn.Sequential()\n",
    "\n",
    "def make_unpool_layer(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "def make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [fire_module(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_hg_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim, stride=2)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "m = hg_module(\n",
    "                4, [256, 256, 384, 384, 512], [2, 2, 2, 2, 4],\n",
    "                make_pool_layer=make_pool_layer,\n",
    "                make_unpool_layer=make_unpool_layer,\n",
    "                make_up_layer=make_layer,\n",
    "                make_low_layer=make_layer,\n",
    "                make_hg_layer_revr=make_layer_revr,\n",
    "                make_hg_layer=make_hg_layer\n",
    "            )\n",
    "\n",
    "summary(m, (256, 256, 256))\n",
    "# m(torch.rand(1, 256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 256, 200, 200]          65,792\n",
      "              ReLU-2        [-1, 256, 200, 200]               0\n",
      "       convolution-3        [-1, 256, 200, 200]               0\n",
      "            Conv2d-4         [-1, 80, 200, 200]          20,560\n",
      "================================================================\n",
      "Total params: 86,352\n",
      "Trainable params: 86,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.06\n",
      "Forward/backward pass size (MB): 258.79\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 298.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def _pred_mod(dim):\n",
    "        return nn.Sequential(\n",
    "            convolution(1, 256, 256, with_bn=False),\n",
    "            nn.Conv2d(256, dim, (1, 1))\n",
    "        )\n",
    "m = _pred_mod(80)\n",
    "summary(m, (256, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-2        [-1, 128, 100, 100]             256\n",
      "              ReLU-3        [-1, 128, 100, 100]               0\n",
      "       convolution-4        [-1, 128, 100, 100]               0\n",
      "           TopPool-5        [-1, 128, 100, 100]               0\n",
      "            Conv2d-6        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-7        [-1, 128, 100, 100]             256\n",
      "              ReLU-8        [-1, 128, 100, 100]               0\n",
      "       convolution-9        [-1, 128, 100, 100]               0\n",
      "         LeftPool-10        [-1, 128, 100, 100]               0\n",
      "           Conv2d-11        [-1, 256, 100, 100]         294,912\n",
      "      BatchNorm2d-12        [-1, 256, 100, 100]             512\n",
      "           Conv2d-13        [-1, 256, 100, 100]          65,536\n",
      "      BatchNorm2d-14        [-1, 256, 100, 100]             512\n",
      "             ReLU-15        [-1, 256, 100, 100]               0\n",
      "           Conv2d-16        [-1, 256, 100, 100]         589,824\n",
      "      BatchNorm2d-17        [-1, 256, 100, 100]             512\n",
      "             ReLU-18        [-1, 256, 100, 100]               0\n",
      "      convolution-19        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 1,542,144\n",
      "Trainable params: 1,542,144\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9.77\n",
      "Forward/backward pass size (MB): 273.44\n",
      "Params size (MB): 5.88\n",
      "Estimated Total Size (MB): 289.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models.py_utils import TopPool, LeftPool\n",
    "\n",
    "class corner_pool(nn.Module):\n",
    "    def __init__(self, dim, pool1, pool2):\n",
    "        super(corner_pool, self).__init__()\n",
    "        self._init_layers(dim, pool1, pool2)\n",
    "\n",
    "    def _init_layers(self, dim, pool1, pool2):\n",
    "        self.p1_conv1 = convolution(3, dim, 128)\n",
    "        self.p2_conv1 = convolution(3, dim, 128)\n",
    "\n",
    "        self.p_conv1 = nn.Conv2d(128, dim, (3, 3), padding=(1, 1), bias=False)\n",
    "        self.p_bn1   = nn.BatchNorm2d(dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (1, 1), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = convolution(3, dim, dim)\n",
    "\n",
    "        self.pool1 = pool1()\n",
    "        self.pool2 = pool2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pool 1\n",
    "        p1_conv1 = self.p1_conv1(x)\n",
    "        pool1    = self.pool1(p1_conv1)\n",
    "\n",
    "        # pool 2\n",
    "        p2_conv1 = self.p2_conv1(x)\n",
    "        pool2    = self.pool2(p2_conv1)\n",
    "\n",
    "        # pool 1 + pool 2\n",
    "        p_conv1 = self.p_conv1(pool1 + pool2)\n",
    "        p_bn1   = self.p_bn1(p_conv1)\n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(p_bn1 + bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        return conv2\n",
    "\n",
    "m = corner_pool(256, TopPool, LeftPool)\n",
    "summary(m, (256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AddCoords-1              [-1, 3, 3, 3]               0\n",
      "            Conv2d-2              [-1, 8, 3, 3]             224\n",
      "            Conv2d-3             [-1, 16, 3, 3]           1,168\n",
      "            Conv2d-4             [-1, 32, 3, 3]           4,640\n",
      "            Conv2d-5            [-1, 128, 3, 3]          36,864\n",
      "       BatchNorm2d-6            [-1, 128, 3, 3]             256\n",
      "              ReLU-7            [-1, 128, 3, 3]               0\n",
      "       convolution-8            [-1, 128, 3, 3]               0\n",
      "           TopPool-9            [-1, 128, 3, 3]               0\n",
      "           Conv2d-10            [-1, 128, 3, 3]          36,864\n",
      "      BatchNorm2d-11            [-1, 128, 3, 3]             256\n",
      "             ReLU-12            [-1, 128, 3, 3]               0\n",
      "      convolution-13            [-1, 128, 3, 3]               0\n",
      "         LeftPool-14            [-1, 128, 3, 3]               0\n",
      "           Conv2d-15             [-1, 32, 3, 3]          36,864\n",
      "      BatchNorm2d-16             [-1, 32, 3, 3]              64\n",
      "           Conv2d-17             [-1, 32, 3, 3]           1,024\n",
      "      BatchNorm2d-18             [-1, 32, 3, 3]              64\n",
      "             ReLU-19             [-1, 32, 3, 3]               0\n",
      "           Conv2d-20             [-1, 32, 3, 3]           9,216\n",
      "      BatchNorm2d-21             [-1, 32, 3, 3]              64\n",
      "             ReLU-22             [-1, 32, 3, 3]               0\n",
      "      convolution-23             [-1, 32, 3, 3]               0\n",
      "      corner_pool-24             [-1, 32, 3, 3]               0\n",
      "           Conv2d-25              [-1, 1, 3, 3]              33\n",
      "           Conv2d-26              [-1, 1, 3, 3]               2\n",
      "================================================================\n",
      "Total params: 127,603\n",
      "Trainable params: 127,603\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.49\n",
      "Estimated Total Size (MB): 0.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.width = width\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = corner_pool(32, TopPool, LeftPool)\n",
    "        self.conv4 = nn.Conv2d(32, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(1, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C_in, H, W)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "m = SimpleNet(width=3)\n",
    "summary(m, (1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74535599, -0.47140452, -0.33333333],\n",
       "       [-0.66666667, -0.33333333, -0.        ],\n",
       "       [-0.74535599, -0.47140452, -0.33333333]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# def norm(x, width):\n",
    "#     return x * width\n",
    "\n",
    "def norm(x, width):\n",
    "    return (int)(x * (width - 1) + 0.5)\n",
    "\n",
    "def _draw_rect(points, width=64):\n",
    "    x0, y0, x1, y1 = points\n",
    "    print(x0, y0, x1, y1)\n",
    "    x0 = norm(x0, width)\n",
    "    y0 = norm(y0, width)\n",
    "    x1 = norm(x1, width)\n",
    "    y1 = norm(y1, width)\n",
    "#     if (x1 == 1):\n",
    "#         x1 -= 0.1\n",
    "#     if (y1 == 1):\n",
    "#         y1 -= 0.1\n",
    "    print(x0, y0, x1, y1)\n",
    "    im = Image.new(\"F\", (width, width))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.rectangle([x0, y0, x1, y1], fill=1, outline=None, width=0)\n",
    "    im = np.array(im)  # (H, W)\n",
    "    print(im)\n",
    "    im = np.expand_dims(im, axis=-1)  # (H, W, 1)\n",
    "    return im\n",
    "\n",
    "def draw_rect(xy, width=3):\n",
    "    x0, y0, x1, y1 = xy\n",
    "    rect = np.zeros()\n",
    "    \n",
    "\n",
    "width = 3\n",
    "xy = []\n",
    "for x0, y0 in itertools.product(range(width), range(width)):\n",
    "    for _w, _h in itertools.product(range(1, width - x0 + 1), range(1, width-y0+1)):\n",
    "        x1 = x0 + _w\n",
    "        y1 = y0 + _h\n",
    "        xy.append([x0, y0, x1, y1])\n",
    "#         x = np.array([x0, y0, x1, y1], dtype=float)\n",
    "#         x /= width\n",
    "#         draw_rect(x, width)\n",
    "        \n",
    "for (x0, y0, x1, y1) in xy:\n",
    "    rect = np.zeros((width, width))\n",
    "    for i, j in itertools.product(range(x0, x1), range(y0, y1)):\n",
    "        rect[i][j] = 1.\n",
    "#     print(rect)\n",
    "\n",
    "# x = np.array([1,2], dtype=int)\n",
    "# x = x.astype(float) / 2\n",
    "x = np.stack([np.array([1,2]), np.array([3,4]), np.array([3,4])])\n",
    "len(x)\n",
    "\n",
    "def draw_l2_distance(x, y, width=64):\n",
    "    im = np.zeros((width, width), dtype=float)\n",
    "    for (i, j), _ in np.ndenumerate(im):\n",
    "        im[i][j] = -np.linalg.norm(np.array([x, y]) - np.array([i, j])) / width\n",
    "#     im = im.transpose(1, 0).reshape(width * width)  # (W, H) -> (H, W) -> (H*W)\n",
    "    return im\n",
    "\n",
    "draw_l2_distance(1, 2, width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 100, 100])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.rand(1,3,100, 100)\n",
    "m = nn.MaxPool2d(2)\n",
    "x1 = m(x0)\n",
    "x1 = F.interpolate(x1, scale_factor=2)\n",
    "# x = torch.cat((x0, x1), dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets...\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_rect_pil(xy, width=64):\n",
    "    x0, y0, x1, y1 = xy\n",
    "    x1 -= 0.5\n",
    "    y1 -= 0.5\n",
    "    im = Image.new(\"F\", (width, width))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.rectangle([x0, y0, x1, y1], fill=1)\n",
    "    im = np.array(im)  # (H, W)\n",
    "    return im\n",
    "\n",
    "\n",
    "def rand_draw(draw_fn=draw_rect_pil, n_strokes=2, width=64):\n",
    "    canvas = np.zeros((width, width, 3), dtype=np.int8)\n",
    "    im = [canvas.copy()]\n",
    "    x = []\n",
    "    for _ in range(n_strokes):\n",
    "        x0, y0 = np.random.randint(width, size=2)\n",
    "        x1 = x0 + np.random.randint(1, width - x0 + 1)\n",
    "        y1 = y0 + np.random.randint(1, width - y0 + 1)\n",
    "        _x = np.array((x0, y0, x1, y1))\n",
    "        #         _x = np.random.rand(action_dim)\n",
    "        color = np.random.randint(255, size=(3))  # (3)\n",
    "        #         x.append(np.concatenate((_x, color / 255.0)))\n",
    "        stroke = draw_fn(_x, width)  # (w, w)\n",
    "        stroke = np.expand_dims(stroke, axis=2)  # (w, w, 1)\n",
    "        canvas = canvas * (1 - stroke) + stroke * color  # (w, h, 3)\n",
    "        x.append(_x)\n",
    "        im.append(canvas.copy())\n",
    "    x = np.stack(x) / width  # (n_strokes, action_dim+3)\n",
    "    im = np.stack(im)\n",
    "    return x, im\n",
    "\n",
    "def generate_data(width=128, n_sample=1000, n_strokes=2):\n",
    "    print(\"Generating datasets...\")\n",
    "    if not os.path.exists(\"data-rect/\"):\n",
    "        os.makedirs(\"data-rect/\")\n",
    "\n",
    "    x, im = [], []\n",
    "    for _ in range(n_sample):\n",
    "        _x, _im = rand_draw(n_strokes=n_strokes, width=width)\n",
    "        _im_inter = _im[:-1]\n",
    "        _im_target = _im[-1]\n",
    "#         print(_im_inter.shape)\n",
    "        for i in range(n_strokes):\n",
    "            im.append(np.concatenate((_im_inter[i], _im_target), axis=2))\n",
    "            x.append(_x[i])\n",
    "    print(len(x))\n",
    "    print(len(im))\n",
    "#     print(im[0][:,:,-3:])\n",
    "#     plt.imshow(Image.fromarray(im[1][:,:,:-3].astype('uint8')))\n",
    "    plt.imshow(Image.fromarray(im[1][:,:,-3:].astype('uint8')))\n",
    "\n",
    "generate_data(width=16, n_sample=1, n_strokes=2)\n",
    "# a = np.array([1,2,3])\n",
    "# a = np.random.rand(5,3)\n",
    "# b = np.array([3,4,5])\n",
    "# b = np.random.rand(5,3)\n",
    "# np.concatenate([a,b],axis=1).shape\n",
    "# x[:-1]\n",
    "\n",
    "# x, im = rand_draw()\n",
    "# fig, ax = plt.subplots(1,3)\n",
    "# ax[0].imshow(Image.fromarray(im[0].astype(np.uint8)))\n",
    "# ax[1].imshow(Image.fromarray(im[1].astype(np.uint8)))\n",
    "# ax[2].imshow(Image.fromarray(im[2].astype(np.uint8)))\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel, width, out_size=4, latent_size=128):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "#         self.conv0 = nn.Conv2d(in_channel + 2, latent_size, 3, padding=1)\n",
    "#         self.bn0 = BatchNorm2d(latent_size)\n",
    "#         self.conv1 = nn.Conv2d(latent_size, latent_size, 3, padding=1)\n",
    "#         self.bn1 = BatchNorm2d(latent_size)\n",
    "# self.conv2 = nn.Conv2d(latent_size, out_size, 1)\n",
    "        self.conv0 = nn.Conv2d(in_channel + 2, latent_size, 1, stride=1, padding=0)\n",
    "        self.bn0 = nn.BatchNorm2d(latent_size)\n",
    "        self.conv2 = nn.Conv2d(latent_size, out_size, 1, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(width, stride=width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_coords(x)\n",
    "        x = self.conv0(x)\n",
    "        x = F.relu(self.bn0(x))\n",
    "#         x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "    \n",
    "class HRNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(HRNet, self).__init__()\n",
    "        cfg = get_cfg_defaults()\n",
    "        cfg.merge_from_file(\"./hrnet.yaml\")\n",
    "        self.hr0 = HighResolutionNet(cfg)\n",
    "        self.hr1 = HighResolutionNet(cfg)\n",
    "        self.rg = Regressor(in_channel=540, width=int(width/4))\n",
    "        \n",
    "    def forward(self, im_current, im_target):\n",
    "        x0 = self.hr0(im_current)\n",
    "        x1 = self.hr1(im_target)\n",
    "        x = torch.cat((x0, x1), dim=1)\n",
    "        x = self.rg(x)\n",
    "        return x\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, in_dim=128, out_dim=4, hidden_dim=128):\n",
    "        super(Painter, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden_dim)\n",
    "        self.fc0 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "m = HRNet(64)\n",
    "x0 = torch.rand(1,3,64,64)\n",
    "x1 = torch.rand(1,3,64,64)\n",
    "m(x0, x1).shape\n",
    "\n",
    "# summary(m, (1, 128))\n",
    "\n",
    "# rnn = nn.GRU(10, 20)\n",
    "# input = torch.randn(5, 3, 10)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# # output, hn = rnn(input, h0)\n",
    "# output, hn = rnn(input)\n",
    "# print(output.shape)\n",
    "# print(hn.shape)\n",
    "\n",
    "\n",
    "# summary(rnn, (3, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
