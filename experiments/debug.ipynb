{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.conv as conv\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from coordconv import AddCoords\n",
    "from hrnet import HighResolutionNet\n",
    "from config import get_cfg_defaults\n",
    "\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([3.])\n",
      "tensor([[ 3.,  2.,  1.,  0., -1.]])\n",
      "tensor([[0., 0., 6., 6., 6.]])\n",
      "tensor([[6., 6., 6., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.sigmoid(torch.rand(3, 2)) * 5\n",
    "x = torch.tensor([1]).float()\n",
    "l = 5\n",
    "N = 1\n",
    "x1 = x\n",
    "x2 = x+2.\n",
    "print(x1, x2)\n",
    "_x1 = F.relu6((torch.arange(l).expand(N, -1).float() - x1) * 6)\n",
    "_x2 = F.relu6((x2 - torch.arange(l).expand(N, -1).float()) * 6)\n",
    "print((x2 - torch.arange(l).expand(N, -1).float()))\n",
    "print(_x1)\n",
    "print(_x2)\n",
    "y = _x1 * _x2\n",
    "y = y / (36)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2728, 0.6679],\n",
      "        [0.2252, 0.6757],\n",
      "        [0.1806, 0.6840]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.0000, 0.0000, 3.8174, 6.0000, 6.0000],\n",
      "        [0.0000, 0.0000, 5.2428, 6.0000, 6.0000],\n",
      "        [0.0000, 0.5822, 6.0000, 6.0000, 6.0000]], grad_fn=<HardtanhBackward0>)\n",
      "tensor([[6.0000, 6.0000, 6.0000, 2.0382, 0.0000],\n",
      "        [6.0000, 6.0000, 6.0000, 2.2699, 0.0000],\n",
      "        [6.0000, 6.0000, 6.0000, 2.5186, 0.0000]], grad_fn=<HardtanhBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.6362, 0.3397, 0.0000],\n",
       "        [0.0000, 0.0000, 0.8738, 0.3783, 0.0000],\n",
       "        [0.0000, 0.0970, 1.0000, 0.4198, 0.0000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid())\n",
    "x = m(torch.rand(3, 1))\n",
    "print(x)\n",
    "\n",
    "x1 = x[:, 0].view(-1, 1)*5\n",
    "x2 = x[:, 1].view(-1, 1)*5\n",
    "\n",
    "_x1 = F.relu6((torch.arange(5).expand(3,-1).float()-x1) * 6)\n",
    "_x2 = F.relu6((x2-torch.arange(5).float()) * 6)\n",
    "print(_x1)\n",
    "print(_x2)\n",
    "y = _x1 * _x2\n",
    "y = y / 36.\n",
    "y\n",
    "# y = y / 36.\n",
    "# print(_x1)\n",
    "# _x2 = F.relu6((x2-torch.arange(5).float()) * 6)\n",
    "# print(_x2)\n",
    "# y = _x1 * _x2\n",
    "# y = y / 36.\n",
    "\n",
    "# torch.arange(5).expand(3,-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 5.4000, 6.0000, 6.0000, 6.0000], grad_fn=<HardtanhBackward0>)\n",
      "tensor([6.0000, 6.0000, 6.0000, 0.6000, 0.0000], grad_fn=<HardtanhBackward0>)\n",
      "tensor([0.0000, 0.9000, 1.0000, 0.1000, 0.0000], grad_fn=<DivBackward0>)\n",
      "tensor([0.0000, 0.9000, 1.0000, 0.1000, 0.0000], grad_fn=<DivBackward0>)\n",
      "tensor(-99.6000, grad_fn=<MeanBackward0>)\n",
      "tensor([-0.2000])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor([2.1, 2.1, 2.1, 2.1, 2.1], requires_grad=True)\n",
    "x1 = torch.tensor([0.1], requires_grad=True)\n",
    "x2 = torch.tensor([3.1], requires_grad=True)\n",
    "gt = torch.tensor([100.])\n",
    "\n",
    "_x1 = F.relu6((torch.arange(5).float()-x1) * 6)\n",
    "print(_x1)\n",
    "_x2 = F.relu6((x2-torch.arange(5).float()) * 6)\n",
    "print(_x2)\n",
    "y = _x1 * _x2\n",
    "y = y / 36.\n",
    "# y = y - torch.tensor([1., 1., 2., 0, 0])\n",
    "# y = y - torch.arange(5).float()\n",
    "print(y)\n",
    "# y = x * 100\n",
    "# y = y * 100\n",
    "# y = torch.arange(10).float() - y\n",
    "# y = y.div(y + 1e-6)\n",
    "# y = y / (y + .0001)\n",
    "# y = y * y * y\n",
    "\n",
    "loss = (y - gt).mean()\n",
    "loss.backward()\n",
    "print(y)\n",
    "print(loss)\n",
    "print(x1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3., 4.]), tensor([[1., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 1.]]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = Faker()\n",
    "\n",
    "img_size = 5\n",
    "index = fake.pyint(min=0, max=img_size-1)\n",
    "F.one_hot(torch.tensor(index), img_size)\n",
    "torch.arange(0,img_size)\n",
    "\n",
    "def sampler():\n",
    "    x, y = [], []\n",
    "    for i in range(img_size):\n",
    "        x.append(torch.tensor(i))\n",
    "        y.append(F.one_hot(torch.tensor(i),img_size))\n",
    "    x = torch.stack(x).float()\n",
    "    y = torch.stack(y).float()\n",
    "    return x, y\n",
    "\n",
    "sampler()\n",
    "\n",
    "\n",
    "\n",
    "# a = torch.tensor([3.],  requires_grad=True)\n",
    "# y = a.expand(int(a[0]))\n",
    "# gt = torch.tensor([1., 1., 1.])\n",
    "\n",
    "# b = torch.tensor([2., 2.],  requires_grad=True)\n",
    "# (x > 1) & (x > 2)\n",
    "# a = x.add(1e-8)\n",
    "# a = x/x\n",
    "# y = x2.sum()\n",
    "# y.backward()\n",
    "# print(x.grad)\n",
    "# x\n",
    "\n",
    "# c = torch.cat((a,b), 0)\n",
    "# y = c.mean()\n",
    "# loss = (gt - y).mean()\n",
    "# loss.backward()\n",
    "\n",
    "# print(y)\n",
    "# print(loss)\n",
    "# print(a.grad)\n",
    "\n",
    "# y = a * 10\n",
    "# y.backward()\n",
    "# print(a.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,088\n",
      "         LeakyReLU-2           [-1, 64, 32, 32]               0\n",
      "            Conv2d-3          [-1, 128, 16, 16]         131,200\n",
      "    InstanceNorm2d-4          [-1, 128, 16, 16]               0\n",
      "         LeakyReLU-5          [-1, 128, 16, 16]               0\n",
      "            Conv2d-6            [-1, 256, 8, 8]         524,544\n",
      "    InstanceNorm2d-7            [-1, 256, 8, 8]               0\n",
      "         LeakyReLU-8            [-1, 256, 8, 8]               0\n",
      "            Conv2d-9            [-1, 512, 4, 4]       2,097,664\n",
      "   InstanceNorm2d-10            [-1, 512, 4, 4]               0\n",
      "        LeakyReLU-11            [-1, 512, 4, 4]               0\n",
      "        ZeroPad2d-12            [-1, 512, 5, 5]               0\n",
      "           Conv2d-13              [-1, 1, 4, 4]           8,192\n",
      "================================================================\n",
      "Total params: 2,762,688\n",
      "Trainable params: 2,762,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 2.41\n",
      "Params size (MB): 10.54\n",
      "Estimated Total Size (MB): 12.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "#         img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img)\n",
    "\n",
    "\n",
    "d = Discriminator()\n",
    "\n",
    "summary(d, (1, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_real_samples(n_sample=100, save_path=\"data/layout/\"):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    width, height = 28, 28\n",
    "    fake = Faker()\n",
    "\n",
    "    def _sample(save_path):\n",
    "        n_circles = fake.pyint(min=1, max=3)\n",
    "        radius = fake.pyint(min=2, max=5)\n",
    "        space = fake.pyint(min=2, max=4)\n",
    "        x0 = fake.pyint(min=0, max=width - 1 - (n_circles * (radius + space)))\n",
    "        y0 = fake.pyint(min=0, max=height - 1 - radius)\n",
    "\n",
    "        im = Image.new(\"L\", (width, height))\n",
    "        draw = ImageDraw.Draw(im)\n",
    "\n",
    "        for _ in range(n_circles):\n",
    "            draw.rectangle((x0, y0, x0 + radius, y0 + radius), fill=255)\n",
    "            x0 += radius + space\n",
    "#         plt.imshow(im)\n",
    "#         im.save(save_path, \"PNG\")\n",
    "\n",
    "    for i in range(n_sample):\n",
    "        _sample(os.path.join(save_path, \"%d.png\" % (i)))\n",
    "\n",
    "generate_real_samples(n_sample=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f191f640208>"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = Image.open(\"data/0.png\")\n",
    "plt.imshow(np.array(im))\n",
    "# im.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAABACAIAAABdtOgoAAAFSElEQVR4nO2aSyi0XxjAz9xeGWM+FAvJ/ZaaiCJFLk0uiSIsbJCGDaWIkJRy2UluZWNDyCUbLIRxt3DZWLiUGLcwjeuMGYzzLU4d099n5vX/PnNenN9m3vd5n6d+c2bOO+c57wBAoVAoFAqFQqFQKBSOsbq6CiGEEO7v7/N4PNI6P4zAwEBoQkxMDGmj7wbf/OW8vDwzp5TPhc/nHx8fm86Au7s7Ozs70l4/hqSkJDTuLy8vBoMBHefm5pL2+jEMDAygQZ+fnx8aGkLHs7OzpL1+Br9+/Xp4eECDrlAo0tLS8Gzw9PQkbccWb2/v+vr65eXltrY20i4fpKioCI24Xq93cHAQiUSXl5coUldXR9qOLYODg3gNTdrlg6ysrCD1oaEhFGlra/taDYGjo6Ner8crCD8/P9JGrAkICMDeaWlpKBgeHv61GoLi4mLTJVxJSQlpI9Y0NTUhabVaLRKJcHx7exvFe3p6COqxZGNjA0I4OTmJnMfHx0kbsYPP5x8dHSHp9vZ200s1NTVfpSEIDg6GEG5uboaEhCBnrVZrY2ND2osFiYmJeNpGRESYXvLw8Hh5efkSDUFrayuEsLy8HABweHiInBMSEkh7saC/vx/p7u7uvr2qVCq53xAwDKNWq41Go5ubGwCgo6MDObe2tpJWs4Tp8r+2tvZtQkFBAfcbgszMTAjhzMwMOsUt/d7eHlkxyxQWFuLx9fLyepsglUp1Oh3HG4KJiQkIYUFBATq1sbG5u7v7GovR5eVlJLqwsPBeDt6i4GZD4Orq+vz8jPpHHBwdHUXOpaWlxMws4u/vj39+CwsL30tLSUnhckNQVVUFIRwZGTEN5ufnI+GpqSlSYpZpbGyEH4SDDcHOzg6EMCMjwzTo7OxsNBohhAaDwd7enpSbOUyX/+zhWkMQHR0NIby+vn675Md316ysLCJuf+T1iZhcLkeLtg8hkUgyMzP/qdJfkZ+fDwAYHh42GAymcbFYvLi4iI7x5goXeP0J7evry8nJAQBcXV3JZDKj0Wi+cnFx0cfHBwCgVCrj4uI+1ZIlEonk7OxMIpFotVq9Xo+CQqHQ1taWYRicptFoXFxcLL5B6yBEL1KpND09HR2PjY2dnJxYrBwcHKyurgYAxMTEeHp6HhwcfJokW7KzsyUSCQCAx+NpNBqNRqPT6R4fH3k8Ho/HE4lEoaGhUqnUyckpKipqbm6OtK8JCoUC39YTExPZlMhkMlzCkYZgYWEBQtjc3PxeAt5Rb2lpsaaYZZaWlpCZWq0WCoUsq7a2tlAVFxoCPz8/JBMUFPReTkJCAsrhwnx9BatDCLu7u9kX1tbW4kLiDQHaQt/c3DSTwzDM7e0tEg4LC7OamwUaGhrwOMrlcvaFvr6+HGkIBALByckJhLCsrMx85vDwMBJubGy0jpsF+Hy+SqVCThcXFwKB4EPl6+vrXGgI0tPTkYa7u7v5TLyZuLe3R/y2CQAAcrkcf4u7uro+Wl5eXo7LST0hEAgEa2trEMLj42OLyehBDSI1NdUKet8chmE6OzvRgOp0OmdnZ/P5GRkZ+AO4uLiIj4+3jud7cGAO/h03NzdSqRSfnp+fK5XK6+trAMD09DT+S0dWVlZkZKRMJouNjf3PMu/09LS3t7eystKK1t8I+D4VFRU4Df/J/o/8j3vvv+LLzwBHR0eDwfD8/Pz09MQwjK2trVgstrOzc3JyUqlUZ2dnKC05OVmv15+fn9/f32u1Wq1W+/j4yDAMwzBisfjh4eHm5obsG6FQKBQKhUKhUCgUCoVCoVAoFAqFQqFQKBQKhUL5VvwGA2JFEU0GJQwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=128x64 at 0x7F1928631DD8>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = \"A\"\n",
    "width = 64\n",
    "fonts = ['/notebooks/post-generator/asset/fonts_en/Roboto/Roboto-Regular.ttf', \n",
    "            '/notebooks/post-generator/asset/fonts_en/Alegreya/Alegreya-Regular.ttf']\n",
    "\n",
    "im = Image.new(\"RGB\", (width * 2, width))\n",
    "\n",
    "for i, fnt in enumerate(fonts):\n",
    "    _im = Image.new(\"RGB\", (width, width))\n",
    "    _draw = ImageDraw.Draw(_im)\n",
    "    _font = ImageFont.truetype(fnt, 40)\n",
    "    _w, _h = _draw.textsize(char, _font)  # size of token\n",
    "#     _draw.text(((width - _w) / 2, (width - _h) / 2), char, font=_font, fill=(255, 255, 255))\n",
    "    _draw.text((0, 0), char, font=_font, fill=(255, 255, 255))\n",
    "    im.paste(_im, (i * width, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f191f6c20b8>"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAM40lEQVR4nO3df6jd9X3H8edr+VltbUzrQmZkcRgq/jFjufgDpbQ6u8yV6h8iSilhBPKPG8oKNm4wKOwPZTDrH2MQqmv+cFVn6yJSarNbZQxK9FpjG02tqVNMFr1da7ArLE3se3+cb8o13OSe3PMr7vN8QDjn+z3f4/eN5z7v+XEP32+qCkn///3OpAeQNB7GLjXC2KVGGLvUCGOXGmHsUiMGij3JpiSvJNmfZNuwhpI0fFns39mTLAF+AlwPHACeA26rqpeHN56kYVk6wH0vB/ZX1WsASR4GbgROGvvyrKiVnD3ALiWdyv/yK35dRzLfbYPEfj7w5pzlA8AVp7rDSs7milw3wC4lncrumj7pbYPE3pckW4GtACs5a9S7k3QSg3xAdxC4YM7yum7d+1TV9qqaqqqpZawYYHeSBjFI7M8BG5JcmGQ5cCvwxHDGkjRsi34ZX1XHkvw58BSwBHiwql4a2mSShmqg9+xV9W3g20OaRdII+Q06qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRELxp7kwSSzSfbOWbc6ya4kr3aX5452TEmD6ueZ/evAphPWbQOmq2oDMN0tSzqDLXiut6r69yTrT1h9I/Dp7voO4Bngy0Oca6Se+q89kx5hXn/8exvnXf9BmxfOzJlPNW8LFvuefU1VHequvwWsGdI8kkZk4A/oqqqAOtntSbYmmUkyc5Qjg+5O0iItNva3k6wF6C5nT7ZhVW2vqqmqmlrGikXuTtKgFhv7E8Dm7vpmYOdwxpE0Kv386e0bwPeBTyQ5kGQLcA9wfZJXgT/qliWdwfr5NP62k9x03ZBnkTRCfoNOaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdakQ/p3+6IMnTSV5O8lKSO7r1q5PsSvJqd3nu6MeVtFj9PLMfA75UVZcAVwK3J7kE2AZMV9UGYLpblnSGWjD2qjpUVT/orv8S2AecD9wI7Og22wHcNKohJQ3utN6zJ1kPXAbsBtZU1aHupreANUOdTNJQ9R17kg8D3wTurKp3595WVQXUSe63NclMkpmjHBloWEmL11fsSZbRC/2hqvpWt/rtJGu729cCs/Pdt6q2V9VUVU0tY8UwZpa0COk9KZ9igyT03pP/oqrunLP+74CfV9U9SbYBq6vqrlP9t87J6rointZdGpXdNc279YvMd9vSPu5/NfBF4EdJ9nTr/gq4B3g0yRbgDeCWYQwraTQWjL2q/gOY9zcF4NO09AHhN+ikRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRiwYe5KVSZ5N8mKSl5J8pVt/YZLdSfYneSTJ8tGPK2mx+nlmPwJcW1WXAhuBTUmuBO4F7quqi4B3gC2jG1PSoBaMvXr+p1tc1v0r4FrgsW79DuCmkUwoaSj6PT/7ku4MrrPALuCnwOGqOtZtcgA4fzQjShqGvmKvqveqaiOwDrgcuLjfHSTZmmQmycxRjixyTEmDOq1P46vqMPA0cBWwKsnxUz6vAw6e5D7bq2qqqqaWsWKgYSUtXj+fxp+XZFV3/UPA9cA+etHf3G22Gdg5qiElDW7pwpuwFtiRZAm9Xw6PVtWTSV4GHk7yt8ALwAMjnFPSgBaMvap+CFw2z/rX6L1/l/QB4DfopEYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUb0HXt32uYXkjzZLV+YZHeS/UkeSbJ8dGNKGtTpPLPfQe+EjsfdC9xXVRcB7wBbhjmYpOHqK/Yk64A/Bb7WLQe4Fnis22QHcNMoBpQ0HP0+s38VuAv4Tbf8MeBwVR3rlg8A5w95NklD1M/52T8HzFbV84vZQZKtSWaSzBzlyGL+E5KGoJ/zs18NfD7JDcBK4BzgfmBVkqXds/s64OB8d66q7cB2gHOyuoYytaTTtuAze1XdXVXrqmo9cCvwvar6AvA0cHO32WZg58imlDSwQf7O/mXgL5Psp/ce/oHhjCRpFPp5Gf9bVfUM8Ex3/TXg8uGPJGkU/Aad1Ahjlxph7FIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ii+zgiT5HXgl8B7wLGqmkqyGngEWA+8DtxSVe+MZkxJgzqdZ/bPVNXGqprqlrcB01W1AZjuliWdoQZ5GX8jsKO7vgO4afBxJI1Kv7EX8N0kzyfZ2q1bU1WHuutvAWuGPp2koen3LK7XVNXBJL8L7Ery47k3VlUlqfnu2P1y2AqwkrMGGlbS4vX1zF5VB7vLWeBxeqdqfjvJWoDucvYk991eVVNVNbWMFcOZWtJpWzD2JGcn+cjx68Bngb3AE8DmbrPNwM5RDSlpcP28jF8DPJ7k+Pb/XFXfSfIc8GiSLcAbwC2jG1PSoBaMvapeAy6dZ/3PgetGMZSk4fMbdFIjjF1qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGmHsUiOMXWqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI4xdaoSxS40wdqkRxi41wtilRhi71Ahjlxph7FIj+oo9yaokjyX5cZJ9Sa5KsjrJriSvdpfnjnpYSYvX7zP7/cB3qupieqeC2gdsA6aragMw3S1LOkP1cxbXjwKfAh4AqKpfV9Vh4EZgR7fZDuCmUQ0paXD9PLNfCPwM+KckLyT5Wnfq5jVVdajb5i16Z3uVdIbqJ/alwCeBf6yqy4BfccJL9qoqoOa7c5KtSWaSzBzlyKDzSlqkfmI/AByoqt3d8mP04n87yVqA7nJ2vjtX1faqmqqqqWWsGMbMkhZhwdir6i3gzSSf6FZdB7wMPAFs7tZtBnaOZEJJQ7G0z+3+AngoyXLgNeDP6P2ieDTJFuAN4JbRjChpGPqKvar2AFPz3HTdcMeRNCp+g05qhLFLjTB2qRHGLjXC2KVGGLvUCGOXGpHe19rHtLPkZ/S+gPNx4L/HtuP5nQkzgHOcyDne73Tn+P2qOm++G8Ya+293msxU1Xxf0mlqBudwjnHO4ct4qRHGLjViUrFvn9B+5zoTZgDnOJFzvN/Q5pjIe3ZJ4+fLeKkRY409yaYkryTZn2RsR6NN8mCS2SR756wb+6Gwk1yQ5OkkLyd5Kckdk5glycokzyZ5sZvjK936C5Ps7h6fR7rjF4xckiXd8Q2fnNQcSV5P8qMke5LMdOsm8TMyssO2jy32JEuAfwD+BLgEuC3JJWPa/deBTSesm8ShsI8BX6qqS4Argdu7/wfjnuUIcG1VXQpsBDYluRK4F7ivqi4C3gG2jHiO4+6gd3jy4yY1x2eqauOcP3VN4mdkdIdtr6qx/AOuAp6as3w3cPcY978e2Dtn+RVgbXd9LfDKuGaZM8NO4PpJzgKcBfwAuILelzeWzvd4jXD/67of4GuBJ4FMaI7XgY+fsG6sjwvwUeA/6T5LG/Yc43wZfz7w5pzlA926SZnoobCTrAcuA3ZPYpbupfMeegcK3QX8FDhcVce6Tcb1+HwVuAv4Tbf8sQnNUcB3kzyfZGu3btyPy0gP2+4HdJz6UNijkOTDwDeBO6vq3UnMUlXvVdVGes+slwMXj3qfJ0ryOWC2qp4f977ncU1VfZLe28zbk3xq7o1jelwGOmz7QsYZ+0HggjnL67p1k9LXobCHLckyeqE/VFXfmuQsANU7u8/T9F4ur0py/LiE43h8rgY+n+R14GF6L+Xvn8AcVNXB7nIWeJzeL8BxPy4DHbZ9IeOM/TlgQ/dJ63LgVnqHo56UsR8KO0nonUZrX1X9/aRmSXJeklXd9Q/R+9xgH73obx7XHFV1d1Wtq6r19H4evldVXxj3HEnOTvKR49eBzwJ7GfPjUqM+bPuoP/g44YOGG4Cf0Ht/+Ndj3O83gEPAUXq/PbfQe284DbwK/BuwegxzXEPvJdgPgT3dvxvGPQvwh8AL3Rx7gb/p1v8B8CywH/gXYMUYH6NPA09OYo5ufy92/146/rM5oZ+RjcBM99j8K3DusObwG3RSI/yATmqEsUuNMHapEcYuNcLYpUYYu9QIY5caYexSI/4PB85F15zX7FsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "width, height = 64, 64\n",
    "im = Image.new(\"L\", (width, width))\n",
    "\n",
    "draw = ImageDraw.Draw(im)\n",
    "n_circles = 3\n",
    "radius, space = 10, 3\n",
    "x0, y0 = 10, 10\n",
    "for i in range(n_circles):\n",
    "#     draw.ellipse((x0, y0, x0+radius, y0+radius), fill=1)\n",
    "    draw.rectangle((x0, y0, x0+radius, y0+radius), fill=1)\n",
    "    x0 += radius + space\n",
    "plt.imshow(np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AddCoords-1           [-1, 22, 16, 16]               0\n",
      "            Conv2d-2          [-1, 256, 16, 16]           5,888\n",
      "       BatchNorm2d-3          [-1, 256, 16, 16]             512\n",
      "              ReLU-4          [-1, 256, 16, 16]               0\n",
      "            Conv2d-5          [-1, 256, 16, 16]          65,792\n",
      "       BatchNorm2d-6          [-1, 256, 16, 16]             512\n",
      "              ReLU-7          [-1, 256, 16, 16]               0\n",
      "            Linear-8                    [-1, 3]         196,611\n",
      "            Conv2d-9          [-1, 256, 16, 16]           5,888\n",
      "      BatchNorm2d-10          [-1, 256, 16, 16]             512\n",
      "             ReLU-11          [-1, 256, 16, 16]               0\n",
      "           Conv2d-12          [-1, 256, 16, 16]          65,792\n",
      "      BatchNorm2d-13          [-1, 256, 16, 16]             512\n",
      "             ReLU-14          [-1, 256, 16, 16]               0\n",
      "           Linear-15                    [-1, 4]         262,148\n",
      "================================================================\n",
      "Total params: 604,167\n",
      "Trainable params: 604,167\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 6.04\n",
      "Params size (MB): 2.30\n",
      "Estimated Total Size (MB): 8.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel, width, latent_size=256, n_class=3, bbox_size=4):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv_class = nn.Sequential(\n",
    "            nn.Conv2d(in_channel + 2, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_size, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(latent_size, n_class, 1, stride=1, padding=0),\n",
    "            # nn.Linear(latent_size, n_class, 1, stride=1, padding=0),\n",
    "        )\n",
    "        self.conv_bbox = nn.Sequential(\n",
    "            nn.Conv2d(in_channel + 2, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_size, latent_size, 1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(latent_size),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(latent_size, bbox_dim, 1, stride=1, padding=0),\n",
    "        )\n",
    "        self.fc_class = nn.Linear((width ** 2) * latent_size, n_class)\n",
    "        self.fc_bbox = nn.Linear((width ** 2) * latent_size, bbox_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N = x.shape[0]\n",
    "        x = self.add_coords(x)\n",
    "        x_class = self.conv_class(x)\n",
    "        x_class = x_class.view(N, -1)\n",
    "        x_class = self.fc_class(x_class)\n",
    "        x_bbox = self.conv_bbox(x)\n",
    "        x_bbox = x_bbox.view(N, -1)\n",
    "        x_bbox = self.fc_bbox(x_bbox)\n",
    "        return x_class, x_bbox\n",
    "    \n",
    "m = Regressor(in_channel=20, width=16)\n",
    "summary(m, (20, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class HRNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(HRNet, self).__init__()\n",
    "        cfg = get_cfg_defaults()\n",
    "        cfg.merge_from_file(\"./exp.yaml\")\n",
    "        self.hr = HighResolutionNet(cfg)\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv5 = nn.Conv2d(7, 7, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(7, 7, 3, padding=1)\n",
    "        self.conv7 = nn.Conv2d(7, 4, 1)\n",
    "        self.pool = nn.MaxPool2d(width, stride=width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.hr(x)\n",
    "        x1 = F.interpolate(x1, scale_factor=4)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.conv7(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "m = HRNet(width=64)\n",
    "# summary(m, (3, 64, 64))\n",
    "x = m(torch.rand(1,1,64,64))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 200, 200]          18,816\n",
      "       BatchNorm2d-2        [-1, 128, 200, 200]             256\n",
      "              ReLU-3        [-1, 128, 200, 200]               0\n",
      "       convolution-4        [-1, 128, 200, 200]               0\n",
      "            Conv2d-5        [-1, 256, 100, 100]         294,912\n",
      "       BatchNorm2d-6        [-1, 256, 100, 100]             512\n",
      "              ReLU-7        [-1, 256, 100, 100]               0\n",
      "            Conv2d-8        [-1, 256, 100, 100]         589,824\n",
      "       BatchNorm2d-9        [-1, 256, 100, 100]             512\n",
      "           Conv2d-10        [-1, 256, 100, 100]          32,768\n",
      "      BatchNorm2d-11        [-1, 256, 100, 100]             512\n",
      "             ReLU-12        [-1, 256, 100, 100]               0\n",
      "         residual-13        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 938,112\n",
      "Trainable params: 938,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 332.03\n",
      "Params size (MB): 3.58\n",
      "Estimated Total Size (MB): 337.44\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from models.CornerNet import model\n",
    "\n",
    "# m = model()\n",
    "\n",
    "class convolution(nn.Module):\n",
    "    def __init__(self, k, inp_dim, out_dim, stride=1, with_bn=True):\n",
    "        super(convolution, self).__init__()\n",
    "\n",
    "        pad = (k - 1) // 2\n",
    "        self.conv = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(pad, pad), stride=(stride, stride), bias=not with_bn)\n",
    "        self.bn   = nn.BatchNorm2d(out_dim) if with_bn else nn.Sequential()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.conv(x)\n",
    "        bn   = self.bn(conv)\n",
    "        relu = self.relu(bn)\n",
    "        return relu\n",
    "\n",
    "class residual(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, k=3, stride=1):\n",
    "        super(residual, self).__init__()\n",
    "        p = (k - 1) // 2\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp_dim, out_dim, (k, k), padding=(p, p), stride=(stride, stride), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_dim, out_dim, (k, k), padding=(p, p), bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_dim)\n",
    "        \n",
    "        self.skip  = nn.Sequential(\n",
    "            nn.Conv2d(inp_dim, out_dim, (1, 1), stride=(stride, stride), bias=False),\n",
    "            nn.BatchNorm2d(out_dim)\n",
    "        ) if stride != 1 or inp_dim != out_dim else nn.Sequential()\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "\n",
    "        skip  = self.skip(x)\n",
    "        return self.relu(bn2 + skip)\n",
    "\n",
    "\n",
    "pre = nn.Sequential(\n",
    "            convolution(7, 3, 128, stride=2), residual(128, 256, stride=2)\n",
    "        )\n",
    "# pre(torch.from_numpy(np.random.rand(1, 3, 128, 128)).double())\n",
    "# conv = convolution(7, 3, 128, stride=2)\n",
    "x = torch.rand(1, 3, 512, 512)\n",
    "x = pre(x)\n",
    "# x.shape\n",
    "\n",
    "summary(pre, (3, 400, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 32, 32])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 384, 64, 64])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 128, 128])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "torch.Size([2, 256, 256, 256])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-2        [-1, 128, 256, 256]             256\n",
      "            Conv2d-3        [-1, 128, 256, 256]          16,384\n",
      "            Conv2d-4        [-1, 128, 256, 256]           1,152\n",
      "       BatchNorm2d-5        [-1, 256, 256, 256]             512\n",
      "              ReLU-6        [-1, 256, 256, 256]               0\n",
      "       fire_module-7        [-1, 256, 256, 256]               0\n",
      "            Conv2d-8        [-1, 128, 256, 256]          32,768\n",
      "       BatchNorm2d-9        [-1, 128, 256, 256]             256\n",
      "           Conv2d-10        [-1, 128, 256, 256]          16,384\n",
      "           Conv2d-11        [-1, 128, 256, 256]           1,152\n",
      "      BatchNorm2d-12        [-1, 256, 256, 256]             512\n",
      "             ReLU-13        [-1, 256, 256, 256]               0\n",
      "      fire_module-14        [-1, 256, 256, 256]               0\n",
      "           Conv2d-15        [-1, 128, 256, 256]          32,768\n",
      "      BatchNorm2d-16        [-1, 128, 256, 256]             256\n",
      "           Conv2d-17        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-18        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-19        [-1, 256, 128, 128]             512\n",
      "             ReLU-20        [-1, 256, 128, 128]               0\n",
      "      fire_module-21        [-1, 256, 128, 128]               0\n",
      "           Conv2d-22        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-23        [-1, 128, 128, 128]             256\n",
      "           Conv2d-24        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-25        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-26        [-1, 256, 128, 128]             512\n",
      "             ReLU-27        [-1, 256, 128, 128]               0\n",
      "      fire_module-28        [-1, 256, 128, 128]               0\n",
      "           Conv2d-29        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-30        [-1, 128, 128, 128]             256\n",
      "           Conv2d-31        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-32        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-33        [-1, 256, 128, 128]             512\n",
      "             ReLU-34        [-1, 256, 128, 128]               0\n",
      "      fire_module-35        [-1, 256, 128, 128]               0\n",
      "           Conv2d-36        [-1, 128, 128, 128]          32,768\n",
      "      BatchNorm2d-37        [-1, 128, 128, 128]             256\n",
      "           Conv2d-38        [-1, 128, 128, 128]          16,384\n",
      "           Conv2d-39        [-1, 128, 128, 128]           1,152\n",
      "      BatchNorm2d-40        [-1, 256, 128, 128]             512\n",
      "             ReLU-41        [-1, 256, 128, 128]               0\n",
      "      fire_module-42        [-1, 256, 128, 128]               0\n",
      "           Conv2d-43        [-1, 192, 128, 128]          49,152\n",
      "      BatchNorm2d-44        [-1, 192, 128, 128]             384\n",
      "           Conv2d-45          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-46          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-47          [-1, 384, 64, 64]             768\n",
      "             ReLU-48          [-1, 384, 64, 64]               0\n",
      "      fire_module-49          [-1, 384, 64, 64]               0\n",
      "           Conv2d-50          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-51          [-1, 192, 64, 64]             384\n",
      "           Conv2d-52          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-53          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-54          [-1, 384, 64, 64]             768\n",
      "             ReLU-55          [-1, 384, 64, 64]               0\n",
      "      fire_module-56          [-1, 384, 64, 64]               0\n",
      "           Conv2d-57          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-58          [-1, 192, 64, 64]             384\n",
      "           Conv2d-59          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-60          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-61          [-1, 384, 64, 64]             768\n",
      "             ReLU-62          [-1, 384, 64, 64]               0\n",
      "      fire_module-63          [-1, 384, 64, 64]               0\n",
      "           Conv2d-64          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-65          [-1, 192, 64, 64]             384\n",
      "           Conv2d-66          [-1, 192, 64, 64]          36,864\n",
      "           Conv2d-67          [-1, 192, 64, 64]           1,728\n",
      "      BatchNorm2d-68          [-1, 384, 64, 64]             768\n",
      "             ReLU-69          [-1, 384, 64, 64]               0\n",
      "      fire_module-70          [-1, 384, 64, 64]               0\n",
      "           Conv2d-71          [-1, 192, 64, 64]          73,728\n",
      "      BatchNorm2d-72          [-1, 192, 64, 64]             384\n",
      "           Conv2d-73          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-74          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-75          [-1, 384, 32, 32]             768\n",
      "             ReLU-76          [-1, 384, 32, 32]               0\n",
      "      fire_module-77          [-1, 384, 32, 32]               0\n",
      "           Conv2d-78          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-79          [-1, 192, 32, 32]             384\n",
      "           Conv2d-80          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-81          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-82          [-1, 384, 32, 32]             768\n",
      "             ReLU-83          [-1, 384, 32, 32]               0\n",
      "      fire_module-84          [-1, 384, 32, 32]               0\n",
      "           Conv2d-85          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-86          [-1, 192, 32, 32]             384\n",
      "           Conv2d-87          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-88          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-89          [-1, 384, 32, 32]             768\n",
      "             ReLU-90          [-1, 384, 32, 32]               0\n",
      "      fire_module-91          [-1, 384, 32, 32]               0\n",
      "           Conv2d-92          [-1, 192, 32, 32]          73,728\n",
      "      BatchNorm2d-93          [-1, 192, 32, 32]             384\n",
      "           Conv2d-94          [-1, 192, 32, 32]          36,864\n",
      "           Conv2d-95          [-1, 192, 32, 32]           1,728\n",
      "      BatchNorm2d-96          [-1, 384, 32, 32]             768\n",
      "             ReLU-97          [-1, 384, 32, 32]               0\n",
      "      fire_module-98          [-1, 384, 32, 32]               0\n",
      "           Conv2d-99          [-1, 256, 32, 32]          98,304\n",
      "     BatchNorm2d-100          [-1, 256, 32, 32]             512\n",
      "          Conv2d-101          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-102          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-103          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-104          [-1, 512, 16, 16]               0\n",
      "     fire_module-105          [-1, 512, 16, 16]               0\n",
      "          Conv2d-106          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-107          [-1, 256, 16, 16]             512\n",
      "          Conv2d-108          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-109          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-110          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-111          [-1, 512, 16, 16]               0\n",
      "     fire_module-112          [-1, 512, 16, 16]               0\n",
      "          Conv2d-113          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-114          [-1, 256, 16, 16]             512\n",
      "          Conv2d-115          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-116          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-117          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-118          [-1, 512, 16, 16]               0\n",
      "     fire_module-119          [-1, 512, 16, 16]               0\n",
      "          Conv2d-120          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-121          [-1, 256, 16, 16]             512\n",
      "          Conv2d-122          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-123          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-124          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-125          [-1, 512, 16, 16]               0\n",
      "     fire_module-126          [-1, 512, 16, 16]               0\n",
      "          Conv2d-127          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-128          [-1, 256, 16, 16]             512\n",
      "          Conv2d-129          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-130          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-131          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-132          [-1, 512, 16, 16]               0\n",
      "     fire_module-133          [-1, 512, 16, 16]               0\n",
      "          Conv2d-134          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-135          [-1, 256, 16, 16]             512\n",
      "          Conv2d-136          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-137          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-138          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-139          [-1, 512, 16, 16]               0\n",
      "     fire_module-140          [-1, 512, 16, 16]               0\n",
      "          Conv2d-141          [-1, 256, 16, 16]         131,072\n",
      "     BatchNorm2d-142          [-1, 256, 16, 16]             512\n",
      "          Conv2d-143          [-1, 256, 16, 16]          65,536\n",
      "          Conv2d-144          [-1, 256, 16, 16]           2,304\n",
      "     BatchNorm2d-145          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-146          [-1, 512, 16, 16]               0\n",
      "     fire_module-147          [-1, 512, 16, 16]               0\n",
      "          Conv2d-148          [-1, 192, 16, 16]          98,304\n",
      "     BatchNorm2d-149          [-1, 192, 16, 16]             384\n",
      "          Conv2d-150          [-1, 192, 16, 16]          36,864\n",
      "          Conv2d-151          [-1, 192, 16, 16]           1,728\n",
      "     BatchNorm2d-152          [-1, 384, 16, 16]             768\n",
      "            ReLU-153          [-1, 384, 16, 16]               0\n",
      "     fire_module-154          [-1, 384, 16, 16]               0\n",
      " ConvTranspose2d-155          [-1, 384, 32, 32]       2,359,680\n",
      "           merge-156          [-1, 384, 32, 32]               0\n",
      "       hg_module-157          [-1, 384, 32, 32]               0\n",
      "          Conv2d-158          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-159          [-1, 192, 32, 32]             384\n",
      "          Conv2d-160          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-161          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-162          [-1, 384, 32, 32]             768\n",
      "            ReLU-163          [-1, 384, 32, 32]               0\n",
      "     fire_module-164          [-1, 384, 32, 32]               0\n",
      "          Conv2d-165          [-1, 192, 32, 32]          73,728\n",
      "     BatchNorm2d-166          [-1, 192, 32, 32]             384\n",
      "          Conv2d-167          [-1, 192, 32, 32]          36,864\n",
      "          Conv2d-168          [-1, 192, 32, 32]           1,728\n",
      "     BatchNorm2d-169          [-1, 384, 32, 32]             768\n",
      "            ReLU-170          [-1, 384, 32, 32]               0\n",
      "     fire_module-171          [-1, 384, 32, 32]               0\n",
      " ConvTranspose2d-172          [-1, 384, 64, 64]       2,359,680\n",
      "           merge-173          [-1, 384, 64, 64]               0\n",
      "       hg_module-174          [-1, 384, 64, 64]               0\n",
      "          Conv2d-175          [-1, 192, 64, 64]          73,728\n",
      "     BatchNorm2d-176          [-1, 192, 64, 64]             384\n",
      "          Conv2d-177          [-1, 192, 64, 64]          36,864\n",
      "          Conv2d-178          [-1, 192, 64, 64]           1,728\n",
      "     BatchNorm2d-179          [-1, 384, 64, 64]             768\n",
      "            ReLU-180          [-1, 384, 64, 64]               0\n",
      "     fire_module-181          [-1, 384, 64, 64]               0\n",
      "          Conv2d-182          [-1, 128, 64, 64]          49,152\n",
      "     BatchNorm2d-183          [-1, 128, 64, 64]             256\n",
      "          Conv2d-184          [-1, 128, 64, 64]          16,384\n",
      "          Conv2d-185          [-1, 128, 64, 64]           1,152\n",
      "     BatchNorm2d-186          [-1, 256, 64, 64]             512\n",
      "            ReLU-187          [-1, 256, 64, 64]               0\n",
      "     fire_module-188          [-1, 256, 64, 64]               0\n",
      " ConvTranspose2d-189        [-1, 256, 128, 128]       1,048,832\n",
      "           merge-190        [-1, 256, 128, 128]               0\n",
      "       hg_module-191        [-1, 256, 128, 128]               0\n",
      "          Conv2d-192        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-193        [-1, 128, 128, 128]             256\n",
      "          Conv2d-194        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-195        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-196        [-1, 256, 128, 128]             512\n",
      "            ReLU-197        [-1, 256, 128, 128]               0\n",
      "     fire_module-198        [-1, 256, 128, 128]               0\n",
      "          Conv2d-199        [-1, 128, 128, 128]          32,768\n",
      "     BatchNorm2d-200        [-1, 128, 128, 128]             256\n",
      "          Conv2d-201        [-1, 128, 128, 128]          16,384\n",
      "          Conv2d-202        [-1, 128, 128, 128]           1,152\n",
      "     BatchNorm2d-203        [-1, 256, 128, 128]             512\n",
      "            ReLU-204        [-1, 256, 128, 128]               0\n",
      "     fire_module-205        [-1, 256, 128, 128]               0\n",
      " ConvTranspose2d-206        [-1, 256, 256, 256]       1,048,832\n",
      "           merge-207        [-1, 256, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 10,025,088\n",
      "Trainable params: 10,025,088\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 64.00\n",
      "Forward/backward pass size (MB): 3249.75\n",
      "Params size (MB): 38.24\n",
      "Estimated Total Size (MB): 3351.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class upsample(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super(upsample, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.interpolate(x, scale_factor=self.scale_factor)\n",
    "\n",
    "class merge(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "def _make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, out_dim)]\n",
    "    layers += [residual(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [residual(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [residual(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def _make_pool_layer(dim):\n",
    "    return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "def _make_unpool_layer(dim):\n",
    "    return upsample(scale_factor=2)\n",
    "\n",
    "def _make_merge_layer(dim):\n",
    "    return merge()\n",
    "\n",
    "class fire_module(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, sr=2, stride=1):\n",
    "        super(fire_module, self).__init__()\n",
    "        self.conv1    = nn.Conv2d(inp_dim, out_dim // sr, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1      = nn.BatchNorm2d(out_dim // sr)\n",
    "        self.conv_1x1 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=1, stride=stride, bias=False)\n",
    "        self.conv_3x3 = nn.Conv2d(out_dim // sr, out_dim // 2, kernel_size=3, padding=1, \n",
    "                                  stride=stride, groups=out_dim // sr, bias=False)\n",
    "        self.bn2      = nn.BatchNorm2d(out_dim)\n",
    "        self.skip     = (stride == 1 and inp_dim == out_dim)\n",
    "        self.relu     = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        conv2 = torch.cat((self.conv_1x1(bn1), self.conv_3x3(bn1)), 1)\n",
    "        bn2   = self.bn2(conv2)\n",
    "        if self.skip:\n",
    "            return self.relu(bn2 + x)\n",
    "        else:\n",
    "            return self.relu(bn2)\n",
    "\n",
    "class hg_module(nn.Module):\n",
    "    def __init__(\n",
    "        self, n, dims, modules, make_up_layer=_make_layer,\n",
    "        make_pool_layer=_make_pool_layer, make_hg_layer=_make_layer,\n",
    "        make_low_layer=_make_layer, make_hg_layer_revr=_make_layer_revr,\n",
    "        make_unpool_layer=_make_unpool_layer, make_merge_layer=_make_merge_layer\n",
    "    ):\n",
    "        super(hg_module, self).__init__()\n",
    "\n",
    "        curr_mod = modules[0]\n",
    "        next_mod = modules[1]\n",
    "\n",
    "        curr_dim = dims[0]\n",
    "        next_dim = dims[1]\n",
    "\n",
    "        self.n    = n\n",
    "        self.up1  = make_up_layer(curr_dim, curr_dim, curr_mod)\n",
    "        self.max1 = make_pool_layer(curr_dim)\n",
    "        self.low1 = make_hg_layer(curr_dim, next_dim, curr_mod)\n",
    "        self.low2 = hg_module(\n",
    "            n - 1, dims[1:], modules[1:],\n",
    "            make_up_layer=make_up_layer,\n",
    "            make_pool_layer=make_pool_layer,\n",
    "            make_hg_layer=make_hg_layer,\n",
    "            make_low_layer=make_low_layer,\n",
    "            make_hg_layer_revr=make_hg_layer_revr,\n",
    "            make_unpool_layer=make_unpool_layer,\n",
    "            make_merge_layer=make_merge_layer\n",
    "        ) if n > 1 else make_low_layer(next_dim, next_dim, next_mod)\n",
    "        self.low3 = make_hg_layer_revr(next_dim, curr_dim, curr_mod)\n",
    "        self.up2  = make_unpool_layer(curr_dim)\n",
    "        self.merg = make_merge_layer(curr_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        up1  = self.up1(x)\n",
    "        max1 = self.max1(x)\n",
    "        low1 = self.low1(max1)\n",
    "        low2 = self.low2(low1)\n",
    "        low3 = self.low3(low2)\n",
    "        up2  = self.up2(low3)\n",
    "        print(up1.shape)\n",
    "        print(up2.shape)\n",
    "        merg = self.merg(up1, up2)\n",
    "        return merg\n",
    "#         return up1\n",
    "\n",
    "    \n",
    "def make_pool_layer(dim):\n",
    "    return nn.Sequential()\n",
    "\n",
    "def make_unpool_layer(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "def make_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_layer_revr(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, inp_dim) for _ in range(modules - 1)]\n",
    "    layers += [fire_module(inp_dim, out_dim)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def make_hg_layer(inp_dim, out_dim, modules):\n",
    "    layers  = [fire_module(inp_dim, out_dim, stride=2)]\n",
    "    layers += [fire_module(out_dim, out_dim) for _ in range(1, modules)]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "m = hg_module(\n",
    "                4, [256, 256, 384, 384, 512], [2, 2, 2, 2, 4],\n",
    "                make_pool_layer=make_pool_layer,\n",
    "                make_unpool_layer=make_unpool_layer,\n",
    "                make_up_layer=make_layer,\n",
    "                make_low_layer=make_layer,\n",
    "                make_hg_layer_revr=make_layer_revr,\n",
    "                make_hg_layer=make_hg_layer\n",
    "            )\n",
    "\n",
    "summary(m, (256, 256, 256))\n",
    "# m(torch.rand(1, 256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 256, 200, 200]          65,792\n",
      "              ReLU-2        [-1, 256, 200, 200]               0\n",
      "       convolution-3        [-1, 256, 200, 200]               0\n",
      "            Conv2d-4         [-1, 80, 200, 200]          20,560\n",
      "================================================================\n",
      "Total params: 86,352\n",
      "Trainable params: 86,352\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 39.06\n",
      "Forward/backward pass size (MB): 258.79\n",
      "Params size (MB): 0.33\n",
      "Estimated Total Size (MB): 298.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def _pred_mod(dim):\n",
    "        return nn.Sequential(\n",
    "            convolution(1, 256, 256, with_bn=False),\n",
    "            nn.Conv2d(256, dim, (1, 1))\n",
    "        )\n",
    "m = _pred_mod(80)\n",
    "summary(m, (256, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-2        [-1, 128, 100, 100]             256\n",
      "              ReLU-3        [-1, 128, 100, 100]               0\n",
      "       convolution-4        [-1, 128, 100, 100]               0\n",
      "           TopPool-5        [-1, 128, 100, 100]               0\n",
      "            Conv2d-6        [-1, 128, 100, 100]         294,912\n",
      "       BatchNorm2d-7        [-1, 128, 100, 100]             256\n",
      "              ReLU-8        [-1, 128, 100, 100]               0\n",
      "       convolution-9        [-1, 128, 100, 100]               0\n",
      "         LeftPool-10        [-1, 128, 100, 100]               0\n",
      "           Conv2d-11        [-1, 256, 100, 100]         294,912\n",
      "      BatchNorm2d-12        [-1, 256, 100, 100]             512\n",
      "           Conv2d-13        [-1, 256, 100, 100]          65,536\n",
      "      BatchNorm2d-14        [-1, 256, 100, 100]             512\n",
      "             ReLU-15        [-1, 256, 100, 100]               0\n",
      "           Conv2d-16        [-1, 256, 100, 100]         589,824\n",
      "      BatchNorm2d-17        [-1, 256, 100, 100]             512\n",
      "             ReLU-18        [-1, 256, 100, 100]               0\n",
      "      convolution-19        [-1, 256, 100, 100]               0\n",
      "================================================================\n",
      "Total params: 1,542,144\n",
      "Trainable params: 1,542,144\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9.77\n",
      "Forward/backward pass size (MB): 273.44\n",
      "Params size (MB): 5.88\n",
      "Estimated Total Size (MB): 289.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models.py_utils import TopPool, LeftPool\n",
    "\n",
    "class corner_pool(nn.Module):\n",
    "    def __init__(self, dim, pool1, pool2):\n",
    "        super(corner_pool, self).__init__()\n",
    "        self._init_layers(dim, pool1, pool2)\n",
    "\n",
    "    def _init_layers(self, dim, pool1, pool2):\n",
    "        self.p1_conv1 = convolution(3, dim, 128)\n",
    "        self.p2_conv1 = convolution(3, dim, 128)\n",
    "\n",
    "        self.p_conv1 = nn.Conv2d(128, dim, (3, 3), padding=(1, 1), bias=False)\n",
    "        self.p_bn1   = nn.BatchNorm2d(dim)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, dim, (1, 1), bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = convolution(3, dim, dim)\n",
    "\n",
    "        self.pool1 = pool1()\n",
    "        self.pool2 = pool2()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pool 1\n",
    "        p1_conv1 = self.p1_conv1(x)\n",
    "        pool1    = self.pool1(p1_conv1)\n",
    "\n",
    "        # pool 2\n",
    "        p2_conv1 = self.p2_conv1(x)\n",
    "        pool2    = self.pool2(p2_conv1)\n",
    "\n",
    "        # pool 1 + pool 2\n",
    "        p_conv1 = self.p_conv1(pool1 + pool2)\n",
    "        p_bn1   = self.p_bn1(p_conv1)\n",
    "\n",
    "        conv1 = self.conv1(x)\n",
    "        bn1   = self.bn1(conv1)\n",
    "        relu1 = self.relu1(p_bn1 + bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        return conv2\n",
    "\n",
    "m = corner_pool(256, TopPool, LeftPool)\n",
    "summary(m, (256, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AddCoords-1              [-1, 3, 3, 3]               0\n",
      "            Conv2d-2              [-1, 8, 3, 3]             224\n",
      "            Conv2d-3             [-1, 16, 3, 3]           1,168\n",
      "            Conv2d-4             [-1, 32, 3, 3]           4,640\n",
      "            Conv2d-5            [-1, 128, 3, 3]          36,864\n",
      "       BatchNorm2d-6            [-1, 128, 3, 3]             256\n",
      "              ReLU-7            [-1, 128, 3, 3]               0\n",
      "       convolution-8            [-1, 128, 3, 3]               0\n",
      "           TopPool-9            [-1, 128, 3, 3]               0\n",
      "           Conv2d-10            [-1, 128, 3, 3]          36,864\n",
      "      BatchNorm2d-11            [-1, 128, 3, 3]             256\n",
      "             ReLU-12            [-1, 128, 3, 3]               0\n",
      "      convolution-13            [-1, 128, 3, 3]               0\n",
      "         LeftPool-14            [-1, 128, 3, 3]               0\n",
      "           Conv2d-15             [-1, 32, 3, 3]          36,864\n",
      "      BatchNorm2d-16             [-1, 32, 3, 3]              64\n",
      "           Conv2d-17             [-1, 32, 3, 3]           1,024\n",
      "      BatchNorm2d-18             [-1, 32, 3, 3]              64\n",
      "             ReLU-19             [-1, 32, 3, 3]               0\n",
      "           Conv2d-20             [-1, 32, 3, 3]           9,216\n",
      "      BatchNorm2d-21             [-1, 32, 3, 3]              64\n",
      "             ReLU-22             [-1, 32, 3, 3]               0\n",
      "      convolution-23             [-1, 32, 3, 3]               0\n",
      "      corner_pool-24             [-1, 32, 3, 3]               0\n",
      "           Conv2d-25              [-1, 1, 3, 3]              33\n",
      "           Conv2d-26              [-1, 1, 3, 3]               2\n",
      "================================================================\n",
      "Total params: 127,603\n",
      "Trainable params: 127,603\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.49\n",
      "Estimated Total Size (MB): 0.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.width = width\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.pool = corner_pool(32, TopPool, LeftPool)\n",
    "        self.conv4 = nn.Conv2d(32, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(1, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C_in, H, W)\n",
    "        x = self.add_coords(x)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "m = SimpleNet(width=3)\n",
    "summary(m, (1, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74535599, -0.47140452, -0.33333333],\n",
       "       [-0.66666667, -0.33333333, -0.        ],\n",
       "       [-0.74535599, -0.47140452, -0.33333333]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# def norm(x, width):\n",
    "#     return x * width\n",
    "\n",
    "def norm(x, width):\n",
    "    return (int)(x * (width - 1) + 0.5)\n",
    "\n",
    "def _draw_rect(points, width=64):\n",
    "    x0, y0, x1, y1 = points\n",
    "    print(x0, y0, x1, y1)\n",
    "    x0 = norm(x0, width)\n",
    "    y0 = norm(y0, width)\n",
    "    x1 = norm(x1, width)\n",
    "    y1 = norm(y1, width)\n",
    "#     if (x1 == 1):\n",
    "#         x1 -= 0.1\n",
    "#     if (y1 == 1):\n",
    "#         y1 -= 0.1\n",
    "    print(x0, y0, x1, y1)\n",
    "    im = Image.new(\"F\", (width, width))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.rectangle([x0, y0, x1, y1], fill=1, outline=None, width=0)\n",
    "    im = np.array(im)  # (H, W)\n",
    "    print(im)\n",
    "    im = np.expand_dims(im, axis=-1)  # (H, W, 1)\n",
    "    return im\n",
    "\n",
    "def draw_rect(xy, width=3):\n",
    "    x0, y0, x1, y1 = xy\n",
    "    rect = np.zeros()\n",
    "    \n",
    "\n",
    "width = 3\n",
    "xy = []\n",
    "for x0, y0 in itertools.product(range(width), range(width)):\n",
    "    for _w, _h in itertools.product(range(1, width - x0 + 1), range(1, width-y0+1)):\n",
    "        x1 = x0 + _w\n",
    "        y1 = y0 + _h\n",
    "        xy.append([x0, y0, x1, y1])\n",
    "#         x = np.array([x0, y0, x1, y1], dtype=float)\n",
    "#         x /= width\n",
    "#         draw_rect(x, width)\n",
    "        \n",
    "for (x0, y0, x1, y1) in xy:\n",
    "    rect = np.zeros((width, width))\n",
    "    for i, j in itertools.product(range(x0, x1), range(y0, y1)):\n",
    "        rect[i][j] = 1.\n",
    "#     print(rect)\n",
    "\n",
    "# x = np.array([1,2], dtype=int)\n",
    "# x = x.astype(float) / 2\n",
    "x = np.stack([np.array([1,2]), np.array([3,4]), np.array([3,4])])\n",
    "len(x)\n",
    "\n",
    "def draw_l2_distance(x, y, width=64):\n",
    "    im = np.zeros((width, width), dtype=float)\n",
    "    for (i, j), _ in np.ndenumerate(im):\n",
    "        im[i][j] = -np.linalg.norm(np.array([x, y]) - np.array([i, j])) / width\n",
    "#     im = im.transpose(1, 0).reshape(width * width)  # (W, H) -> (H, W) -> (H*W)\n",
    "    return im\n",
    "\n",
    "draw_l2_distance(1, 2, width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 100, 100])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.rand(1,3,100, 100)\n",
    "m = nn.MaxPool2d(2)\n",
    "x1 = m(x0)\n",
    "x1 = F.interpolate(x1, scale_factor=2)\n",
    "# x = torch.cat((x0, x1), dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating datasets...\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_rect_pil(xy, width=64):\n",
    "    x0, y0, x1, y1 = xy\n",
    "    x1 -= 0.5\n",
    "    y1 -= 0.5\n",
    "    im = Image.new(\"F\", (width, width))\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    draw.rectangle([x0, y0, x1, y1], fill=1)\n",
    "    im = np.array(im)  # (H, W)\n",
    "    return im\n",
    "\n",
    "\n",
    "def rand_draw(draw_fn=draw_rect_pil, n_strokes=2, width=64):\n",
    "    canvas = np.zeros((width, width, 3), dtype=np.int8)\n",
    "    im = [canvas.copy()]\n",
    "    x = []\n",
    "    for _ in range(n_strokes):\n",
    "        x0, y0 = np.random.randint(width, size=2)\n",
    "        x1 = x0 + np.random.randint(1, width - x0 + 1)\n",
    "        y1 = y0 + np.random.randint(1, width - y0 + 1)\n",
    "        _x = np.array((x0, y0, x1, y1))\n",
    "        #         _x = np.random.rand(action_dim)\n",
    "        color = np.random.randint(255, size=(3))  # (3)\n",
    "        #         x.append(np.concatenate((_x, color / 255.0)))\n",
    "        stroke = draw_fn(_x, width)  # (w, w)\n",
    "        stroke = np.expand_dims(stroke, axis=2)  # (w, w, 1)\n",
    "        canvas = canvas * (1 - stroke) + stroke * color  # (w, h, 3)\n",
    "        x.append(_x)\n",
    "        im.append(canvas.copy())\n",
    "    x = np.stack(x) / width  # (n_strokes, action_dim+3)\n",
    "    im = np.stack(im)\n",
    "    return x, im\n",
    "\n",
    "def generate_data(width=128, n_sample=1000, n_strokes=2):\n",
    "    print(\"Generating datasets...\")\n",
    "    if not os.path.exists(\"data-rect/\"):\n",
    "        os.makedirs(\"data-rect/\")\n",
    "\n",
    "    x, im = [], []\n",
    "    for _ in range(n_sample):\n",
    "        _x, _im = rand_draw(n_strokes=n_strokes, width=width)\n",
    "        _im_inter = _im[:-1]\n",
    "        _im_target = _im[-1]\n",
    "#         print(_im_inter.shape)\n",
    "        for i in range(n_strokes):\n",
    "            im.append(np.concatenate((_im_inter[i], _im_target), axis=2))\n",
    "            x.append(_x[i])\n",
    "    print(len(x))\n",
    "    print(len(im))\n",
    "#     print(im[0][:,:,-3:])\n",
    "#     plt.imshow(Image.fromarray(im[1][:,:,:-3].astype('uint8')))\n",
    "    plt.imshow(Image.fromarray(im[1][:,:,-3:].astype('uint8')))\n",
    "\n",
    "generate_data(width=16, n_sample=1, n_strokes=2)\n",
    "# a = np.array([1,2,3])\n",
    "# a = np.random.rand(5,3)\n",
    "# b = np.array([3,4,5])\n",
    "# b = np.random.rand(5,3)\n",
    "# np.concatenate([a,b],axis=1).shape\n",
    "# x[:-1]\n",
    "\n",
    "# x, im = rand_draw()\n",
    "# fig, ax = plt.subplots(1,3)\n",
    "# ax[0].imshow(Image.fromarray(im[0].astype(np.uint8)))\n",
    "# ax[1].imshow(Image.fromarray(im[1].astype(np.uint8)))\n",
    "# ax[2].imshow(Image.fromarray(im[2].astype(np.uint8)))\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channel, width, out_size=4, latent_size=128):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.add_coords = AddCoords(rank=2)\n",
    "#         self.conv0 = nn.Conv2d(in_channel + 2, latent_size, 3, padding=1)\n",
    "#         self.bn0 = BatchNorm2d(latent_size)\n",
    "#         self.conv1 = nn.Conv2d(latent_size, latent_size, 3, padding=1)\n",
    "#         self.bn1 = BatchNorm2d(latent_size)\n",
    "# self.conv2 = nn.Conv2d(latent_size, out_size, 1)\n",
    "        self.conv0 = nn.Conv2d(in_channel + 2, latent_size, 1, stride=1, padding=0)\n",
    "        self.bn0 = nn.BatchNorm2d(latent_size)\n",
    "        self.conv2 = nn.Conv2d(latent_size, out_size, 1, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(width, stride=width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_coords(x)\n",
    "        x = self.conv0(x)\n",
    "        x = F.relu(self.bn0(x))\n",
    "#         x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4)\n",
    "        return x\n",
    "    \n",
    "class HRNet(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(HRNet, self).__init__()\n",
    "        cfg = get_cfg_defaults()\n",
    "        cfg.merge_from_file(\"./hrnet.yaml\")\n",
    "        self.hr0 = HighResolutionNet(cfg)\n",
    "        self.hr1 = HighResolutionNet(cfg)\n",
    "        self.rg = Regressor(in_channel=540, width=int(width/4))\n",
    "        \n",
    "    def forward(self, im_current, im_target):\n",
    "        x0 = self.hr0(im_current)\n",
    "        x1 = self.hr1(im_target)\n",
    "        x = torch.cat((x0, x1), dim=1)\n",
    "        x = self.rg(x)\n",
    "        return x\n",
    "\n",
    "class Painter(nn.Module):\n",
    "    def __init__(self, in_dim=128, out_dim=4, hidden_dim=128):\n",
    "        super(Painter, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hidden_dim)\n",
    "        self.fc0 = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "m = HRNet(64)\n",
    "x0 = torch.rand(1,3,64,64)\n",
    "x1 = torch.rand(1,3,64,64)\n",
    "m(x0, x1).shape\n",
    "\n",
    "# summary(m, (1, 128))\n",
    "\n",
    "# rnn = nn.GRU(10, 20)\n",
    "# input = torch.randn(5, 3, 10)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# # output, hn = rnn(input, h0)\n",
    "# output, hn = rnn(input)\n",
    "# print(output.shape)\n",
    "# print(hn.shape)\n",
    "\n",
    "\n",
    "# summary(rnn, (3, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
